{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from sklearn.model_selection import train_test_split # train test비율에 맞게 짜르기\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "\n",
    "import easydict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def bind_model(model, optimizer=None):\n",
    "    def save(path, *args, **kwargs):\n",
    "        state = {\n",
    "            'model': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict()\n",
    "        }\n",
    "        torch.save(state, os.path.join(path, 'model.pt'))\n",
    "        print('Model saved')\n",
    "\n",
    "    def load(path, *args, **kwargs):\n",
    "        state = torch.load(os.path.join(path, 'model.pt'))\n",
    "        model.load_state_dict(state['model'])\n",
    "        if 'optimizer' in state and optimizer:\n",
    "            optimizer.load_state_dict(state['optimizer'])\n",
    "        print('Model loaded')\n",
    "\n",
    "    # 추론\n",
    "    def infer(path, **kwargs):\n",
    "        return inference(path, model)\n",
    "\n",
    "def inference(path, model, **kwargs):\n",
    "    model.eval()\n",
    "    \n",
    "    data = Variable(preproc_data(pd.read_csv(path), train=False))\n",
    "    output_pred_labels = torch.round(torch.sigmoid(model(data)))\n",
    "    output_pred_labels = output_pred_labels.detach().numpy()\n",
    "    output_pred_labels = output_pred_labels.astype('int').reshape(-1).tolist()\n",
    "\n",
    "    # output format\n",
    "    # [(step, label), (step, label), ..., (step, label)]\n",
    "    results = [(step, label) for step, label in enumerate(output_pred_labels)]\n",
    "    \n",
    "    return results\n",
    "\n",
    "def preproc_data(data, label=None, train=True, val_ratio=0.2, seed=1234):\n",
    "    if train:\n",
    "        dataset = dict()\n",
    "\n",
    "        # NaN 값 0으로 채우기\n",
    "        data = data.fillna(0)\n",
    "        \n",
    "        # 성별 ['M', 'F'] -> [0, 1]로 변환\n",
    "        #data['gender_enc'] = np.where(data['gender'] == 'M', 0, 1)\n",
    "        data['d_l_match_yn']=data['d_l_match_yn'].replace([True,False],[1,0])\n",
    "        data['d_m_match_yn']=data['d_m_match_yn'].replace([True,False],[1,0])\n",
    "        data['d_s_match_yn']=data['d_s_match_yn'].replace([True,False],[1,0])\n",
    "        data['h_l_match_yn']=data['h_l_match_yn'].replace([True,False],[1,0])\n",
    "        data['h_m_match_yn']=data['h_m_match_yn'].replace([True,False],[1,0])\n",
    "        data['h_s_match_yn']=data['h_s_match_yn'].replace([True,False],[1,0])\n",
    "        # 날짜 datetime으로 변환\n",
    "        # df.loc[:, 'date'] = pd.to_datetime(df['date'], format='%Y%m%d')\n",
    "\n",
    "        #DROP_COLS = ['CDMID', 'gender', 'date', 'date_E','Ht','Wt','LDL','Cr','AST']\n",
    "        DROP_COLS = ['id', 'contents_open_dt']\n",
    "        X = data.drop(columns=DROP_COLS).copy()\n",
    "        y = label\n",
    "\n",
    "        # bsmote=BorderlineSMOTE(radnom_state=42, k_neighbors=5, m_neighbor=10)\n",
    "        # X_bsmote, y_bsmote = bsmote.fit_resample(X,y)\n",
    "\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X, y,\n",
    "                                                          stratify=y,\n",
    "                                                          test_size=val_ratio,\n",
    "                                                          random_state=seed,\n",
    "                                                          )\n",
    "\n",
    "        # print(X_train.values)\n",
    "\n",
    "        X_train = torch.as_tensor(X_train.values).float()\n",
    "        y_train = np.array(y_train)\n",
    "        y_train = torch.as_tensor(y_train.reshape(-1, 1)).float()\n",
    "        X_val = torch.as_tensor(X_val.values).float()\n",
    "        y_val = np.array(y_val)\n",
    "        y_val = torch.as_tensor(y_val.reshape(-1, 1)).float()\n",
    "\n",
    "        dataset['train'] = TensorDataset(X_train, y_train)\n",
    "        dataset['val'] = TensorDataset(X_val, y_val)\n",
    "\n",
    "        return dataset\n",
    "    \n",
    "    else:\n",
    "        # NaN 값 0으로 채우기\n",
    "        data = data.fillna(0)\n",
    "\n",
    "        # 성별 ['M', 'F'] -> [0, 1]로 변환\n",
    "        # data['gender_enc'] = np.where(data['gender'] == 'M', 0, 1)\n",
    "\n",
    "        # 날짜 datetime으로 변환\n",
    "        # df.loc[:, 'date'] = pd.to_datetime(df['date'], format='%Y%m%d')\n",
    "        DROP_COLS = ['id', 'contents_open_dt']\n",
    "        data = data.drop(columns=DROP_COLS).copy()\n",
    "\n",
    "        X_test = torch.as_tensor(data.values).float()\n",
    "\n",
    "        return X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(input_size,16),\n",
    "            nn.BatchNorm1d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, output_size)\n",
    "        )        \n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# args = argparse.ArgumentParser()\n",
    "\n",
    "# # DONOTCHANGE: They are reserved for nsml\n",
    "# args.add_argument('--mode', type=str, default='train', help='submit일때 해당값이 test로 설정됩니다.')\n",
    "# args.add_argument('--iteration', type=str, default='0',\n",
    "#                     help='fork 명령어를 입력할때의 체크포인트로 설정됩니다. 체크포인트 옵션을 안주면 마지막 wall time 의 model 을 가져옵니다.')\n",
    "# args.add_argument('--pause', type=int, default=0, help='model 을 load 할때 1로 설정됩니다.')\n",
    "\n",
    "# args.add_argument('--seed', type=int, default=42)\n",
    "# args.add_argument('--batch_size', type=int, default=128)\n",
    "# args.add_argument('--val_ratio', type=int, default=0.2)\n",
    "# args.add_argument('--lr', type=float, default=0.1)\n",
    "# args.add_argument('--input_size', type=int, default=17)\n",
    "# args.add_argument('--epochs', type=int, default=50)\n",
    "# config = args.parse_args()\n",
    "\n",
    "\n",
    "config = easydict.EasyDict({\n",
    " \n",
    "        \"mode\": 'train',\n",
    " \n",
    "        \"iteration\": '0',\n",
    " \n",
    "        \"pause\": 0,\n",
    " \n",
    "        \"seed\": 42,\n",
    " \n",
    "        \"batch_size\": 64,\n",
    " \n",
    "        \"val_ratio\": 0.2,\n",
    "    \n",
    "        \"lr\": 0.1,\n",
    " \n",
    "        \"input_size\": 32,\n",
    " \n",
    "        \"epochs\": 50,\n",
    " \n",
    "})\n",
    "\n",
    "print(config.epochs)\n",
    "time_init = time.time()\n",
    "\n",
    "torch.manual_seed(config.seed)\n",
    "np.random.seed(config.seed)\n",
    "\n",
    "model = LogisticRegression(config.input_size, 1)\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=config.lr)\n",
    "\n",
    "# nsml.bind() should be called before nsml.paused()\n",
    "bind_model(model, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': <torch.utils.data.dataset.TensorDataset object at 0x0000021F9D85BD00>, 'val': <torch.utils.data.dataset.TensorDataset object at 0x0000021F9D85BCD0>}\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x0000021F9D85BE80>\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x0000021F8B6EB850>\n"
     ]
    },
    {
     "ename": "SyntaxError",
     "evalue": "'break' outside loop (<ipython-input-15-e76cd742c7e8>, line 21)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-15-e76cd742c7e8>\"\u001b[1;36m, line \u001b[1;32m21\u001b[0m\n\u001b[1;33m    break\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m 'break' outside loop\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('C:\\\\Users\\\\user\\\\Desktop\\\\computing\\\\outside\\\\dacon\\\\jobcare_recommend\\\\JobCare_data\\\\train.csv')\n",
    "test = pd.read_csv('C:\\\\Users\\\\user\\\\Desktop\\\\computing\\\\outside\\\\dacon\\\\jobcare_recommend\\\\JobCare_data\\\\test.csv')\n",
    "\n",
    "x = train.iloc[:, :-1]\n",
    "y = train.iloc[:, -1]\n",
    "\n",
    "raw_data = x\n",
    "raw_labels = y\n",
    "dataset = preproc_data(raw_data, raw_labels, train=True, val_ratio=0.2, seed=1234) #train 데이터 분할\n",
    "train_dl = DataLoader(dataset['train'], config.batch_size, shuffle=True) # train 데이터\n",
    "val_dl = DataLoader(dataset['val'], config.batch_size, shuffle=False) # val데이터\n",
    "#config.batch_size=128\n",
    "print(dataset) #train 데이터 분할\n",
    "print(train_dl)\n",
    "print(val_dl)\n",
    "\n",
    "time_dl_init = time.time()\n",
    "# print('Time to dataloader initialization: ', time_dl_init - time_init)\n",
    "\n",
    "\n",
    "model = LogisticRegression(config.input_size, 1)\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=config.lr)\n",
    "\n",
    "\n",
    "min_val_loss = np.inf\n",
    "for epoch in range(config.epochs): #epochs=50\n",
    "    # train model\n",
    "    running_loss = 0.\n",
    "    num_runs = 0\n",
    "    model.train() #모델을 학습시키겠다는 신호\n",
    "    \n",
    "    \n",
    "    # def train(self, mode=True):\n",
    "    #     r\"\"\"Sets the module in training mode.\"\"\"      \n",
    "    #     self.training = mode\n",
    "    #     for module in self.children():\n",
    "    #         module.train(mode)\n",
    "    #     return self\n",
    "\n",
    "\n",
    "    total_length = len(train_dl)\n",
    "    auc = 0\n",
    "    for iter_idx, (data, labels) in enumerate(train_dl):\n",
    "        data = Variable(data) #데이터\n",
    "        labels = Variable(labels) #정답지\n",
    "\n",
    "        output_pred = model(data) #모델이 학습해놓은 예측값\n",
    "        print(output_pred)\n",
    "        auc_labels = np.array(labels)\n",
    "        auc_pred = np.array([])\n",
    "\n",
    "        for i in range(len(labels)):\n",
    "            if labels[i][0] == 0.:\n",
    "                auc_pred = np.append(auc_pred, 1-output_pred[i][0].detach())\n",
    "            else:\n",
    "                auc_pred = np.append(auc_pred, output_pred[i][0].detach())\n",
    "\n",
    "        #auc_pred = np.array([1 if i >=0.5 else 0 for i in output_pred])\n",
    "\n",
    "        if 1. not in auc_labels:\n",
    "            auc_labels = np.append(auc_labels, [[1]], axis =0)\n",
    "            auc_pred = np.append(auc_pred, [0.5], axis=0)\n",
    "\n",
    "        auc += roc_auc_score(auc_labels, auc_pred)\n",
    "\n",
    "        loss = loss_fn(output_pred, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        num_runs += 1\n",
    "\n",
    "\n",
    "        # get current lr\n",
    "        opt_params = optimizer.state_dict()['param_groups'][0]\n",
    "        step = epoch * total_length + iter_idx\n",
    "\n",
    "        # nsml.report(\n",
    "        #     epoch=epoch + int(config.iteration),\n",
    "        #     epoch_total=config.epochs,\n",
    "        #     iter=iter_idx,\n",
    "        #     iter_total=total_length,\n",
    "        #     batch_size=config.batch_size,\n",
    "        #     train__loss=running_loss / num_runs,\n",
    "        #     train_auc=auc / num_runs,\n",
    "        #     step=step,\n",
    "        #     lr=opt_params['lr'],\n",
    "        #     scope=locals()\n",
    "        # )\n",
    "\n",
    "    print(f\"[Epoch {epoch}] Loss: {running_loss / num_runs}\")\n",
    "    print(f\"[Epoch {epoch}] auc: {auc / num_runs}\")\n",
    "\n",
    "    # test model with validation data\n",
    "    model.eval() #모델을 평가하겠다는 신호\n",
    "    running_loss = 0.\n",
    "    auc = 0.\n",
    "    num_runs = 0\n",
    "    for data, labels in val_dl:\n",
    "        data = Variable(data)\n",
    "        labels = Variable(labels)\n",
    "\n",
    "        output_pred = model(data)\n",
    "        loss = loss_fn(output_pred, labels)\n",
    "\n",
    "        auc_labels = np.array(labels)\n",
    "        auc_pred = np.array([])\n",
    "\n",
    "        for i in range(len(labels)):\n",
    "            if labels[i][0] == 0.:\n",
    "                auc_pred = np.append(auc_pred, 1-output_pred[i][0].detach())\n",
    "            else:\n",
    "                auc_pred = np.append(auc_pred, output_pred[i][0].detach())\n",
    "\n",
    "\n",
    "        if 1. not in auc_labels:\n",
    "            auc_labels = np.append(auc_labels, [[1]], axis =0)\n",
    "            auc_pred = np.append(auc_pred, [0.5], axis=0)\n",
    "\n",
    "        auc += roc_auc_score(auc_labels, auc_pred)\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        num_runs += 1\n",
    "\n",
    "    print(f\"[Validation] Loss: {running_loss / num_runs}\")\n",
    "    print(f\"[Validation] auc: {auc}\")\n",
    "\n",
    "    # nsml.report(\n",
    "    #     summary=True,\n",
    "    #     epoch=epoch,\n",
    "    #     epoch_total=config.epochs,\n",
    "    #     val__loss=running_loss / num_runs,\n",
    "    #     val_auc=auc,\n",
    "    #     step=(epoch + 1) * total_length,\n",
    "    #     lr=opt_params['lr']\n",
    "    # )\n",
    "\n",
    "    # if (running_loss < min_val_loss) or (epoch % 10 == 0):\n",
    "    #     nsml.save(epoch)\n",
    "\n",
    "final_time = time.time()\n",
    "print(\"Time to dataloader initialization: \", time_dl_init - time_init)\n",
    "print(\"Time spent on training :\", final_time - time_dl_init)\n",
    "print(\"Total time: \", final_time - time_init)\n",
    "\n",
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f68a52d6746b8edec33e9d0848c665d5f17e646007ceb6470c84aeea645ccfe6"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit ('dacon': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
