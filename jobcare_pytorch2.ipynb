{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from sklearn.model_selection import train_test_split # train test비율에 맞게 짜르기\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "\n",
    "import easydict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_loss(y_true:torch.Tensor, y_pred:torch.Tensor, is_training=False) -> torch.Tensor:\n",
    "\n",
    "    assert y_true.ndim == 1\n",
    "    assert y_pred.ndim == 1 or y_pred.ndim == 2\n",
    "    \n",
    "    if y_pred.ndim == 2:\n",
    "        y_pred = y_pred.argmax(dim=1)\n",
    "        \n",
    "    \n",
    "    tp = (y_true * y_pred).sum().to(torch.float32)\n",
    "    tn = ((1 - y_true) * (1 - y_pred)).sum().to(torch.float32)\n",
    "    fp = ((1 - y_true) * y_pred).sum().to(torch.float32)\n",
    "    fn = (y_true * (1 - y_pred)).sum().to(torch.float32)\n",
    "    \n",
    "    epsilon = 1e-7\n",
    "    \n",
    "    precision = tp / (tp + fp + epsilon)\n",
    "    recall = tp / (tp + fn + epsilon)\n",
    "    \n",
    "    f1 = 2* (precision*recall) / (precision + recall + epsilon)\n",
    "    f1.requires_grad = is_training\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(32,16),\n",
    "            nn.BatchNorm1d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 1)\n",
    "        )        \n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   d_l_match_yn  d_m_match_yn  d_s_match_yn  h_l_match_yn  h_m_match_yn  \\\n",
      "0          True          True          True         False         False   \n",
      "1         False         False         False          True          True   \n",
      "2         False         False         False          True         False   \n",
      "3         False         False         False          True         False   \n",
      "4          True          True          True         False         False   \n",
      "\n",
      "   h_s_match_yn  person_attribute_a  person_attribute_a_1  person_attribute_b  \\\n",
      "0         False                   1                     4                   3   \n",
      "1         False                   1                     3                   4   \n",
      "2         False                   2                     0                   3   \n",
      "3         False                   2                     0                   2   \n",
      "4         False                   1                     3                   4   \n",
      "\n",
      "   person_prefer_c  ...  contents_attribute_c  contents_attribute_k  \\\n",
      "0                5  ...                     1                     2   \n",
      "1                1  ...                     1                     2   \n",
      "2                5  ...                     1                     1   \n",
      "3                5  ...                     1                     2   \n",
      "4                5  ...                     1                     2   \n",
      "\n",
      "   contents_attribute_l  contents_attribute_d  contents_attribute_m  \\\n",
      "0                  1608                   275                     1   \n",
      "1                  1608                   275                     1   \n",
      "2                  1600                    94                     1   \n",
      "3                  1608                   275                     5   \n",
      "4                  1608                   275                     1   \n",
      "\n",
      "   contents_attribute_e  contents_attribute_h  person_rn  contents_rn  target  \n",
      "0                     4                   139     618822       354805       1  \n",
      "1                     4                   133     571659       346213       0  \n",
      "2                     4                    53     399816       206408       0  \n",
      "3                     3                    74     827967       572323       0  \n",
      "4                     4                    74     831614       573899       0  \n",
      "\n",
      "[5 rows x 33 columns]\n",
      "[     1      1      1      0      0      0      1      4      3      5\n",
      "    275    370    369      8      1      1      4     95     59      3\n",
      "      3     10      2      1      2   1608    275      1      4    139\n",
      " 618822 354805]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('C:\\\\Users\\\\user\\\\Desktop\\\\computing\\\\outside\\\\dacon\\\\jobcare_recommend\\\\JobCare_data\\\\train.csv')\n",
    "test = pd.read_csv('C:\\\\Users\\\\user\\\\Desktop\\\\computing\\\\outside\\\\dacon\\\\jobcare_recommend\\\\JobCare_data\\\\test.csv')\n",
    "\n",
    "\n",
    "train = train.drop(['id', 'contents_open_dt'], axis=1) \n",
    "test = test.drop(['id', 'contents_open_dt'], axis=1)\n",
    "\n",
    "print(train.head())\n",
    "train['d_l_match_yn']=train['d_l_match_yn'].replace([True,False],[1,0])\n",
    "train['d_m_match_yn']=train['d_m_match_yn'].replace([True,False],[1,0])\n",
    "train['d_s_match_yn']=train['d_s_match_yn'].replace([True,False],[1,0])\n",
    "train['h_l_match_yn']=train['h_l_match_yn'].replace([True,False],[1,0])\n",
    "train['h_m_match_yn']=train['h_m_match_yn'].replace([True,False],[1,0])\n",
    "train['h_s_match_yn']=train['h_s_match_yn'].replace([True,False],[1,0])\n",
    "\n",
    "test['d_l_match_yn']=test['d_l_match_yn'].replace([True,False],[1,0])\n",
    "test['d_m_match_yn']=test['d_m_match_yn'].replace([True,False],[1,0])\n",
    "test['d_s_match_yn']=test['d_s_match_yn'].replace([True,False],[1,0])\n",
    "test['h_l_match_yn']=test['h_l_match_yn'].replace([True,False],[1,0])\n",
    "test['h_m_match_yn']=test['h_m_match_yn'].replace([True,False],[1,0])\n",
    "test['h_s_match_yn']=test['h_s_match_yn'].replace([True,False],[1,0])\n",
    "\n",
    "x = train.iloc[0:1000, :-1]\n",
    "y = train.iloc[0:1000, -1]\n",
    "\n",
    "\n",
    "\n",
    "x= x.to_numpy()\n",
    "print(x[0])\n",
    "print(y[0])\n",
    "x_train = torch.FloatTensor(x)\n",
    "y_train = torch.FloatTensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 32])\n",
      "torch.Size([1000])\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000])\n",
      "torch.Size([1000])\n",
      "tensor([1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "        1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 0., 0., 1., 1., 0.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1., 0., 0., 1.,\n",
      "        0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1.,\n",
      "        0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0.,\n",
      "        0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1.,\n",
      "        0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 1., 0., 0.,\n",
      "        0., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1.,\n",
      "        0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0.,\n",
      "        1., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1.,\n",
      "        1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "        1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1.,\n",
      "        0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1.,\n",
      "        0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0., 0., 0.,\n",
      "        0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0.,\n",
      "        0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1.,\n",
      "        0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 0.,\n",
      "        0., 1., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0.,\n",
      "        0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 0.,\n",
      "        0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0.,\n",
      "        0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0.,\n",
      "        1., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 1.,\n",
      "        0., 1., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 0., 1., 0.,\n",
      "        0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.,\n",
      "        0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0.,\n",
      "        0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,\n",
      "        0., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 0., 1., 0.,\n",
      "        0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1.,\n",
      "        1., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1.,\n",
      "        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1.,\n",
      "        1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 1., 0.,\n",
      "        0., 1., 0., 0., 1., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1.,\n",
      "        1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1.,\n",
      "        0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
      "        0., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 1., 0., 1.,\n",
      "        0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 1.,\n",
      "        0., 0., 1., 0., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0.,\n",
      "        1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1.,\n",
      "        0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0.,\n",
      "        1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1.,\n",
      "        1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1.,\n",
      "        0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.,\n",
      "        0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1.,\n",
      "        0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1.,\n",
      "        0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0.,\n",
      "        0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1.,\n",
      "        1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1.,\n",
      "        0., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 0., 1., 0.])\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-102-107fe52428c1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;31m# cost로 H(x) 개선\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m     \u001b[0mcost\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\stockpredict\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 307\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\stockpredict\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    152\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 154\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[1;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=1)\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "\n",
    "    # H(x) 계산\n",
    "    hypothesis = model(x_train)\n",
    "\n",
    "    hypothesis=hypothesis.view([1000])\n",
    "    print(hypothesis.size())\n",
    "    print(y_train.size())\n",
    "    # print(hypothesis)\n",
    "    prediction = hypothesis >= torch.FloatTensor([0.5])\n",
    "    prediction= prediction.float()\n",
    "    print(y_train)\n",
    "    print(prediction)\n",
    "    # break\n",
    "\n",
    "\n",
    "    # cost 계산\n",
    "    cost = F.binary_cross_entropy(prediction, y_train)\n",
    "\n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # 20번마다 로그 출력\n",
    "    if epoch % 10 == 0:\n",
    "        prediction = hypothesis >= torch.FloatTensor([0.5]) # 예측값이 0.5를 넘으면 True로 간주\n",
    "        correct_prediction = prediction.float() == y_train # 실제값과 일치하는 경우만 True로 간주\n",
    "        accuracy = correct_prediction.sum().item() / len(correct_prediction) # 정확도를 계산\n",
    "        print('Epoch {:4d}/{} Cost: {:.6f} Accuracy {:2.2f}%'.format( # 각 에포크마다 정확도를 출력\n",
    "            epoch, nb_epochs, cost.item(), accuracy * 100,\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True)\n",
      "tensor([nan], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(W)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 1005628096512.000000  [  128/501951]\n",
      "loss: 424864579584.000000  [12928/501951]\n",
      "loss: 100933312512.000000  [25728/501951]\n",
      "loss: 144303095808.000000  [38528/501951]\n",
      "loss: 24015765504.000000  [51328/501951]\n",
      "loss: 13416057856.000000  [64128/501951]\n",
      "loss: 14653132800.000000  [76928/501951]\n",
      "loss: 7364025344.000000  [89728/501951]\n",
      "loss: 7964484608.000000  [102528/501951]\n",
      "loss: 3720664064.000000  [115328/501951]\n",
      "loss: 4370186752.000000  [128128/501951]\n",
      "loss: 4594412544.000000  [140928/501951]\n",
      "loss: 2557721600.000000  [153728/501951]\n",
      "loss: 1863687040.000000  [166528/501951]\n",
      "loss: 1154097664.000000  [179328/501951]\n",
      "loss: 1370611072.000000  [192128/501951]\n",
      "loss: 594336512.000000  [204928/501951]\n",
      "loss: 689816960.000000  [217728/501951]\n",
      "loss: 539333056.000000  [230528/501951]\n",
      "loss: 1538718336.000000  [243328/501951]\n",
      "loss: 1023725696.000000  [256128/501951]\n",
      "loss: 785935488.000000  [268928/501951]\n",
      "loss: 803020288.000000  [281728/501951]\n",
      "loss: 719183872.000000  [294528/501951]\n",
      "loss: 412873280.000000  [307328/501951]\n",
      "loss: 394363584.000000  [320128/501951]\n",
      "loss: 199374000.000000  [332928/501951]\n",
      "loss: 292342464.000000  [345728/501951]\n",
      "loss: 113296848.000000  [358528/501951]\n",
      "loss: 77457152.000000  [371328/501951]\n",
      "loss: 106841992.000000  [384128/501951]\n",
      "loss: 97247056.000000  [396928/501951]\n",
      "loss: 56641752.000000  [409728/501951]\n",
      "loss: 34023432.000000  [422528/501951]\n",
      "loss: 49361512.000000  [435328/501951]\n",
      "loss: 50851116.000000  [448128/501951]\n",
      "loss: 28959124.000000  [460928/501951]\n",
      "loss: 24442940.000000  [473728/501951]\n",
      "loss: 25626096.000000  [486528/501951]\n",
      "loss: 33973328.000000  [499328/501951]\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 47387264.000000  [  128/501951]\n",
      "loss: 24222668.000000  [12928/501951]\n",
      "loss: 28733546.000000  [25728/501951]\n",
      "loss: 16473932.000000  [38528/501951]\n",
      "loss: 27783454.000000  [51328/501951]\n",
      "loss: 24761252.000000  [64128/501951]\n",
      "loss: 22868014.000000  [76928/501951]\n",
      "loss: 16218882.000000  [89728/501951]\n",
      "loss: 17380808.000000  [102528/501951]\n",
      "loss: 11017735.000000  [115328/501951]\n",
      "loss: 14268664.000000  [128128/501951]\n",
      "loss: 9318684.000000  [140928/501951]\n",
      "loss: 19644462.000000  [153728/501951]\n",
      "loss: 9567225.000000  [166528/501951]\n",
      "loss: 7364848.000000  [179328/501951]\n",
      "loss: 6055349.000000  [192128/501951]\n",
      "loss: 16409169.000000  [204928/501951]\n",
      "loss: 2898633.000000  [217728/501951]\n",
      "loss: 17587944.000000  [230528/501951]\n",
      "loss: 18462774.000000  [243328/501951]\n",
      "loss: 6316528.000000  [256128/501951]\n",
      "loss: 5758929.500000  [268928/501951]\n",
      "loss: 2605923.500000  [281728/501951]\n",
      "loss: 20057102.000000  [294528/501951]\n",
      "loss: 3967660.000000  [307328/501951]\n",
      "loss: 3473664.000000  [320128/501951]\n",
      "loss: 5919070.000000  [332928/501951]\n",
      "loss: 6896680.000000  [345728/501951]\n",
      "loss: 3563802.000000  [358528/501951]\n",
      "loss: 3472997.500000  [371328/501951]\n",
      "loss: 4038479.000000  [384128/501951]\n",
      "loss: 3315859.750000  [396928/501951]\n",
      "loss: 2968044.250000  [409728/501951]\n",
      "loss: 5670968.000000  [422528/501951]\n",
      "loss: 8665228.000000  [435328/501951]\n",
      "loss: 4449758.000000  [448128/501951]\n",
      "loss: 2373432.500000  [460928/501951]\n",
      "loss: 6115592.000000  [473728/501951]\n",
      "loss: 4258774.000000  [486528/501951]\n",
      "loss: 2908554.500000  [499328/501951]\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 2186811.000000  [  128/501951]\n",
      "loss: 7427656.000000  [12928/501951]\n",
      "loss: 1339090.500000  [25728/501951]\n",
      "loss: 7020801.000000  [38528/501951]\n",
      "loss: 3600465.750000  [51328/501951]\n",
      "loss: 6056291.000000  [64128/501951]\n",
      "loss: 4456387.000000  [76928/501951]\n",
      "loss: 3791340.000000  [89728/501951]\n",
      "loss: 2481677.000000  [102528/501951]\n",
      "loss: 3885503.000000  [115328/501951]\n",
      "loss: 3696163.250000  [128128/501951]\n",
      "loss: 9746974.000000  [140928/501951]\n",
      "loss: 5258026.000000  [153728/501951]\n",
      "loss: 1319211.000000  [166528/501951]\n",
      "loss: 487535.843750  [179328/501951]\n",
      "loss: 3342867.250000  [192128/501951]\n",
      "loss: 4477121.000000  [204928/501951]\n",
      "loss: 368710.031250  [217728/501951]\n",
      "loss: 4506118.000000  [230528/501951]\n",
      "loss: 3354291.750000  [243328/501951]\n",
      "loss: 3774933.000000  [256128/501951]\n",
      "loss: 2035986.250000  [268928/501951]\n",
      "loss: 5309312.500000  [281728/501951]\n",
      "loss: 1212320.000000  [294528/501951]\n",
      "loss: 537557.500000  [307328/501951]\n",
      "loss: 4103134.250000  [320128/501951]\n",
      "loss: 2683632.500000  [332928/501951]\n",
      "loss: 1004331.187500  [345728/501951]\n",
      "loss: 2133836.250000  [358528/501951]\n",
      "loss: 4257127.000000  [371328/501951]\n",
      "loss: 4529061.000000  [384128/501951]\n",
      "loss: 592814.937500  [396928/501951]\n",
      "loss: 6256703.500000  [409728/501951]\n",
      "loss: 3458303.500000  [422528/501951]\n",
      "loss: 727974.875000  [435328/501951]\n",
      "loss: 153096.750000  [448128/501951]\n",
      "loss: 3549565.000000  [460928/501951]\n",
      "loss: 5679703.000000  [473728/501951]\n",
      "loss: 3689227.250000  [486528/501951]\n",
      "loss: 3521504.250000  [499328/501951]\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 3703386.250000  [  128/501951]\n",
      "loss: 179667.234375  [12928/501951]\n",
      "loss: 4554916.500000  [25728/501951]\n",
      "loss: 1018095.687500  [38528/501951]\n",
      "loss: 1209810.500000  [51328/501951]\n",
      "loss: 4028514.000000  [64128/501951]\n",
      "loss: 2875976.250000  [76928/501951]\n",
      "loss: 255814.906250  [89728/501951]\n",
      "loss: 601632.687500  [102528/501951]\n",
      "loss: 2547709.500000  [115328/501951]\n",
      "loss: 1694483.000000  [128128/501951]\n",
      "loss: 2804601.000000  [140928/501951]\n",
      "loss: 2398953.500000  [153728/501951]\n",
      "loss: 2949928.500000  [166528/501951]\n",
      "loss: 4730481.000000  [179328/501951]\n",
      "loss: 243414.562500  [192128/501951]\n",
      "loss: 2935687.500000  [204928/501951]\n",
      "loss: 1266878.625000  [217728/501951]\n",
      "loss: 565598.750000  [230528/501951]\n",
      "loss: 1736769.125000  [243328/501951]\n",
      "loss: 579957.000000  [256128/501951]\n",
      "loss: 3280239.250000  [268928/501951]\n",
      "loss: 436658.625000  [281728/501951]\n",
      "loss: 769431.562500  [294528/501951]\n",
      "loss: 2107389.750000  [307328/501951]\n",
      "loss: 1680604.500000  [320128/501951]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-52-edf60f626a02>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[1;31m# Loop over batches in an epoch using DataLoader\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mid_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[0my_batch_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\stockpredict\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    516\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    517\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 518\u001b[1;33m         \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_profile_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    519\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\stockpredict\\lib\\site-packages\\torch\\autograd\\profiler.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    430\u001b[0m         \u001b[1;31m# Stores underlying RecordFunction as a tensor. TODO: move to custom\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    431\u001b[0m         \u001b[1;31m# class (https://github.com/pytorch/pytorch/issues/35026).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 432\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    433\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    434\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "sample=len(train)\n",
    "# Create the dataset with N_SAMPLES samples\n",
    "D_in, H, D_out = 32, 16, 1 #샘플 개수, 인풋값 , 중간값, 결과값 개수\n",
    "\n",
    "x_train = torch.FloatTensor(x)\n",
    "y_train = torch.FloatTensor(y)\n",
    "\n",
    "# Define the batch size and the number of epochs\n",
    "BATCH_SIZE = 128\n",
    "N_EPOCHS = 30\n",
    "\n",
    "# Use torch.utils.data to create a DataLoader \n",
    "# that will take care of creating batches \n",
    "dataset = TensorDataset(x_train, y_train)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# Define model, loss and optimizer\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out),\n",
    ")\n",
    "\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Get the dataset size for printing (it is equal to N_SAMPLES)\n",
    "dataset_size = len(dataloader.dataset)\n",
    "\n",
    "# Loop over epochs\n",
    "for epoch in range(N_EPOCHS):\n",
    "    print(f\"Epoch {epoch + 1}\\n-------------------------------\")\n",
    "\n",
    "    # Loop over batches in an epoch using DataLoader\n",
    "    for id_batch, (x_batch, y_batch) in enumerate(dataloader):\n",
    "\n",
    "        y_batch_pred = model(x_batch)\n",
    "\n",
    "        loss = loss_fn(y_batch_pred, y_batch)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Every 100 batches, print the loss for this batch\n",
    "        # as well as the number of examples processed so far \n",
    "        if id_batch % 100 == 0:\n",
    "            loss, current = loss.item(), (id_batch + 1)* len(x_batch)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{dataset_size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f68a52d6746b8edec33e9d0848c665d5f17e646007ceb6470c84aeea645ccfe6"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit ('dacon': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
