{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "import easydict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#평가 지표\n",
    "\n",
    "def f1_loss(y_true:torch.Tensor, y_pred:torch.Tensor, is_training=False) -> torch.Tensor:\n",
    "\n",
    "    assert y_true.ndim == 1\n",
    "    assert y_pred.ndim == 1 or y_pred.ndim == 2\n",
    "    \n",
    "    if y_pred.ndim == 2:\n",
    "        y_pred = y_pred.argmax(dim=1)\n",
    "        \n",
    "    \n",
    "    tp = (y_true * y_pred).sum().to(torch.float32)\n",
    "    tn = ((1 - y_true) * (1 - y_pred)).sum().to(torch.float32)\n",
    "    fp = ((1 - y_true) * y_pred).sum().to(torch.float32)\n",
    "    fn = (y_true * (1 - y_pred)).sum().to(torch.float32)\n",
    "    \n",
    "    epsilon = 1e-7\n",
    "    \n",
    "    precision = tp / (tp + fp + epsilon)\n",
    "    recall = tp / (tp + fn + epsilon)\n",
    "    \n",
    "    f1 = 2* (precision*recall) / (precision + recall + epsilon)\n",
    "    f1.requires_grad = is_training\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#데이터 불러오기\n",
    "train = pd.read_csv('C:\\\\Users\\\\user\\\\Desktop\\\\computing\\\\outside\\\\dacon\\\\jobcare_recommend\\\\JobCare_data\\\\train.csv')\n",
    "test = pd.read_csv('C:\\\\Users\\\\user\\\\Desktop\\\\computing\\\\outside\\\\dacon\\\\jobcare_recommend\\\\JobCare_data\\\\test.csv')\n",
    "d_code=pd.read_csv('C:\\\\Users\\\\user\\\\Desktop\\\\computing\\\\outside\\\\dacon\\\\jobcare_recommend\\\\Jobcare_data\\\\속성_D_코드.csv',index_col=0).T.to_dict()\n",
    "h_code=pd.read_csv('C:\\\\Users\\\\user\\\\Desktop\\\\computing\\\\outside\\\\dacon\\\\jobcare_recommend\\\\Jobcare_data\\\\속성_H_코드.csv',index_col=0).T.to_dict()\n",
    "l_code=pd.read_csv('C:\\\\Users\\\\user\\\\Desktop\\\\computing\\\\outside\\\\dacon\\\\jobcare_recommend\\\\Jobcare_data\\\\속성_L_코드.csv',index_col=0).T.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#데이터 재가공\n",
    "def add_code(df, d_code, h_code, l_code):\n",
    "    df = df.copy()   \n",
    "\n",
    "    # D Code\n",
    "    df['person_prefer_d_1_n'] = df['person_prefer_d_1'].apply(lambda x: d_code[x]['속성 D 세분류코드'])\n",
    "    df['person_prefer_d_1_s'] = df['person_prefer_d_1'].apply(lambda x: d_code[x]['속성 D 소분류코드'])\n",
    "    df['person_prefer_d_1_m'] = df['person_prefer_d_1'].apply(lambda x: d_code[x]['속성 D 중분류코드'])\n",
    "    df['person_prefer_d_1_l'] = df['person_prefer_d_1'].apply(lambda x: d_code[x]['속성 D 대분류코드'])\n",
    "\n",
    "    df['person_prefer_d_2_n'] = df['person_prefer_d_2'].apply(lambda x: d_code[x]['속성 D 세분류코드'])\n",
    "    df['person_prefer_d_2_s'] = df['person_prefer_d_2'].apply(lambda x: d_code[x]['속성 D 소분류코드'])\n",
    "    df['person_prefer_d_2_m'] = df['person_prefer_d_2'].apply(lambda x: d_code[x]['속성 D 중분류코드'])\n",
    "    df['person_prefer_d_2_l'] = df['person_prefer_d_2'].apply(lambda x: d_code[x]['속성 D 대분류코드'])\n",
    "\n",
    "    df['person_prefer_d_3_n'] = df['person_prefer_d_3'].apply(lambda x: d_code[x]['속성 D 세분류코드'])\n",
    "    df['person_prefer_d_3_s'] = df['person_prefer_d_3'].apply(lambda x: d_code[x]['속성 D 소분류코드'])\n",
    "    df['person_prefer_d_3_m'] = df['person_prefer_d_3'].apply(lambda x: d_code[x]['속성 D 중분류코드'])\n",
    "    df['person_prefer_d_3_l'] = df['person_prefer_d_3'].apply(lambda x: d_code[x]['속성 D 대분류코드'])\n",
    "\n",
    "    df['contents_attribute_d_n'] = df['contents_attribute_d'].apply(lambda x: d_code[x]['속성 D 세분류코드'])\n",
    "    df['contents_attribute_d_s'] = df['contents_attribute_d'].apply(lambda x: d_code[x]['속성 D 소분류코드'])\n",
    "    df['contents_attribute_d_m'] = df['contents_attribute_d'].apply(lambda x: d_code[x]['속성 D 중분류코드'])\n",
    "    df['contents_attribute_d_l'] = df['contents_attribute_d'].apply(lambda x: d_code[x]['속성 D 대분류코드'])\n",
    "\n",
    "    # H Code\n",
    "    df['person_prefer_h_1_m'] = df['person_prefer_h_1'].apply(lambda x: h_code[x]['속성 H 중분류코드'])\n",
    "    df['person_prefer_h_2_m'] = df['person_prefer_h_2'].apply(lambda x: h_code[x]['속성 H 중분류코드'])\n",
    "    df['person_prefer_h_3_m'] = df['person_prefer_h_3'].apply(lambda x: h_code[x]['속성 H 중분류코드'])\n",
    "    df['contents_attribute_h_m'] = df['contents_attribute_h'].apply(lambda x: h_code[x]['속성 H 중분류코드'])\n",
    "\n",
    "    df['person_prefer_h_1_l'] = df['person_prefer_h_1'].apply(lambda x: h_code[x]['속성 H 대분류코드'])\n",
    "    df['person_prefer_h_2_l'] = df['person_prefer_h_2'].apply(lambda x: h_code[x]['속성 H 대분류코드'])\n",
    "    df['person_prefer_h_3_l'] = df['person_prefer_h_3'].apply(lambda x: h_code[x]['속성 H 대분류코드'])\n",
    "    df['contents_attribute_h_l'] = df['contents_attribute_h'].apply(lambda x: h_code[x]['속성 H 대분류코드'])\n",
    "\n",
    "    # L Code\n",
    "    df['contents_attribute_l_n'] = df['contents_attribute_l'].apply(lambda x: l_code[x]['속성 L 세분류코드'])\n",
    "    df['contents_attribute_l_s'] = df['contents_attribute_l'].apply(lambda x: l_code[x]['속성 L 소분류코드'])\n",
    "    df['contents_attribute_l_m'] = df['contents_attribute_l'].apply(lambda x: l_code[x]['속성 L 중분류코드'])\n",
    "    df['contents_attribute_l_l'] = df['contents_attribute_l'].apply(lambda x: l_code[x]['속성 L 대분류코드'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "train = add_code(train, d_code, h_code, l_code)\n",
    "test = add_code(test, d_code, h_code, l_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(57,16),\n",
    "            nn.BatchNorm1d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 1)\n",
    "        )        \n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=LogisticRegression()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.drop(['id', 'contents_open_dt','person_rn', 'contents_rn'], axis=1) \n",
    "test = test.drop(['id', 'contents_open_dt','person_rn', 'contents_rn'], axis=1)\n",
    "\n",
    "# print(train.head())\n",
    "train['d_l_match_yn']=train['d_l_match_yn'].replace([True,False],[1,0])\n",
    "train['d_m_match_yn']=train['d_m_match_yn'].replace([True,False],[1,0])\n",
    "train['d_s_match_yn']=train['d_s_match_yn'].replace([True,False],[1,0])\n",
    "train['h_l_match_yn']=train['h_l_match_yn'].replace([True,False],[1,0])\n",
    "train['h_m_match_yn']=train['h_m_match_yn'].replace([True,False],[1,0])\n",
    "train['h_s_match_yn']=train['h_s_match_yn'].replace([True,False],[1,0])\n",
    "\n",
    "test['d_l_match_yn']=test['d_l_match_yn'].replace([True,False],[1,0])\n",
    "test['d_m_match_yn']=test['d_m_match_yn'].replace([True,False],[1,0])\n",
    "test['d_s_match_yn']=test['d_s_match_yn'].replace([True,False],[1,0])\n",
    "test['h_l_match_yn']=test['h_l_match_yn'].replace([True,False],[1,0])\n",
    "test['h_m_match_yn']=test['h_m_match_yn'].replace([True,False],[1,0])\n",
    "test['h_s_match_yn']=test['h_s_match_yn'].replace([True,False],[1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#컬럼 순서 변경\n",
    "train.columns\n",
    "train=train[['d_l_match_yn', 'd_m_match_yn', 'd_s_match_yn', 'h_l_match_yn',\n",
    "       'h_m_match_yn', 'h_s_match_yn', 'person_attribute_a',\n",
    "       'person_attribute_a_1', 'person_attribute_b', 'person_prefer_c',\n",
    "       'person_prefer_d_1', 'person_prefer_d_2', 'person_prefer_d_3',\n",
    "       'person_prefer_e', 'person_prefer_f', 'person_prefer_g',\n",
    "       'person_prefer_h_1', 'person_prefer_h_2', 'person_prefer_h_3',\n",
    "       'contents_attribute_i', 'contents_attribute_a',\n",
    "       'contents_attribute_j_1', 'contents_attribute_j',\n",
    "       'contents_attribute_c', 'contents_attribute_k', 'contents_attribute_l',\n",
    "       'contents_attribute_d', 'contents_attribute_m', 'contents_attribute_e',\n",
    "       'contents_attribute_h', 'person_prefer_d_1_n',\n",
    "       'person_prefer_d_1_s', 'person_prefer_d_1_m', 'person_prefer_d_1_l',\n",
    "       'person_prefer_d_2_n', 'person_prefer_d_2_s', 'person_prefer_d_2_m',\n",
    "       'person_prefer_d_2_l', 'person_prefer_d_3_n', 'person_prefer_d_3_s',\n",
    "       'person_prefer_d_3_m', 'person_prefer_d_3_l', 'contents_attribute_d_n',\n",
    "       'contents_attribute_d_s', 'contents_attribute_d_m',\n",
    "       'contents_attribute_d_l', 'person_prefer_h_1_m', 'person_prefer_h_2_m',\n",
    "       'person_prefer_h_3_m', 'contents_attribute_h_m', 'person_prefer_h_1_l',\n",
    "       'person_prefer_h_2_l', 'person_prefer_h_3_l', 'contents_attribute_h_l',\n",
    "       'contents_attribute_l_n', 'contents_attribute_l_s',\n",
    "       'contents_attribute_l_m', 'contents_attribute_l_l','target']]\n",
    "\n",
    "test=test[['d_l_match_yn', 'd_m_match_yn', 'd_s_match_yn', 'h_l_match_yn',\n",
    "       'h_m_match_yn', 'h_s_match_yn', 'person_attribute_a',\n",
    "       'person_attribute_a_1', 'person_attribute_b', 'person_prefer_c',\n",
    "       'person_prefer_d_1', 'person_prefer_d_2', 'person_prefer_d_3',\n",
    "       'person_prefer_e', 'person_prefer_f', 'person_prefer_g',\n",
    "       'person_prefer_h_1', 'person_prefer_h_2', 'person_prefer_h_3',\n",
    "       'contents_attribute_i', 'contents_attribute_a',\n",
    "       'contents_attribute_j_1', 'contents_attribute_j',\n",
    "       'contents_attribute_c', 'contents_attribute_k', 'contents_attribute_l',\n",
    "       'contents_attribute_d', 'contents_attribute_m', 'contents_attribute_e',\n",
    "       'contents_attribute_h', 'person_prefer_d_1_n',\n",
    "       'person_prefer_d_1_s', 'person_prefer_d_1_m', 'person_prefer_d_1_l',\n",
    "       'person_prefer_d_2_n', 'person_prefer_d_2_s', 'person_prefer_d_2_m',\n",
    "       'person_prefer_d_2_l', 'person_prefer_d_3_n', 'person_prefer_d_3_s',\n",
    "       'person_prefer_d_3_m', 'person_prefer_d_3_l', 'contents_attribute_d_n',\n",
    "       'contents_attribute_d_s', 'contents_attribute_d_m',\n",
    "       'contents_attribute_d_l', 'person_prefer_h_1_m', 'person_prefer_h_2_m',\n",
    "       'person_prefer_h_3_m', 'contents_attribute_h_m', 'person_prefer_h_1_l',\n",
    "       'person_prefer_h_2_l', 'person_prefer_h_3_l', 'contents_attribute_h_l',\n",
    "       'contents_attribute_l_n', 'contents_attribute_l_s',\n",
    "       'contents_attribute_l_m', 'contents_attribute_l_l']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(46404, 58)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "\n",
    "train.shape\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   1    1    1    0    0    0    1    4    3    5  275  370  369    8\n",
      "    1    1    4   95   59    3    3   10    2    1    2 1608  275    1\n",
      "    4  139  275  274  274  216  369  368  297  216  369  368  297  216\n",
      "  275  274  274  216  316  398  368  422    3   94   58   94 1607 1606\n",
      " 1605 2016]\n",
      "1\n",
      "torch.Size([1000, 58])\n",
      "tensor(0.0100)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "x = train.iloc[0:1000, :-1]\n",
    "y = train.iloc[0:1000, -1]\n",
    "\n",
    "# print(x)\n",
    "# print(y)\n",
    "\n",
    "\n",
    "x= x.to_numpy()\n",
    "print(x[0])\n",
    "print(y[0])\n",
    "x=np.where(x > 1000, x/1000, x)\n",
    "x=np.where(x > 100, x/100, x)\n",
    "x=np.where(x > 10, x/10, x)\n",
    "x=x/100\n",
    "x=torch.Tensor(x)\n",
    "print(x.size())\n",
    "print(x[0][1])\n",
    "y=torch.Tensor(y)\n",
    "# print(y)\n",
    "\n",
    "\n",
    "dataset = TensorDataset(x, y)\n",
    "dataloader = DataLoader(dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "\n",
    "x_test=test.iloc[0:1000]\n",
    "x_test=x_test.to_numpy()\n",
    "x_test=np.where(x_test > 1000, x_test/1000, x_test)\n",
    "x_test=np.where(x_test > 100, x_test/100, x_test)\n",
    "x_test=np.where(x_test > 10, x_test/10, x_test)\n",
    "x_test=x_test/100\n",
    "\n",
    "\n",
    "x_test=torch.FloatTensor(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BinaryClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(58, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.sigmoid(self.linear(x))\n",
    "\n",
    "model = BinaryClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 Batch 1/8 Cost: 0.707902 Accuracy 40.62%\n",
      "Epoch    0/1000 Batch 2/8 Cost: 0.708412 Accuracy 38.28%\n",
      "Epoch    0/1000 Batch 3/8 Cost: 0.701929 Accuracy 44.53%\n",
      "Epoch    0/1000 Batch 4/8 Cost: 0.708581 Accuracy 39.84%\n",
      "Epoch    0/1000 Batch 5/8 Cost: 0.706997 Accuracy 39.84%\n",
      "Epoch    0/1000 Batch 6/8 Cost: 0.705432 Accuracy 42.19%\n",
      "Epoch    0/1000 Batch 7/8 Cost: 0.709250 Accuracy 39.06%\n",
      "Epoch    0/1000 Batch 8/8 Cost: 0.715065 Accuracy 34.62%\n",
      "Epoch   10/1000 Batch 1/8 Cost: 0.706732 Accuracy 39.84%\n",
      "Epoch   10/1000 Batch 2/8 Cost: 0.699776 Accuracy 45.31%\n",
      "Epoch   10/1000 Batch 3/8 Cost: 0.709759 Accuracy 37.50%\n",
      "Epoch   10/1000 Batch 4/8 Cost: 0.711419 Accuracy 35.94%\n",
      "Epoch   10/1000 Batch 5/8 Cost: 0.712630 Accuracy 34.38%\n",
      "Epoch   10/1000 Batch 6/8 Cost: 0.699804 Accuracy 45.31%\n",
      "Epoch   10/1000 Batch 7/8 Cost: 0.705289 Accuracy 40.62%\n",
      "Epoch   10/1000 Batch 8/8 Cost: 0.704654 Accuracy 41.35%\n",
      "Epoch   20/1000 Batch 1/8 Cost: 0.704501 Accuracy 39.84%\n",
      "Epoch   20/1000 Batch 2/8 Cost: 0.702923 Accuracy 41.41%\n",
      "Epoch   20/1000 Batch 3/8 Cost: 0.704045 Accuracy 40.62%\n",
      "Epoch   20/1000 Batch 4/8 Cost: 0.710868 Accuracy 34.38%\n",
      "Epoch   20/1000 Batch 5/8 Cost: 0.703827 Accuracy 41.41%\n",
      "Epoch   20/1000 Batch 6/8 Cost: 0.707052 Accuracy 38.28%\n",
      "Epoch   20/1000 Batch 7/8 Cost: 0.701930 Accuracy 42.19%\n",
      "Epoch   20/1000 Batch 8/8 Cost: 0.703688 Accuracy 42.31%\n",
      "Epoch   30/1000 Batch 1/8 Cost: 0.701933 Accuracy 40.62%\n",
      "Epoch   30/1000 Batch 2/8 Cost: 0.703133 Accuracy 41.41%\n",
      "Epoch   30/1000 Batch 3/8 Cost: 0.702710 Accuracy 41.41%\n",
      "Epoch   30/1000 Batch 4/8 Cost: 0.703725 Accuracy 39.06%\n",
      "Epoch   30/1000 Batch 5/8 Cost: 0.703550 Accuracy 39.84%\n",
      "Epoch   30/1000 Batch 6/8 Cost: 0.702013 Accuracy 42.19%\n",
      "Epoch   30/1000 Batch 7/8 Cost: 0.711272 Accuracy 32.03%\n",
      "Epoch   30/1000 Batch 8/8 Cost: 0.699038 Accuracy 44.23%\n",
      "Epoch   40/1000 Batch 1/8 Cost: 0.704490 Accuracy 37.50%\n",
      "Epoch   40/1000 Batch 2/8 Cost: 0.707093 Accuracy 35.16%\n",
      "Epoch   40/1000 Batch 3/8 Cost: 0.704318 Accuracy 37.50%\n",
      "Epoch   40/1000 Batch 4/8 Cost: 0.699510 Accuracy 42.97%\n",
      "Epoch   40/1000 Batch 5/8 Cost: 0.693821 Accuracy 49.22%\n",
      "Epoch   40/1000 Batch 6/8 Cost: 0.697417 Accuracy 45.31%\n",
      "Epoch   40/1000 Batch 7/8 Cost: 0.701296 Accuracy 41.41%\n",
      "Epoch   40/1000 Batch 8/8 Cost: 0.711681 Accuracy 28.85%\n",
      "Epoch   50/1000 Batch 1/8 Cost: 0.702428 Accuracy 38.28%\n",
      "Epoch   50/1000 Batch 2/8 Cost: 0.702117 Accuracy 39.84%\n",
      "Epoch   50/1000 Batch 3/8 Cost: 0.703133 Accuracy 36.72%\n",
      "Epoch   50/1000 Batch 4/8 Cost: 0.704130 Accuracy 35.16%\n",
      "Epoch   50/1000 Batch 5/8 Cost: 0.702390 Accuracy 38.28%\n",
      "Epoch   50/1000 Batch 6/8 Cost: 0.699778 Accuracy 42.19%\n",
      "Epoch   50/1000 Batch 7/8 Cost: 0.694046 Accuracy 48.44%\n",
      "Epoch   50/1000 Batch 8/8 Cost: 0.699639 Accuracy 41.35%\n",
      "Epoch   60/1000 Batch 1/8 Cost: 0.699524 Accuracy 39.84%\n",
      "Epoch   60/1000 Batch 2/8 Cost: 0.701028 Accuracy 39.84%\n",
      "Epoch   60/1000 Batch 3/8 Cost: 0.697561 Accuracy 42.97%\n",
      "Epoch   60/1000 Batch 4/8 Cost: 0.702665 Accuracy 36.72%\n",
      "Epoch   60/1000 Batch 5/8 Cost: 0.702737 Accuracy 35.94%\n",
      "Epoch   60/1000 Batch 6/8 Cost: 0.699593 Accuracy 41.41%\n",
      "Epoch   60/1000 Batch 7/8 Cost: 0.696175 Accuracy 42.97%\n",
      "Epoch   60/1000 Batch 8/8 Cost: 0.698935 Accuracy 40.38%\n",
      "Epoch   70/1000 Batch 1/8 Cost: 0.698744 Accuracy 39.84%\n",
      "Epoch   70/1000 Batch 2/8 Cost: 0.696582 Accuracy 44.53%\n",
      "Epoch   70/1000 Batch 3/8 Cost: 0.699373 Accuracy 38.28%\n",
      "Epoch   70/1000 Batch 4/8 Cost: 0.698983 Accuracy 39.84%\n",
      "Epoch   70/1000 Batch 5/8 Cost: 0.695196 Accuracy 45.31%\n",
      "Epoch   70/1000 Batch 6/8 Cost: 0.696828 Accuracy 44.53%\n",
      "Epoch   70/1000 Batch 7/8 Cost: 0.701427 Accuracy 34.38%\n",
      "Epoch   70/1000 Batch 8/8 Cost: 0.702965 Accuracy 31.73%\n",
      "Epoch   80/1000 Batch 1/8 Cost: 0.694845 Accuracy 44.53%\n",
      "Epoch   80/1000 Batch 2/8 Cost: 0.697235 Accuracy 42.19%\n",
      "Epoch   80/1000 Batch 3/8 Cost: 0.700849 Accuracy 30.47%\n",
      "Epoch   80/1000 Batch 4/8 Cost: 0.696847 Accuracy 40.62%\n",
      "Epoch   80/1000 Batch 5/8 Cost: 0.700524 Accuracy 35.16%\n",
      "Epoch   80/1000 Batch 6/8 Cost: 0.695552 Accuracy 46.09%\n",
      "Epoch   80/1000 Batch 7/8 Cost: 0.698014 Accuracy 39.06%\n",
      "Epoch   80/1000 Batch 8/8 Cost: 0.696506 Accuracy 42.31%\n",
      "Epoch   90/1000 Batch 1/8 Cost: 0.695860 Accuracy 39.84%\n",
      "Epoch   90/1000 Batch 2/8 Cost: 0.694774 Accuracy 42.97%\n",
      "Epoch   90/1000 Batch 3/8 Cost: 0.693573 Accuracy 48.44%\n",
      "Epoch   90/1000 Batch 4/8 Cost: 0.699604 Accuracy 33.59%\n",
      "Epoch   90/1000 Batch 5/8 Cost: 0.696666 Accuracy 41.41%\n",
      "Epoch   90/1000 Batch 6/8 Cost: 0.694798 Accuracy 44.53%\n",
      "Epoch   90/1000 Batch 7/8 Cost: 0.698205 Accuracy 35.94%\n",
      "Epoch   90/1000 Batch 8/8 Cost: 0.699261 Accuracy 31.73%\n",
      "Epoch  100/1000 Batch 1/8 Cost: 0.695511 Accuracy 38.28%\n",
      "Epoch  100/1000 Batch 2/8 Cost: 0.694241 Accuracy 47.66%\n",
      "Epoch  100/1000 Batch 3/8 Cost: 0.694044 Accuracy 44.53%\n",
      "Epoch  100/1000 Batch 4/8 Cost: 0.697777 Accuracy 33.59%\n",
      "Epoch  100/1000 Batch 5/8 Cost: 0.694885 Accuracy 44.53%\n",
      "Epoch  100/1000 Batch 6/8 Cost: 0.695065 Accuracy 41.41%\n",
      "Epoch  100/1000 Batch 7/8 Cost: 0.695309 Accuracy 41.41%\n",
      "Epoch  100/1000 Batch 8/8 Cost: 0.697832 Accuracy 35.58%\n",
      "Epoch  110/1000 Batch 1/8 Cost: 0.696358 Accuracy 34.38%\n",
      "Epoch  110/1000 Batch 2/8 Cost: 0.695046 Accuracy 43.75%\n",
      "Epoch  110/1000 Batch 3/8 Cost: 0.695217 Accuracy 38.28%\n",
      "Epoch  110/1000 Batch 4/8 Cost: 0.693177 Accuracy 50.00%\n",
      "Epoch  110/1000 Batch 5/8 Cost: 0.693826 Accuracy 45.31%\n",
      "Epoch  110/1000 Batch 6/8 Cost: 0.694861 Accuracy 43.75%\n",
      "Epoch  110/1000 Batch 7/8 Cost: 0.694249 Accuracy 46.88%\n",
      "Epoch  110/1000 Batch 8/8 Cost: 0.693664 Accuracy 46.15%\n",
      "Epoch  120/1000 Batch 1/8 Cost: 0.694528 Accuracy 45.31%\n",
      "Epoch  120/1000 Batch 2/8 Cost: 0.693445 Accuracy 46.88%\n",
      "Epoch  120/1000 Batch 3/8 Cost: 0.693524 Accuracy 53.12%\n",
      "Epoch  120/1000 Batch 4/8 Cost: 0.693311 Accuracy 50.00%\n",
      "Epoch  120/1000 Batch 5/8 Cost: 0.693787 Accuracy 45.31%\n",
      "Epoch  120/1000 Batch 6/8 Cost: 0.693024 Accuracy 53.12%\n",
      "Epoch  120/1000 Batch 7/8 Cost: 0.693983 Accuracy 42.97%\n",
      "Epoch  120/1000 Batch 8/8 Cost: 0.693585 Accuracy 45.19%\n",
      "Epoch  130/1000 Batch 1/8 Cost: 0.693262 Accuracy 49.22%\n",
      "Epoch  130/1000 Batch 2/8 Cost: 0.692516 Accuracy 54.69%\n",
      "Epoch  130/1000 Batch 3/8 Cost: 0.691820 Accuracy 57.81%\n",
      "Epoch  130/1000 Batch 4/8 Cost: 0.692367 Accuracy 59.38%\n",
      "Epoch  130/1000 Batch 5/8 Cost: 0.693702 Accuracy 48.44%\n",
      "Epoch  130/1000 Batch 6/8 Cost: 0.693138 Accuracy 45.31%\n",
      "Epoch  130/1000 Batch 7/8 Cost: 0.692527 Accuracy 53.91%\n",
      "Epoch  130/1000 Batch 8/8 Cost: 0.692843 Accuracy 52.88%\n",
      "Epoch  140/1000 Batch 1/8 Cost: 0.692174 Accuracy 55.47%\n",
      "Epoch  140/1000 Batch 2/8 Cost: 0.692318 Accuracy 53.91%\n",
      "Epoch  140/1000 Batch 3/8 Cost: 0.692871 Accuracy 53.91%\n",
      "Epoch  140/1000 Batch 4/8 Cost: 0.692273 Accuracy 52.34%\n",
      "Epoch  140/1000 Batch 5/8 Cost: 0.691545 Accuracy 60.94%\n",
      "Epoch  140/1000 Batch 6/8 Cost: 0.690920 Accuracy 59.38%\n",
      "Epoch  140/1000 Batch 7/8 Cost: 0.691557 Accuracy 56.25%\n",
      "Epoch  140/1000 Batch 8/8 Cost: 0.691715 Accuracy 56.73%\n",
      "Epoch  150/1000 Batch 1/8 Cost: 0.691904 Accuracy 50.78%\n",
      "Epoch  150/1000 Batch 2/8 Cost: 0.690499 Accuracy 65.62%\n",
      "Epoch  150/1000 Batch 3/8 Cost: 0.692321 Accuracy 51.56%\n",
      "Epoch  150/1000 Batch 4/8 Cost: 0.690203 Accuracy 62.50%\n",
      "Epoch  150/1000 Batch 5/8 Cost: 0.690236 Accuracy 64.84%\n",
      "Epoch  150/1000 Batch 6/8 Cost: 0.690343 Accuracy 62.50%\n",
      "Epoch  150/1000 Batch 7/8 Cost: 0.691686 Accuracy 56.25%\n",
      "Epoch  150/1000 Batch 8/8 Cost: 0.691958 Accuracy 53.85%\n",
      "Epoch  160/1000 Batch 1/8 Cost: 0.692198 Accuracy 49.22%\n",
      "Epoch  160/1000 Batch 2/8 Cost: 0.690909 Accuracy 57.03%\n",
      "Epoch  160/1000 Batch 3/8 Cost: 0.687935 Accuracy 67.19%\n",
      "Epoch  160/1000 Batch 4/8 Cost: 0.691085 Accuracy 59.38%\n",
      "Epoch  160/1000 Batch 5/8 Cost: 0.691341 Accuracy 56.25%\n",
      "Epoch  160/1000 Batch 6/8 Cost: 0.689278 Accuracy 62.50%\n",
      "Epoch  160/1000 Batch 7/8 Cost: 0.689675 Accuracy 61.72%\n",
      "Epoch  160/1000 Batch 8/8 Cost: 0.690395 Accuracy 62.50%\n",
      "Epoch  170/1000 Batch 1/8 Cost: 0.688467 Accuracy 62.50%\n",
      "Epoch  170/1000 Batch 2/8 Cost: 0.691596 Accuracy 53.91%\n",
      "Epoch  170/1000 Batch 3/8 Cost: 0.690518 Accuracy 56.25%\n",
      "Epoch  170/1000 Batch 4/8 Cost: 0.689855 Accuracy 58.59%\n",
      "Epoch  170/1000 Batch 5/8 Cost: 0.687786 Accuracy 63.28%\n",
      "Epoch  170/1000 Batch 6/8 Cost: 0.692023 Accuracy 53.91%\n",
      "Epoch  170/1000 Batch 7/8 Cost: 0.688637 Accuracy 63.28%\n",
      "Epoch  170/1000 Batch 8/8 Cost: 0.687662 Accuracy 68.27%\n",
      "Epoch  180/1000 Batch 1/8 Cost: 0.688524 Accuracy 59.38%\n",
      "Epoch  180/1000 Batch 2/8 Cost: 0.690375 Accuracy 58.59%\n",
      "Epoch  180/1000 Batch 3/8 Cost: 0.690071 Accuracy 57.81%\n",
      "Epoch  180/1000 Batch 4/8 Cost: 0.689624 Accuracy 59.38%\n",
      "Epoch  180/1000 Batch 5/8 Cost: 0.689938 Accuracy 55.47%\n",
      "Epoch  180/1000 Batch 6/8 Cost: 0.687453 Accuracy 63.28%\n",
      "Epoch  180/1000 Batch 7/8 Cost: 0.688453 Accuracy 60.94%\n",
      "Epoch  180/1000 Batch 8/8 Cost: 0.686333 Accuracy 66.35%\n",
      "Epoch  190/1000 Batch 1/8 Cost: 0.686055 Accuracy 65.62%\n",
      "Epoch  190/1000 Batch 2/8 Cost: 0.689655 Accuracy 57.03%\n",
      "Epoch  190/1000 Batch 3/8 Cost: 0.688682 Accuracy 59.38%\n",
      "Epoch  190/1000 Batch 4/8 Cost: 0.687335 Accuracy 62.50%\n",
      "Epoch  190/1000 Batch 5/8 Cost: 0.688460 Accuracy 57.81%\n",
      "Epoch  190/1000 Batch 6/8 Cost: 0.689061 Accuracy 57.03%\n",
      "Epoch  190/1000 Batch 7/8 Cost: 0.685515 Accuracy 66.41%\n",
      "Epoch  190/1000 Batch 8/8 Cost: 0.691770 Accuracy 52.88%\n",
      "Epoch  200/1000 Batch 1/8 Cost: 0.686820 Accuracy 60.16%\n",
      "Epoch  200/1000 Batch 2/8 Cost: 0.688445 Accuracy 57.81%\n",
      "Epoch  200/1000 Batch 3/8 Cost: 0.687152 Accuracy 61.72%\n",
      "Epoch  200/1000 Batch 4/8 Cost: 0.686473 Accuracy 62.50%\n",
      "Epoch  200/1000 Batch 5/8 Cost: 0.687788 Accuracy 59.38%\n",
      "Epoch  200/1000 Batch 6/8 Cost: 0.688602 Accuracy 59.38%\n",
      "Epoch  200/1000 Batch 7/8 Cost: 0.687434 Accuracy 60.16%\n",
      "Epoch  200/1000 Batch 8/8 Cost: 0.688069 Accuracy 58.65%\n",
      "Epoch  210/1000 Batch 1/8 Cost: 0.687381 Accuracy 58.59%\n",
      "Epoch  210/1000 Batch 2/8 Cost: 0.687286 Accuracy 60.16%\n",
      "Epoch  210/1000 Batch 3/8 Cost: 0.684767 Accuracy 64.06%\n",
      "Epoch  210/1000 Batch 4/8 Cost: 0.693562 Accuracy 50.00%\n",
      "Epoch  210/1000 Batch 5/8 Cost: 0.686777 Accuracy 60.94%\n",
      "Epoch  210/1000 Batch 6/8 Cost: 0.684475 Accuracy 63.28%\n",
      "Epoch  210/1000 Batch 7/8 Cost: 0.682742 Accuracy 66.41%\n",
      "Epoch  210/1000 Batch 8/8 Cost: 0.689179 Accuracy 55.77%\n",
      "Epoch  220/1000 Batch 1/8 Cost: 0.682517 Accuracy 65.62%\n",
      "Epoch  220/1000 Batch 2/8 Cost: 0.685464 Accuracy 60.16%\n",
      "Epoch  220/1000 Batch 3/8 Cost: 0.689008 Accuracy 57.03%\n",
      "Epoch  220/1000 Batch 4/8 Cost: 0.684887 Accuracy 63.28%\n",
      "Epoch  220/1000 Batch 5/8 Cost: 0.683279 Accuracy 64.06%\n",
      "Epoch  220/1000 Batch 6/8 Cost: 0.687159 Accuracy 57.81%\n",
      "Epoch  220/1000 Batch 7/8 Cost: 0.690465 Accuracy 53.91%\n",
      "Epoch  220/1000 Batch 8/8 Cost: 0.688643 Accuracy 57.69%\n",
      "Epoch  230/1000 Batch 1/8 Cost: 0.693208 Accuracy 50.78%\n",
      "Epoch  230/1000 Batch 2/8 Cost: 0.689026 Accuracy 55.47%\n",
      "Epoch  230/1000 Batch 3/8 Cost: 0.685934 Accuracy 59.38%\n",
      "Epoch  230/1000 Batch 4/8 Cost: 0.687177 Accuracy 58.59%\n",
      "Epoch  230/1000 Batch 5/8 Cost: 0.683153 Accuracy 63.28%\n",
      "Epoch  230/1000 Batch 6/8 Cost: 0.679342 Accuracy 68.75%\n",
      "Epoch  230/1000 Batch 7/8 Cost: 0.682675 Accuracy 64.06%\n",
      "Epoch  230/1000 Batch 8/8 Cost: 0.685989 Accuracy 59.62%\n",
      "Epoch  240/1000 Batch 1/8 Cost: 0.692853 Accuracy 51.56%\n",
      "Epoch  240/1000 Batch 2/8 Cost: 0.685223 Accuracy 59.38%\n",
      "Epoch  240/1000 Batch 3/8 Cost: 0.684518 Accuracy 60.94%\n",
      "Epoch  240/1000 Batch 4/8 Cost: 0.684091 Accuracy 61.72%\n",
      "Epoch  240/1000 Batch 5/8 Cost: 0.682274 Accuracy 64.06%\n",
      "Epoch  240/1000 Batch 6/8 Cost: 0.683644 Accuracy 61.72%\n",
      "Epoch  240/1000 Batch 7/8 Cost: 0.685349 Accuracy 59.38%\n",
      "Epoch  240/1000 Batch 8/8 Cost: 0.683932 Accuracy 61.54%\n",
      "Epoch  250/1000 Batch 1/8 Cost: 0.681018 Accuracy 64.06%\n",
      "Epoch  250/1000 Batch 2/8 Cost: 0.684224 Accuracy 60.16%\n",
      "Epoch  250/1000 Batch 3/8 Cost: 0.691254 Accuracy 53.12%\n",
      "Epoch  250/1000 Batch 4/8 Cost: 0.683018 Accuracy 61.72%\n",
      "Epoch  250/1000 Batch 5/8 Cost: 0.686739 Accuracy 58.59%\n",
      "Epoch  250/1000 Batch 6/8 Cost: 0.690348 Accuracy 54.69%\n",
      "Epoch  250/1000 Batch 7/8 Cost: 0.683462 Accuracy 60.94%\n",
      "Epoch  250/1000 Batch 8/8 Cost: 0.676323 Accuracy 68.27%\n",
      "Epoch  260/1000 Batch 1/8 Cost: 0.677941 Accuracy 66.41%\n",
      "Epoch  260/1000 Batch 2/8 Cost: 0.690754 Accuracy 54.69%\n",
      "Epoch  260/1000 Batch 3/8 Cost: 0.684695 Accuracy 59.38%\n",
      "Epoch  260/1000 Batch 4/8 Cost: 0.684261 Accuracy 60.16%\n",
      "Epoch  260/1000 Batch 5/8 Cost: 0.683994 Accuracy 59.38%\n",
      "Epoch  260/1000 Batch 6/8 Cost: 0.683990 Accuracy 60.94%\n",
      "Epoch  260/1000 Batch 7/8 Cost: 0.680780 Accuracy 63.28%\n",
      "Epoch  260/1000 Batch 8/8 Cost: 0.688327 Accuracy 54.81%\n",
      "Epoch  270/1000 Batch 1/8 Cost: 0.683219 Accuracy 60.94%\n",
      "Epoch  270/1000 Batch 2/8 Cost: 0.684582 Accuracy 59.38%\n",
      "Epoch  270/1000 Batch 3/8 Cost: 0.681824 Accuracy 61.72%\n",
      "Epoch  270/1000 Batch 4/8 Cost: 0.690155 Accuracy 53.91%\n",
      "Epoch  270/1000 Batch 5/8 Cost: 0.682887 Accuracy 60.94%\n",
      "Epoch  270/1000 Batch 6/8 Cost: 0.677455 Accuracy 65.62%\n",
      "Epoch  270/1000 Batch 7/8 Cost: 0.680701 Accuracy 62.50%\n",
      "Epoch  270/1000 Batch 8/8 Cost: 0.690610 Accuracy 53.85%\n",
      "Epoch  280/1000 Batch 1/8 Cost: 0.686333 Accuracy 57.03%\n",
      "Epoch  280/1000 Batch 2/8 Cost: 0.675129 Accuracy 66.41%\n",
      "Epoch  280/1000 Batch 3/8 Cost: 0.681414 Accuracy 62.50%\n",
      "Epoch  280/1000 Batch 4/8 Cost: 0.679521 Accuracy 64.06%\n",
      "Epoch  280/1000 Batch 5/8 Cost: 0.687197 Accuracy 57.03%\n",
      "Epoch  280/1000 Batch 6/8 Cost: 0.686684 Accuracy 57.03%\n",
      "Epoch  280/1000 Batch 7/8 Cost: 0.686053 Accuracy 57.03%\n",
      "Epoch  280/1000 Batch 8/8 Cost: 0.684345 Accuracy 58.65%\n",
      "Epoch  290/1000 Batch 1/8 Cost: 0.681709 Accuracy 60.16%\n",
      "Epoch  290/1000 Batch 2/8 Cost: 0.685226 Accuracy 58.59%\n",
      "Epoch  290/1000 Batch 3/8 Cost: 0.690269 Accuracy 53.91%\n",
      "Epoch  290/1000 Batch 4/8 Cost: 0.675486 Accuracy 65.62%\n",
      "Epoch  290/1000 Batch 5/8 Cost: 0.681027 Accuracy 61.72%\n",
      "Epoch  290/1000 Batch 6/8 Cost: 0.670975 Accuracy 70.31%\n",
      "Epoch  290/1000 Batch 7/8 Cost: 0.690905 Accuracy 53.12%\n",
      "Epoch  290/1000 Batch 8/8 Cost: 0.688433 Accuracy 55.77%\n",
      "Epoch  300/1000 Batch 1/8 Cost: 0.693425 Accuracy 51.56%\n",
      "Epoch  300/1000 Batch 2/8 Cost: 0.674338 Accuracy 66.41%\n",
      "Epoch  300/1000 Batch 3/8 Cost: 0.691492 Accuracy 53.12%\n",
      "Epoch  300/1000 Batch 4/8 Cost: 0.676022 Accuracy 64.06%\n",
      "Epoch  300/1000 Batch 5/8 Cost: 0.691660 Accuracy 52.34%\n",
      "Epoch  300/1000 Batch 6/8 Cost: 0.674802 Accuracy 66.41%\n",
      "Epoch  300/1000 Batch 7/8 Cost: 0.675193 Accuracy 66.41%\n",
      "Epoch  300/1000 Batch 8/8 Cost: 0.682765 Accuracy 59.62%\n",
      "Epoch  310/1000 Batch 1/8 Cost: 0.688640 Accuracy 55.47%\n",
      "Epoch  310/1000 Batch 2/8 Cost: 0.677816 Accuracy 63.28%\n",
      "Epoch  310/1000 Batch 3/8 Cost: 0.683722 Accuracy 58.59%\n",
      "Epoch  310/1000 Batch 4/8 Cost: 0.688444 Accuracy 54.69%\n",
      "Epoch  310/1000 Batch 5/8 Cost: 0.677724 Accuracy 63.28%\n",
      "Epoch  310/1000 Batch 6/8 Cost: 0.674742 Accuracy 65.62%\n",
      "Epoch  310/1000 Batch 7/8 Cost: 0.686278 Accuracy 57.03%\n",
      "Epoch  310/1000 Batch 8/8 Cost: 0.678349 Accuracy 62.50%\n",
      "Epoch  320/1000 Batch 1/8 Cost: 0.682027 Accuracy 59.38%\n",
      "Epoch  320/1000 Batch 2/8 Cost: 0.679313 Accuracy 61.72%\n",
      "Epoch  320/1000 Batch 3/8 Cost: 0.691079 Accuracy 53.12%\n",
      "Epoch  320/1000 Batch 4/8 Cost: 0.674142 Accuracy 65.62%\n",
      "Epoch  320/1000 Batch 5/8 Cost: 0.669309 Accuracy 68.75%\n",
      "Epoch  320/1000 Batch 6/8 Cost: 0.693850 Accuracy 51.56%\n",
      "Epoch  320/1000 Batch 7/8 Cost: 0.683437 Accuracy 58.59%\n",
      "Epoch  320/1000 Batch 8/8 Cost: 0.679765 Accuracy 61.54%\n",
      "Epoch  330/1000 Batch 1/8 Cost: 0.676581 Accuracy 63.28%\n",
      "Epoch  330/1000 Batch 2/8 Cost: 0.680196 Accuracy 60.94%\n",
      "Epoch  330/1000 Batch 3/8 Cost: 0.680401 Accuracy 60.16%\n",
      "Epoch  330/1000 Batch 4/8 Cost: 0.677588 Accuracy 62.50%\n",
      "Epoch  330/1000 Batch 5/8 Cost: 0.684603 Accuracy 57.81%\n",
      "Epoch  330/1000 Batch 6/8 Cost: 0.677181 Accuracy 63.28%\n",
      "Epoch  330/1000 Batch 7/8 Cost: 0.684316 Accuracy 57.81%\n",
      "Epoch  330/1000 Batch 8/8 Cost: 0.691335 Accuracy 52.88%\n",
      "Epoch  340/1000 Batch 1/8 Cost: 0.681954 Accuracy 59.38%\n",
      "Epoch  340/1000 Batch 2/8 Cost: 0.681352 Accuracy 59.38%\n",
      "Epoch  340/1000 Batch 3/8 Cost: 0.672529 Accuracy 66.41%\n",
      "Epoch  340/1000 Batch 4/8 Cost: 0.677179 Accuracy 62.50%\n",
      "Epoch  340/1000 Batch 5/8 Cost: 0.684472 Accuracy 57.81%\n",
      "Epoch  340/1000 Batch 6/8 Cost: 0.685678 Accuracy 57.03%\n",
      "Epoch  340/1000 Batch 7/8 Cost: 0.673173 Accuracy 64.84%\n",
      "Epoch  340/1000 Batch 8/8 Cost: 0.693541 Accuracy 50.96%\n",
      "Epoch  350/1000 Batch 1/8 Cost: 0.682134 Accuracy 59.38%\n",
      "Epoch  350/1000 Batch 2/8 Cost: 0.679081 Accuracy 60.94%\n",
      "Epoch  350/1000 Batch 3/8 Cost: 0.672546 Accuracy 64.84%\n",
      "Epoch  350/1000 Batch 4/8 Cost: 0.680547 Accuracy 60.16%\n",
      "Epoch  350/1000 Batch 5/8 Cost: 0.676825 Accuracy 62.50%\n",
      "Epoch  350/1000 Batch 6/8 Cost: 0.689196 Accuracy 53.91%\n",
      "Epoch  350/1000 Batch 7/8 Cost: 0.676899 Accuracy 62.50%\n",
      "Epoch  350/1000 Batch 8/8 Cost: 0.689198 Accuracy 54.81%\n",
      "Epoch  360/1000 Batch 1/8 Cost: 0.668324 Accuracy 67.19%\n",
      "Epoch  360/1000 Batch 2/8 Cost: 0.688970 Accuracy 54.69%\n",
      "Epoch  360/1000 Batch 3/8 Cost: 0.682560 Accuracy 58.59%\n",
      "Epoch  360/1000 Batch 4/8 Cost: 0.674512 Accuracy 64.06%\n",
      "Epoch  360/1000 Batch 5/8 Cost: 0.677221 Accuracy 61.72%\n",
      "Epoch  360/1000 Batch 6/8 Cost: 0.680746 Accuracy 59.38%\n",
      "Epoch  360/1000 Batch 7/8 Cost: 0.677685 Accuracy 61.72%\n",
      "Epoch  360/1000 Batch 8/8 Cost: 0.694972 Accuracy 50.96%\n",
      "Epoch  370/1000 Batch 1/8 Cost: 0.679305 Accuracy 60.94%\n",
      "Epoch  370/1000 Batch 2/8 Cost: 0.669074 Accuracy 66.41%\n",
      "Epoch  370/1000 Batch 3/8 Cost: 0.677565 Accuracy 61.72%\n",
      "Epoch  370/1000 Batch 4/8 Cost: 0.688638 Accuracy 54.69%\n",
      "Epoch  370/1000 Batch 5/8 Cost: 0.678838 Accuracy 60.16%\n",
      "Epoch  370/1000 Batch 6/8 Cost: 0.680886 Accuracy 59.38%\n",
      "Epoch  370/1000 Batch 7/8 Cost: 0.682492 Accuracy 58.59%\n",
      "Epoch  370/1000 Batch 8/8 Cost: 0.683641 Accuracy 57.69%\n",
      "Epoch  380/1000 Batch 1/8 Cost: 0.681837 Accuracy 59.38%\n",
      "Epoch  380/1000 Batch 2/8 Cost: 0.676337 Accuracy 62.50%\n",
      "Epoch  380/1000 Batch 3/8 Cost: 0.672687 Accuracy 64.06%\n",
      "Epoch  380/1000 Batch 4/8 Cost: 0.684217 Accuracy 57.03%\n",
      "Epoch  380/1000 Batch 5/8 Cost: 0.675185 Accuracy 62.50%\n",
      "Epoch  380/1000 Batch 6/8 Cost: 0.676806 Accuracy 61.72%\n",
      "Epoch  380/1000 Batch 7/8 Cost: 0.678733 Accuracy 60.16%\n",
      "Epoch  380/1000 Batch 8/8 Cost: 0.694344 Accuracy 50.96%\n",
      "Epoch  390/1000 Batch 1/8 Cost: 0.668752 Accuracy 65.62%\n",
      "Epoch  390/1000 Batch 2/8 Cost: 0.696839 Accuracy 50.00%\n",
      "Epoch  390/1000 Batch 3/8 Cost: 0.677705 Accuracy 60.94%\n",
      "Epoch  390/1000 Batch 4/8 Cost: 0.677682 Accuracy 60.94%\n",
      "Epoch  390/1000 Batch 5/8 Cost: 0.681934 Accuracy 58.59%\n",
      "Epoch  390/1000 Batch 6/8 Cost: 0.688582 Accuracy 54.69%\n",
      "Epoch  390/1000 Batch 7/8 Cost: 0.678098 Accuracy 60.94%\n",
      "Epoch  390/1000 Batch 8/8 Cost: 0.662355 Accuracy 70.19%\n",
      "Epoch  400/1000 Batch 1/8 Cost: 0.697803 Accuracy 50.00%\n",
      "Epoch  400/1000 Batch 2/8 Cost: 0.674160 Accuracy 62.50%\n",
      "Epoch  400/1000 Batch 3/8 Cost: 0.679009 Accuracy 60.16%\n",
      "Epoch  400/1000 Batch 4/8 Cost: 0.682123 Accuracy 58.59%\n",
      "Epoch  400/1000 Batch 5/8 Cost: 0.678928 Accuracy 60.16%\n",
      "Epoch  400/1000 Batch 6/8 Cost: 0.674006 Accuracy 62.50%\n",
      "Epoch  400/1000 Batch 7/8 Cost: 0.672507 Accuracy 63.28%\n",
      "Epoch  400/1000 Batch 8/8 Cost: 0.673330 Accuracy 63.46%\n",
      "Epoch  410/1000 Batch 1/8 Cost: 0.682818 Accuracy 57.81%\n",
      "Epoch  410/1000 Batch 2/8 Cost: 0.681469 Accuracy 58.59%\n",
      "Epoch  410/1000 Batch 3/8 Cost: 0.681699 Accuracy 58.59%\n",
      "Epoch  410/1000 Batch 4/8 Cost: 0.687081 Accuracy 55.47%\n",
      "Epoch  410/1000 Batch 5/8 Cost: 0.673746 Accuracy 62.50%\n",
      "Epoch  410/1000 Batch 6/8 Cost: 0.664078 Accuracy 67.97%\n",
      "Epoch  410/1000 Batch 7/8 Cost: 0.675918 Accuracy 61.72%\n",
      "Epoch  410/1000 Batch 8/8 Cost: 0.685261 Accuracy 56.73%\n",
      "Epoch  420/1000 Batch 1/8 Cost: 0.687908 Accuracy 55.47%\n",
      "Epoch  420/1000 Batch 2/8 Cost: 0.681556 Accuracy 58.59%\n",
      "Epoch  420/1000 Batch 3/8 Cost: 0.670443 Accuracy 64.06%\n",
      "Epoch  420/1000 Batch 4/8 Cost: 0.683100 Accuracy 57.81%\n",
      "Epoch  420/1000 Batch 5/8 Cost: 0.675566 Accuracy 61.72%\n",
      "Epoch  420/1000 Batch 6/8 Cost: 0.670732 Accuracy 64.06%\n",
      "Epoch  420/1000 Batch 7/8 Cost: 0.685176 Accuracy 56.25%\n",
      "Epoch  420/1000 Batch 8/8 Cost: 0.673407 Accuracy 62.50%\n",
      "Epoch  430/1000 Batch 1/8 Cost: 0.686290 Accuracy 56.25%\n",
      "Epoch  430/1000 Batch 2/8 Cost: 0.678026 Accuracy 60.16%\n",
      "Epoch  430/1000 Batch 3/8 Cost: 0.673434 Accuracy 62.50%\n",
      "Epoch  430/1000 Batch 4/8 Cost: 0.693411 Accuracy 52.34%\n",
      "Epoch  430/1000 Batch 5/8 Cost: 0.664070 Accuracy 67.19%\n",
      "Epoch  430/1000 Batch 6/8 Cost: 0.677742 Accuracy 60.16%\n",
      "Epoch  430/1000 Batch 7/8 Cost: 0.675174 Accuracy 61.72%\n",
      "Epoch  430/1000 Batch 8/8 Cost: 0.678890 Accuracy 59.62%\n",
      "Epoch  440/1000 Batch 1/8 Cost: 0.685182 Accuracy 56.25%\n",
      "Epoch  440/1000 Batch 2/8 Cost: 0.670733 Accuracy 64.06%\n",
      "Epoch  440/1000 Batch 3/8 Cost: 0.673029 Accuracy 62.50%\n",
      "Epoch  440/1000 Batch 4/8 Cost: 0.673781 Accuracy 62.50%\n",
      "Epoch  440/1000 Batch 5/8 Cost: 0.679592 Accuracy 59.38%\n",
      "Epoch  440/1000 Batch 6/8 Cost: 0.674644 Accuracy 61.72%\n",
      "Epoch  440/1000 Batch 7/8 Cost: 0.694398 Accuracy 51.56%\n",
      "Epoch  440/1000 Batch 8/8 Cost: 0.672727 Accuracy 62.50%\n",
      "Epoch  450/1000 Batch 1/8 Cost: 0.687215 Accuracy 55.47%\n",
      "Epoch  450/1000 Batch 2/8 Cost: 0.664238 Accuracy 66.41%\n",
      "Epoch  450/1000 Batch 3/8 Cost: 0.678404 Accuracy 60.16%\n",
      "Epoch  450/1000 Batch 4/8 Cost: 0.673030 Accuracy 62.50%\n",
      "Epoch  450/1000 Batch 5/8 Cost: 0.683968 Accuracy 57.03%\n",
      "Epoch  450/1000 Batch 6/8 Cost: 0.675230 Accuracy 60.94%\n",
      "Epoch  450/1000 Batch 7/8 Cost: 0.674484 Accuracy 61.72%\n",
      "Epoch  450/1000 Batch 8/8 Cost: 0.688829 Accuracy 54.81%\n",
      "Epoch  460/1000 Batch 1/8 Cost: 0.671328 Accuracy 63.28%\n",
      "Epoch  460/1000 Batch 2/8 Cost: 0.673374 Accuracy 61.72%\n",
      "Epoch  460/1000 Batch 3/8 Cost: 0.674439 Accuracy 61.72%\n",
      "Epoch  460/1000 Batch 4/8 Cost: 0.680922 Accuracy 58.59%\n",
      "Epoch  460/1000 Batch 5/8 Cost: 0.675601 Accuracy 60.94%\n",
      "Epoch  460/1000 Batch 6/8 Cost: 0.674596 Accuracy 61.72%\n",
      "Epoch  460/1000 Batch 7/8 Cost: 0.693459 Accuracy 52.34%\n",
      "Epoch  460/1000 Batch 8/8 Cost: 0.677998 Accuracy 59.62%\n",
      "Epoch  470/1000 Batch 1/8 Cost: 0.684237 Accuracy 57.03%\n",
      "Epoch  470/1000 Batch 2/8 Cost: 0.670994 Accuracy 63.28%\n",
      "Epoch  470/1000 Batch 3/8 Cost: 0.685327 Accuracy 56.25%\n",
      "Epoch  470/1000 Batch 4/8 Cost: 0.678934 Accuracy 59.38%\n",
      "Epoch  470/1000 Batch 5/8 Cost: 0.667081 Accuracy 64.84%\n",
      "Epoch  470/1000 Batch 6/8 Cost: 0.691142 Accuracy 53.12%\n",
      "Epoch  470/1000 Batch 7/8 Cost: 0.670542 Accuracy 63.28%\n",
      "Epoch  470/1000 Batch 8/8 Cost: 0.670478 Accuracy 63.46%\n",
      "Epoch  480/1000 Batch 1/8 Cost: 0.664198 Accuracy 65.62%\n",
      "Epoch  480/1000 Batch 2/8 Cost: 0.688449 Accuracy 54.69%\n",
      "Epoch  480/1000 Batch 3/8 Cost: 0.656972 Accuracy 69.53%\n",
      "Epoch  480/1000 Batch 4/8 Cost: 0.678656 Accuracy 59.38%\n",
      "Epoch  480/1000 Batch 5/8 Cost: 0.686640 Accuracy 56.25%\n",
      "Epoch  480/1000 Batch 6/8 Cost: 0.676322 Accuracy 60.16%\n",
      "Epoch  480/1000 Batch 7/8 Cost: 0.690545 Accuracy 53.91%\n",
      "Epoch  480/1000 Batch 8/8 Cost: 0.676579 Accuracy 60.58%\n",
      "Epoch  490/1000 Batch 1/8 Cost: 0.670452 Accuracy 62.50%\n",
      "Epoch  490/1000 Batch 2/8 Cost: 0.671622 Accuracy 62.50%\n",
      "Epoch  490/1000 Batch 3/8 Cost: 0.668074 Accuracy 64.06%\n",
      "Epoch  490/1000 Batch 4/8 Cost: 0.678802 Accuracy 59.38%\n",
      "Epoch  490/1000 Batch 5/8 Cost: 0.687532 Accuracy 55.47%\n",
      "Epoch  490/1000 Batch 6/8 Cost: 0.676887 Accuracy 60.16%\n",
      "Epoch  490/1000 Batch 7/8 Cost: 0.689892 Accuracy 53.91%\n",
      "Epoch  490/1000 Batch 8/8 Cost: 0.672999 Accuracy 62.50%\n",
      "Epoch  500/1000 Batch 1/8 Cost: 0.668449 Accuracy 64.06%\n",
      "Epoch  500/1000 Batch 2/8 Cost: 0.676825 Accuracy 60.16%\n",
      "Epoch  500/1000 Batch 3/8 Cost: 0.670879 Accuracy 62.50%\n",
      "Epoch  500/1000 Batch 4/8 Cost: 0.674001 Accuracy 60.94%\n",
      "Epoch  500/1000 Batch 5/8 Cost: 0.678458 Accuracy 59.38%\n",
      "Epoch  500/1000 Batch 6/8 Cost: 0.661625 Accuracy 67.19%\n",
      "Epoch  500/1000 Batch 7/8 Cost: 0.687510 Accuracy 55.47%\n",
      "Epoch  500/1000 Batch 8/8 Cost: 0.702688 Accuracy 48.08%\n",
      "Epoch  510/1000 Batch 1/8 Cost: 0.674381 Accuracy 60.94%\n",
      "Epoch  510/1000 Batch 2/8 Cost: 0.687389 Accuracy 55.47%\n",
      "Epoch  510/1000 Batch 3/8 Cost: 0.689051 Accuracy 54.69%\n",
      "Epoch  510/1000 Batch 4/8 Cost: 0.674778 Accuracy 60.94%\n",
      "Epoch  510/1000 Batch 5/8 Cost: 0.655211 Accuracy 69.53%\n",
      "Epoch  510/1000 Batch 6/8 Cost: 0.665263 Accuracy 64.84%\n",
      "Epoch  510/1000 Batch 7/8 Cost: 0.695513 Accuracy 51.56%\n",
      "Epoch  510/1000 Batch 8/8 Cost: 0.671731 Accuracy 62.50%\n",
      "Epoch  520/1000 Batch 1/8 Cost: 0.683568 Accuracy 57.03%\n",
      "Epoch  520/1000 Batch 2/8 Cost: 0.679776 Accuracy 58.59%\n",
      "Epoch  520/1000 Batch 3/8 Cost: 0.674942 Accuracy 60.94%\n",
      "Epoch  520/1000 Batch 4/8 Cost: 0.667253 Accuracy 64.06%\n",
      "Epoch  520/1000 Batch 5/8 Cost: 0.682547 Accuracy 57.03%\n",
      "Epoch  520/1000 Batch 6/8 Cost: 0.683364 Accuracy 57.03%\n",
      "Epoch  520/1000 Batch 7/8 Cost: 0.665667 Accuracy 64.84%\n",
      "Epoch  520/1000 Batch 8/8 Cost: 0.675654 Accuracy 60.58%\n",
      "Epoch  530/1000 Batch 1/8 Cost: 0.665294 Accuracy 64.84%\n",
      "Epoch  530/1000 Batch 2/8 Cost: 0.665734 Accuracy 64.84%\n",
      "Epoch  530/1000 Batch 3/8 Cost: 0.694457 Accuracy 52.34%\n",
      "Epoch  530/1000 Batch 4/8 Cost: 0.681307 Accuracy 57.81%\n",
      "Epoch  530/1000 Batch 5/8 Cost: 0.683381 Accuracy 57.03%\n",
      "Epoch  530/1000 Batch 6/8 Cost: 0.686283 Accuracy 55.47%\n",
      "Epoch  530/1000 Batch 7/8 Cost: 0.669324 Accuracy 63.28%\n",
      "Epoch  530/1000 Batch 8/8 Cost: 0.663474 Accuracy 65.38%\n",
      "Epoch  540/1000 Batch 1/8 Cost: 0.687382 Accuracy 55.47%\n",
      "Epoch  540/1000 Batch 2/8 Cost: 0.684410 Accuracy 56.25%\n",
      "Epoch  540/1000 Batch 3/8 Cost: 0.687497 Accuracy 55.47%\n",
      "Epoch  540/1000 Batch 4/8 Cost: 0.674133 Accuracy 60.94%\n",
      "Epoch  540/1000 Batch 5/8 Cost: 0.657324 Accuracy 67.97%\n",
      "Epoch  540/1000 Batch 6/8 Cost: 0.684843 Accuracy 56.25%\n",
      "Epoch  540/1000 Batch 7/8 Cost: 0.660584 Accuracy 66.41%\n",
      "Epoch  540/1000 Batch 8/8 Cost: 0.673858 Accuracy 61.54%\n",
      "Epoch  550/1000 Batch 1/8 Cost: 0.679712 Accuracy 58.59%\n",
      "Epoch  550/1000 Batch 2/8 Cost: 0.677112 Accuracy 59.38%\n",
      "Epoch  550/1000 Batch 3/8 Cost: 0.679084 Accuracy 58.59%\n",
      "Epoch  550/1000 Batch 4/8 Cost: 0.676220 Accuracy 60.16%\n",
      "Epoch  550/1000 Batch 5/8 Cost: 0.682962 Accuracy 57.03%\n",
      "Epoch  550/1000 Batch 6/8 Cost: 0.672454 Accuracy 61.72%\n",
      "Epoch  550/1000 Batch 7/8 Cost: 0.655147 Accuracy 68.75%\n",
      "Epoch  550/1000 Batch 8/8 Cost: 0.689063 Accuracy 54.81%\n",
      "Epoch  560/1000 Batch 1/8 Cost: 0.692940 Accuracy 53.12%\n",
      "Epoch  560/1000 Batch 2/8 Cost: 0.676442 Accuracy 60.16%\n",
      "Epoch  560/1000 Batch 3/8 Cost: 0.672588 Accuracy 60.94%\n",
      "Epoch  560/1000 Batch 4/8 Cost: 0.672437 Accuracy 61.72%\n",
      "Epoch  560/1000 Batch 5/8 Cost: 0.679813 Accuracy 58.59%\n",
      "Epoch  560/1000 Batch 6/8 Cost: 0.664051 Accuracy 64.84%\n",
      "Epoch  560/1000 Batch 7/8 Cost: 0.685218 Accuracy 56.25%\n",
      "Epoch  560/1000 Batch 8/8 Cost: 0.662150 Accuracy 65.38%\n",
      "Epoch  570/1000 Batch 1/8 Cost: 0.679279 Accuracy 58.59%\n",
      "Epoch  570/1000 Batch 2/8 Cost: 0.671331 Accuracy 61.72%\n",
      "Epoch  570/1000 Batch 3/8 Cost: 0.683025 Accuracy 57.03%\n",
      "Epoch  570/1000 Batch 4/8 Cost: 0.684516 Accuracy 56.25%\n",
      "Epoch  570/1000 Batch 5/8 Cost: 0.681427 Accuracy 57.81%\n",
      "Epoch  570/1000 Batch 6/8 Cost: 0.668247 Accuracy 63.28%\n",
      "Epoch  570/1000 Batch 7/8 Cost: 0.666012 Accuracy 64.06%\n",
      "Epoch  570/1000 Batch 8/8 Cost: 0.672754 Accuracy 61.54%\n",
      "Epoch  580/1000 Batch 1/8 Cost: 0.663321 Accuracy 64.84%\n",
      "Epoch  580/1000 Batch 2/8 Cost: 0.664030 Accuracy 64.84%\n",
      "Epoch  580/1000 Batch 3/8 Cost: 0.675647 Accuracy 60.16%\n",
      "Epoch  580/1000 Batch 4/8 Cost: 0.690684 Accuracy 53.91%\n",
      "Epoch  580/1000 Batch 5/8 Cost: 0.667106 Accuracy 63.28%\n",
      "Epoch  580/1000 Batch 6/8 Cost: 0.701056 Accuracy 50.00%\n",
      "Epoch  580/1000 Batch 7/8 Cost: 0.665788 Accuracy 64.06%\n",
      "Epoch  580/1000 Batch 8/8 Cost: 0.679186 Accuracy 58.65%\n",
      "Epoch  590/1000 Batch 1/8 Cost: 0.673112 Accuracy 60.94%\n",
      "Epoch  590/1000 Batch 2/8 Cost: 0.676806 Accuracy 59.38%\n",
      "Epoch  590/1000 Batch 3/8 Cost: 0.657335 Accuracy 67.19%\n",
      "Epoch  590/1000 Batch 4/8 Cost: 0.676085 Accuracy 60.16%\n",
      "Epoch  590/1000 Batch 5/8 Cost: 0.685062 Accuracy 56.25%\n",
      "Epoch  590/1000 Batch 6/8 Cost: 0.673331 Accuracy 60.94%\n",
      "Epoch  590/1000 Batch 7/8 Cost: 0.677188 Accuracy 59.38%\n",
      "Epoch  590/1000 Batch 8/8 Cost: 0.688733 Accuracy 54.81%\n",
      "Epoch  600/1000 Batch 1/8 Cost: 0.665678 Accuracy 64.06%\n",
      "Epoch  600/1000 Batch 2/8 Cost: 0.682762 Accuracy 57.03%\n",
      "Epoch  600/1000 Batch 3/8 Cost: 0.672794 Accuracy 60.94%\n",
      "Epoch  600/1000 Batch 4/8 Cost: 0.691594 Accuracy 53.91%\n",
      "Epoch  600/1000 Batch 5/8 Cost: 0.681126 Accuracy 57.81%\n",
      "Epoch  600/1000 Batch 6/8 Cost: 0.665011 Accuracy 64.06%\n",
      "Epoch  600/1000 Batch 7/8 Cost: 0.688879 Accuracy 54.69%\n",
      "Epoch  600/1000 Batch 8/8 Cost: 0.652018 Accuracy 69.23%\n",
      "Epoch  610/1000 Batch 1/8 Cost: 0.696336 Accuracy 52.34%\n",
      "Epoch  610/1000 Batch 2/8 Cost: 0.669479 Accuracy 62.50%\n",
      "Epoch  610/1000 Batch 3/8 Cost: 0.670231 Accuracy 61.72%\n",
      "Epoch  610/1000 Batch 4/8 Cost: 0.658432 Accuracy 66.41%\n",
      "Epoch  610/1000 Batch 5/8 Cost: 0.679581 Accuracy 58.59%\n",
      "Epoch  610/1000 Batch 6/8 Cost: 0.679233 Accuracy 58.59%\n",
      "Epoch  610/1000 Batch 7/8 Cost: 0.665989 Accuracy 63.28%\n",
      "Epoch  610/1000 Batch 8/8 Cost: 0.686107 Accuracy 55.77%\n",
      "Epoch  620/1000 Batch 1/8 Cost: 0.678540 Accuracy 58.59%\n",
      "Epoch  620/1000 Batch 2/8 Cost: 0.679018 Accuracy 58.59%\n",
      "Epoch  620/1000 Batch 3/8 Cost: 0.668476 Accuracy 62.50%\n",
      "Epoch  620/1000 Batch 4/8 Cost: 0.668405 Accuracy 62.50%\n",
      "Epoch  620/1000 Batch 5/8 Cost: 0.677893 Accuracy 59.38%\n",
      "Epoch  620/1000 Batch 6/8 Cost: 0.675210 Accuracy 60.16%\n",
      "Epoch  620/1000 Batch 7/8 Cost: 0.664567 Accuracy 64.06%\n",
      "Epoch  620/1000 Batch 8/8 Cost: 0.693897 Accuracy 52.88%\n",
      "Epoch  630/1000 Batch 1/8 Cost: 0.687331 Accuracy 55.47%\n",
      "Epoch  630/1000 Batch 2/8 Cost: 0.689418 Accuracy 54.69%\n",
      "Epoch  630/1000 Batch 3/8 Cost: 0.666815 Accuracy 63.28%\n",
      "Epoch  630/1000 Batch 4/8 Cost: 0.660450 Accuracy 65.62%\n",
      "Epoch  630/1000 Batch 5/8 Cost: 0.657557 Accuracy 66.41%\n",
      "Epoch  630/1000 Batch 6/8 Cost: 0.684664 Accuracy 56.25%\n",
      "Epoch  630/1000 Batch 7/8 Cost: 0.670499 Accuracy 61.72%\n",
      "Epoch  630/1000 Batch 8/8 Cost: 0.687233 Accuracy 55.77%\n",
      "Epoch  640/1000 Batch 1/8 Cost: 0.666017 Accuracy 63.28%\n",
      "Epoch  640/1000 Batch 2/8 Cost: 0.672653 Accuracy 60.94%\n",
      "Epoch  640/1000 Batch 3/8 Cost: 0.687209 Accuracy 55.47%\n",
      "Epoch  640/1000 Batch 4/8 Cost: 0.666721 Accuracy 63.28%\n",
      "Epoch  640/1000 Batch 5/8 Cost: 0.676946 Accuracy 59.38%\n",
      "Epoch  640/1000 Batch 6/8 Cost: 0.676754 Accuracy 59.38%\n",
      "Epoch  640/1000 Batch 7/8 Cost: 0.668611 Accuracy 62.50%\n",
      "Epoch  640/1000 Batch 8/8 Cost: 0.688530 Accuracy 54.81%\n",
      "Epoch  650/1000 Batch 1/8 Cost: 0.655468 Accuracy 67.19%\n",
      "Epoch  650/1000 Batch 2/8 Cost: 0.685423 Accuracy 56.25%\n",
      "Epoch  650/1000 Batch 3/8 Cost: 0.683253 Accuracy 57.03%\n",
      "Epoch  650/1000 Batch 4/8 Cost: 0.683230 Accuracy 57.03%\n",
      "Epoch  650/1000 Batch 5/8 Cost: 0.674539 Accuracy 60.16%\n",
      "Epoch  650/1000 Batch 6/8 Cost: 0.674418 Accuracy 60.16%\n",
      "Epoch  650/1000 Batch 7/8 Cost: 0.674550 Accuracy 60.16%\n",
      "Epoch  650/1000 Batch 8/8 Cost: 0.667959 Accuracy 62.50%\n",
      "Epoch  660/1000 Batch 1/8 Cost: 0.676841 Accuracy 59.38%\n",
      "Epoch  660/1000 Batch 2/8 Cost: 0.673968 Accuracy 60.16%\n",
      "Epoch  660/1000 Batch 3/8 Cost: 0.673729 Accuracy 60.16%\n",
      "Epoch  660/1000 Batch 4/8 Cost: 0.658404 Accuracy 66.41%\n",
      "Epoch  660/1000 Batch 5/8 Cost: 0.670389 Accuracy 61.72%\n",
      "Epoch  660/1000 Batch 6/8 Cost: 0.672866 Accuracy 60.94%\n",
      "Epoch  660/1000 Batch 7/8 Cost: 0.678354 Accuracy 58.59%\n",
      "Epoch  660/1000 Batch 8/8 Cost: 0.699499 Accuracy 50.96%\n",
      "Epoch  670/1000 Batch 1/8 Cost: 0.680648 Accuracy 57.81%\n",
      "Epoch  670/1000 Batch 2/8 Cost: 0.679400 Accuracy 58.59%\n",
      "Epoch  670/1000 Batch 3/8 Cost: 0.676420 Accuracy 59.38%\n",
      "Epoch  670/1000 Batch 4/8 Cost: 0.669783 Accuracy 61.72%\n",
      "Epoch  670/1000 Batch 5/8 Cost: 0.683379 Accuracy 57.03%\n",
      "Epoch  670/1000 Batch 6/8 Cost: 0.665462 Accuracy 63.28%\n",
      "Epoch  670/1000 Batch 7/8 Cost: 0.665959 Accuracy 63.28%\n",
      "Epoch  670/1000 Batch 8/8 Cost: 0.678378 Accuracy 58.65%\n",
      "Epoch  680/1000 Batch 1/8 Cost: 0.673598 Accuracy 60.16%\n",
      "Epoch  680/1000 Batch 2/8 Cost: 0.674415 Accuracy 60.16%\n",
      "Epoch  680/1000 Batch 3/8 Cost: 0.681048 Accuracy 57.81%\n",
      "Epoch  680/1000 Batch 4/8 Cost: 0.661523 Accuracy 64.84%\n",
      "Epoch  680/1000 Batch 5/8 Cost: 0.691863 Accuracy 53.91%\n",
      "Epoch  680/1000 Batch 6/8 Cost: 0.665584 Accuracy 63.28%\n",
      "Epoch  680/1000 Batch 7/8 Cost: 0.674396 Accuracy 60.16%\n",
      "Epoch  680/1000 Batch 8/8 Cost: 0.675879 Accuracy 59.62%\n",
      "Epoch  690/1000 Batch 1/8 Cost: 0.671892 Accuracy 60.94%\n",
      "Epoch  690/1000 Batch 2/8 Cost: 0.659171 Accuracy 65.62%\n",
      "Epoch  690/1000 Batch 3/8 Cost: 0.684772 Accuracy 56.25%\n",
      "Epoch  690/1000 Batch 4/8 Cost: 0.687488 Accuracy 55.47%\n",
      "Epoch  690/1000 Batch 5/8 Cost: 0.658866 Accuracy 65.62%\n",
      "Epoch  690/1000 Batch 6/8 Cost: 0.683078 Accuracy 57.03%\n",
      "Epoch  690/1000 Batch 7/8 Cost: 0.674283 Accuracy 60.16%\n",
      "Epoch  690/1000 Batch 8/8 Cost: 0.678676 Accuracy 58.65%\n",
      "Epoch  700/1000 Batch 1/8 Cost: 0.656070 Accuracy 66.41%\n",
      "Epoch  700/1000 Batch 2/8 Cost: 0.678704 Accuracy 58.59%\n",
      "Epoch  700/1000 Batch 3/8 Cost: 0.689548 Accuracy 54.69%\n",
      "Epoch  700/1000 Batch 4/8 Cost: 0.665424 Accuracy 63.28%\n",
      "Epoch  700/1000 Batch 5/8 Cost: 0.676337 Accuracy 59.38%\n",
      "Epoch  700/1000 Batch 6/8 Cost: 0.678703 Accuracy 58.59%\n",
      "Epoch  700/1000 Batch 7/8 Cost: 0.687158 Accuracy 55.47%\n",
      "Epoch  700/1000 Batch 8/8 Cost: 0.662683 Accuracy 64.42%\n",
      "Epoch  710/1000 Batch 1/8 Cost: 0.671430 Accuracy 60.94%\n",
      "Epoch  710/1000 Batch 2/8 Cost: 0.667722 Accuracy 62.50%\n",
      "Epoch  710/1000 Batch 3/8 Cost: 0.696780 Accuracy 52.34%\n",
      "Epoch  710/1000 Batch 4/8 Cost: 0.660355 Accuracy 64.84%\n",
      "Epoch  710/1000 Batch 5/8 Cost: 0.680142 Accuracy 57.81%\n",
      "Epoch  710/1000 Batch 6/8 Cost: 0.672017 Accuracy 60.94%\n",
      "Epoch  710/1000 Batch 7/8 Cost: 0.669958 Accuracy 61.72%\n",
      "Epoch  710/1000 Batch 8/8 Cost: 0.678640 Accuracy 58.65%\n",
      "Epoch  720/1000 Batch 1/8 Cost: 0.665417 Accuracy 63.28%\n",
      "Epoch  720/1000 Batch 2/8 Cost: 0.671660 Accuracy 60.94%\n",
      "Epoch  720/1000 Batch 3/8 Cost: 0.685244 Accuracy 56.25%\n",
      "Epoch  720/1000 Batch 4/8 Cost: 0.678218 Accuracy 58.59%\n",
      "Epoch  720/1000 Batch 5/8 Cost: 0.675648 Accuracy 59.38%\n",
      "Epoch  720/1000 Batch 6/8 Cost: 0.665178 Accuracy 63.28%\n",
      "Epoch  720/1000 Batch 7/8 Cost: 0.671516 Accuracy 60.94%\n",
      "Epoch  720/1000 Batch 8/8 Cost: 0.684757 Accuracy 56.73%\n",
      "Epoch  730/1000 Batch 1/8 Cost: 0.678058 Accuracy 58.59%\n",
      "Epoch  730/1000 Batch 2/8 Cost: 0.683941 Accuracy 57.03%\n",
      "Epoch  730/1000 Batch 3/8 Cost: 0.648858 Accuracy 68.75%\n",
      "Epoch  730/1000 Batch 4/8 Cost: 0.667624 Accuracy 62.50%\n",
      "Epoch  730/1000 Batch 5/8 Cost: 0.685222 Accuracy 56.25%\n",
      "Epoch  730/1000 Batch 6/8 Cost: 0.673307 Accuracy 60.16%\n",
      "Epoch  730/1000 Batch 7/8 Cost: 0.677994 Accuracy 58.59%\n",
      "Epoch  730/1000 Batch 8/8 Cost: 0.681494 Accuracy 57.69%\n",
      "Epoch  740/1000 Batch 1/8 Cost: 0.675475 Accuracy 59.38%\n",
      "Epoch  740/1000 Batch 2/8 Cost: 0.675906 Accuracy 59.38%\n",
      "Epoch  740/1000 Batch 3/8 Cost: 0.671980 Accuracy 60.94%\n",
      "Epoch  740/1000 Batch 4/8 Cost: 0.699566 Accuracy 51.56%\n",
      "Epoch  740/1000 Batch 5/8 Cost: 0.658016 Accuracy 65.62%\n",
      "Epoch  740/1000 Batch 6/8 Cost: 0.658176 Accuracy 65.62%\n",
      "Epoch  740/1000 Batch 7/8 Cost: 0.686985 Accuracy 55.47%\n",
      "Epoch  740/1000 Batch 8/8 Cost: 0.667213 Accuracy 62.50%\n",
      "Epoch  750/1000 Batch 1/8 Cost: 0.687728 Accuracy 55.47%\n",
      "Epoch  750/1000 Batch 2/8 Cost: 0.693120 Accuracy 53.91%\n",
      "Epoch  750/1000 Batch 3/8 Cost: 0.683299 Accuracy 57.03%\n",
      "Epoch  750/1000 Batch 4/8 Cost: 0.667975 Accuracy 61.72%\n",
      "Epoch  750/1000 Batch 5/8 Cost: 0.664077 Accuracy 63.28%\n",
      "Epoch  750/1000 Batch 6/8 Cost: 0.648532 Accuracy 68.75%\n",
      "Epoch  750/1000 Batch 7/8 Cost: 0.671915 Accuracy 60.94%\n",
      "Epoch  750/1000 Batch 8/8 Cost: 0.678260 Accuracy 58.65%\n",
      "Epoch  760/1000 Batch 1/8 Cost: 0.669133 Accuracy 61.72%\n",
      "Epoch  760/1000 Batch 2/8 Cost: 0.684294 Accuracy 56.25%\n",
      "Epoch  760/1000 Batch 3/8 Cost: 0.680570 Accuracy 57.81%\n",
      "Epoch  760/1000 Batch 4/8 Cost: 0.668683 Accuracy 61.72%\n",
      "Epoch  760/1000 Batch 5/8 Cost: 0.683781 Accuracy 57.03%\n",
      "Epoch  760/1000 Batch 6/8 Cost: 0.678832 Accuracy 58.59%\n",
      "Epoch  760/1000 Batch 7/8 Cost: 0.646343 Accuracy 69.53%\n",
      "Epoch  760/1000 Batch 8/8 Cost: 0.683867 Accuracy 56.73%\n",
      "Epoch  770/1000 Batch 1/8 Cost: 0.662247 Accuracy 64.06%\n",
      "Epoch  770/1000 Batch 2/8 Cost: 0.680699 Accuracy 57.81%\n",
      "Epoch  770/1000 Batch 3/8 Cost: 0.668113 Accuracy 61.72%\n",
      "Epoch  770/1000 Batch 4/8 Cost: 0.662004 Accuracy 64.06%\n",
      "Epoch  770/1000 Batch 5/8 Cost: 0.678861 Accuracy 58.59%\n",
      "Epoch  770/1000 Batch 6/8 Cost: 0.675936 Accuracy 59.38%\n",
      "Epoch  770/1000 Batch 7/8 Cost: 0.673971 Accuracy 60.16%\n",
      "Epoch  770/1000 Batch 8/8 Cost: 0.695395 Accuracy 52.88%\n",
      "Epoch  780/1000 Batch 1/8 Cost: 0.671169 Accuracy 60.94%\n",
      "Epoch  780/1000 Batch 2/8 Cost: 0.668510 Accuracy 61.72%\n",
      "Epoch  780/1000 Batch 3/8 Cost: 0.661819 Accuracy 64.06%\n",
      "Epoch  780/1000 Batch 4/8 Cost: 0.690743 Accuracy 54.69%\n",
      "Epoch  780/1000 Batch 5/8 Cost: 0.675872 Accuracy 59.38%\n",
      "Epoch  780/1000 Batch 6/8 Cost: 0.670856 Accuracy 60.94%\n",
      "Epoch  780/1000 Batch 7/8 Cost: 0.675935 Accuracy 59.38%\n",
      "Epoch  780/1000 Batch 8/8 Cost: 0.678786 Accuracy 58.65%\n",
      "Epoch  790/1000 Batch 1/8 Cost: 0.645256 Accuracy 69.53%\n",
      "Epoch  790/1000 Batch 2/8 Cost: 0.661848 Accuracy 64.06%\n",
      "Epoch  790/1000 Batch 3/8 Cost: 0.669023 Accuracy 61.72%\n",
      "Epoch  790/1000 Batch 4/8 Cost: 0.690104 Accuracy 54.69%\n",
      "Epoch  790/1000 Batch 5/8 Cost: 0.687754 Accuracy 55.47%\n",
      "Epoch  790/1000 Batch 6/8 Cost: 0.668945 Accuracy 61.72%\n",
      "Epoch  790/1000 Batch 7/8 Cost: 0.699648 Accuracy 51.56%\n",
      "Epoch  790/1000 Batch 8/8 Cost: 0.668847 Accuracy 61.54%\n",
      "Epoch  800/1000 Batch 1/8 Cost: 0.688075 Accuracy 55.47%\n",
      "Epoch  800/1000 Batch 2/8 Cost: 0.668866 Accuracy 61.72%\n",
      "Epoch  800/1000 Batch 3/8 Cost: 0.682457 Accuracy 57.03%\n",
      "Epoch  800/1000 Batch 4/8 Cost: 0.681107 Accuracy 57.81%\n",
      "Epoch  800/1000 Batch 5/8 Cost: 0.647522 Accuracy 68.75%\n",
      "Epoch  800/1000 Batch 6/8 Cost: 0.687552 Accuracy 55.47%\n",
      "Epoch  800/1000 Batch 7/8 Cost: 0.664090 Accuracy 63.28%\n",
      "Epoch  800/1000 Batch 8/8 Cost: 0.671942 Accuracy 60.58%\n",
      "Epoch  810/1000 Batch 1/8 Cost: 0.680415 Accuracy 57.81%\n",
      "Epoch  810/1000 Batch 2/8 Cost: 0.659534 Accuracy 64.84%\n",
      "Epoch  810/1000 Batch 3/8 Cost: 0.667093 Accuracy 62.50%\n",
      "Epoch  810/1000 Batch 4/8 Cost: 0.680590 Accuracy 57.81%\n",
      "Epoch  810/1000 Batch 5/8 Cost: 0.690164 Accuracy 54.69%\n",
      "Epoch  810/1000 Batch 6/8 Cost: 0.683029 Accuracy 57.03%\n",
      "Epoch  810/1000 Batch 7/8 Cost: 0.661362 Accuracy 64.06%\n",
      "Epoch  810/1000 Batch 8/8 Cost: 0.668385 Accuracy 61.54%\n",
      "Epoch  820/1000 Batch 1/8 Cost: 0.677504 Accuracy 58.59%\n",
      "Epoch  820/1000 Batch 2/8 Cost: 0.675958 Accuracy 59.38%\n",
      "Epoch  820/1000 Batch 3/8 Cost: 0.680905 Accuracy 57.81%\n",
      "Epoch  820/1000 Batch 4/8 Cost: 0.664195 Accuracy 63.28%\n",
      "Epoch  820/1000 Batch 5/8 Cost: 0.678180 Accuracy 58.59%\n",
      "Epoch  820/1000 Batch 6/8 Cost: 0.692672 Accuracy 53.91%\n",
      "Epoch  820/1000 Batch 7/8 Cost: 0.658648 Accuracy 64.84%\n",
      "Epoch  820/1000 Batch 8/8 Cost: 0.660709 Accuracy 64.42%\n",
      "Epoch  830/1000 Batch 1/8 Cost: 0.656551 Accuracy 65.62%\n",
      "Epoch  830/1000 Batch 2/8 Cost: 0.661712 Accuracy 64.06%\n",
      "Epoch  830/1000 Batch 3/8 Cost: 0.676553 Accuracy 59.38%\n",
      "Epoch  830/1000 Batch 4/8 Cost: 0.675232 Accuracy 59.38%\n",
      "Epoch  830/1000 Batch 5/8 Cost: 0.680362 Accuracy 57.81%\n",
      "Epoch  830/1000 Batch 6/8 Cost: 0.685681 Accuracy 56.25%\n",
      "Epoch  830/1000 Batch 7/8 Cost: 0.662799 Accuracy 63.28%\n",
      "Epoch  830/1000 Batch 8/8 Cost: 0.696184 Accuracy 52.88%\n",
      "Epoch  840/1000 Batch 1/8 Cost: 0.668017 Accuracy 61.72%\n",
      "Epoch  840/1000 Batch 2/8 Cost: 0.670654 Accuracy 60.94%\n",
      "Epoch  840/1000 Batch 3/8 Cost: 0.663725 Accuracy 63.28%\n",
      "Epoch  840/1000 Batch 4/8 Cost: 0.680385 Accuracy 57.81%\n",
      "Epoch  840/1000 Batch 5/8 Cost: 0.676805 Accuracy 59.38%\n",
      "Epoch  840/1000 Batch 6/8 Cost: 0.671337 Accuracy 60.94%\n",
      "Epoch  840/1000 Batch 7/8 Cost: 0.690218 Accuracy 54.69%\n",
      "Epoch  840/1000 Batch 8/8 Cost: 0.668389 Accuracy 61.54%\n",
      "Epoch  850/1000 Batch 1/8 Cost: 0.671251 Accuracy 60.94%\n",
      "Epoch  850/1000 Batch 2/8 Cost: 0.673114 Accuracy 60.16%\n",
      "Epoch  850/1000 Batch 3/8 Cost: 0.695550 Accuracy 53.12%\n",
      "Epoch  850/1000 Batch 4/8 Cost: 0.658066 Accuracy 64.84%\n",
      "Epoch  850/1000 Batch 5/8 Cost: 0.661338 Accuracy 64.06%\n",
      "Epoch  850/1000 Batch 6/8 Cost: 0.676146 Accuracy 59.38%\n",
      "Epoch  850/1000 Batch 7/8 Cost: 0.668184 Accuracy 61.72%\n",
      "Epoch  850/1000 Batch 8/8 Cost: 0.689534 Accuracy 54.81%\n",
      "Epoch  860/1000 Batch 1/8 Cost: 0.670835 Accuracy 60.94%\n",
      "Epoch  860/1000 Batch 2/8 Cost: 0.663444 Accuracy 63.28%\n",
      "Epoch  860/1000 Batch 3/8 Cost: 0.675366 Accuracy 59.38%\n",
      "Epoch  860/1000 Batch 4/8 Cost: 0.670995 Accuracy 60.94%\n",
      "Epoch  860/1000 Batch 5/8 Cost: 0.683215 Accuracy 57.03%\n",
      "Epoch  860/1000 Batch 6/8 Cost: 0.660501 Accuracy 64.06%\n",
      "Epoch  860/1000 Batch 7/8 Cost: 0.688182 Accuracy 55.47%\n",
      "Epoch  860/1000 Batch 8/8 Cost: 0.678211 Accuracy 58.65%\n",
      "Epoch  870/1000 Batch 1/8 Cost: 0.660570 Accuracy 64.06%\n",
      "Epoch  870/1000 Batch 2/8 Cost: 0.675730 Accuracy 59.38%\n",
      "Epoch  870/1000 Batch 3/8 Cost: 0.685910 Accuracy 56.25%\n",
      "Epoch  870/1000 Batch 4/8 Cost: 0.677872 Accuracy 58.59%\n",
      "Epoch  870/1000 Batch 5/8 Cost: 0.678029 Accuracy 58.59%\n",
      "Epoch  870/1000 Batch 6/8 Cost: 0.686099 Accuracy 56.25%\n",
      "Epoch  870/1000 Batch 7/8 Cost: 0.655387 Accuracy 65.62%\n",
      "Epoch  870/1000 Batch 8/8 Cost: 0.669160 Accuracy 61.54%\n",
      "Epoch  880/1000 Batch 1/8 Cost: 0.655714 Accuracy 65.62%\n",
      "Epoch  880/1000 Batch 2/8 Cost: 0.670935 Accuracy 60.94%\n",
      "Epoch  880/1000 Batch 3/8 Cost: 0.696092 Accuracy 53.12%\n",
      "Epoch  880/1000 Batch 4/8 Cost: 0.692847 Accuracy 53.91%\n",
      "Epoch  880/1000 Batch 5/8 Cost: 0.680484 Accuracy 57.81%\n",
      "Epoch  880/1000 Batch 6/8 Cost: 0.671226 Accuracy 60.94%\n",
      "Epoch  880/1000 Batch 7/8 Cost: 0.662582 Accuracy 63.28%\n",
      "Epoch  880/1000 Batch 8/8 Cost: 0.656157 Accuracy 65.38%\n",
      "Epoch  890/1000 Batch 1/8 Cost: 0.651162 Accuracy 67.19%\n",
      "Epoch  890/1000 Batch 2/8 Cost: 0.678342 Accuracy 58.59%\n",
      "Epoch  890/1000 Batch 3/8 Cost: 0.680032 Accuracy 57.81%\n",
      "Epoch  890/1000 Batch 4/8 Cost: 0.669804 Accuracy 60.94%\n",
      "Epoch  890/1000 Batch 5/8 Cost: 0.685981 Accuracy 56.25%\n",
      "Epoch  890/1000 Batch 6/8 Cost: 0.658616 Accuracy 64.84%\n",
      "Epoch  890/1000 Batch 7/8 Cost: 0.669849 Accuracy 60.94%\n",
      "Epoch  890/1000 Batch 8/8 Cost: 0.700243 Accuracy 51.92%\n",
      "Epoch  900/1000 Batch 1/8 Cost: 0.685627 Accuracy 56.25%\n",
      "Epoch  900/1000 Batch 2/8 Cost: 0.675308 Accuracy 59.38%\n",
      "Epoch  900/1000 Batch 3/8 Cost: 0.678005 Accuracy 58.59%\n",
      "Epoch  900/1000 Batch 4/8 Cost: 0.662260 Accuracy 63.28%\n",
      "Epoch  900/1000 Batch 5/8 Cost: 0.706266 Accuracy 50.00%\n",
      "Epoch  900/1000 Batch 6/8 Cost: 0.655949 Accuracy 65.62%\n",
      "Epoch  900/1000 Batch 7/8 Cost: 0.655717 Accuracy 65.62%\n",
      "Epoch  900/1000 Batch 8/8 Cost: 0.668743 Accuracy 61.54%\n",
      "Epoch  910/1000 Batch 1/8 Cost: 0.668369 Accuracy 61.72%\n",
      "Epoch  910/1000 Batch 2/8 Cost: 0.664933 Accuracy 62.50%\n",
      "Epoch  910/1000 Batch 3/8 Cost: 0.680898 Accuracy 57.81%\n",
      "Epoch  910/1000 Batch 4/8 Cost: 0.683234 Accuracy 57.03%\n",
      "Epoch  910/1000 Batch 5/8 Cost: 0.688621 Accuracy 55.47%\n",
      "Epoch  910/1000 Batch 6/8 Cost: 0.665115 Accuracy 62.50%\n",
      "Epoch  910/1000 Batch 7/8 Cost: 0.678055 Accuracy 58.59%\n",
      "Epoch  910/1000 Batch 8/8 Cost: 0.656014 Accuracy 65.38%\n",
      "Epoch  920/1000 Batch 1/8 Cost: 0.675647 Accuracy 59.38%\n",
      "Epoch  920/1000 Batch 2/8 Cost: 0.686324 Accuracy 56.25%\n",
      "Epoch  920/1000 Batch 3/8 Cost: 0.654174 Accuracy 65.62%\n",
      "Epoch  920/1000 Batch 4/8 Cost: 0.688436 Accuracy 55.47%\n",
      "Epoch  920/1000 Batch 5/8 Cost: 0.685781 Accuracy 56.25%\n",
      "Epoch  920/1000 Batch 6/8 Cost: 0.659689 Accuracy 64.06%\n",
      "Epoch  920/1000 Batch 7/8 Cost: 0.660315 Accuracy 64.06%\n",
      "Epoch  920/1000 Batch 8/8 Cost: 0.678929 Accuracy 58.65%\n",
      "Epoch  930/1000 Batch 1/8 Cost: 0.657433 Accuracy 64.84%\n",
      "Epoch  930/1000 Batch 2/8 Cost: 0.682413 Accuracy 57.03%\n",
      "Epoch  930/1000 Batch 3/8 Cost: 0.676263 Accuracy 59.38%\n",
      "Epoch  930/1000 Batch 4/8 Cost: 0.658054 Accuracy 64.84%\n",
      "Epoch  930/1000 Batch 5/8 Cost: 0.678036 Accuracy 58.59%\n",
      "Epoch  930/1000 Batch 6/8 Cost: 0.660128 Accuracy 64.06%\n",
      "Epoch  930/1000 Batch 7/8 Cost: 0.685378 Accuracy 56.25%\n",
      "Epoch  930/1000 Batch 8/8 Cost: 0.694230 Accuracy 53.85%\n",
      "Epoch  940/1000 Batch 1/8 Cost: 0.654952 Accuracy 65.62%\n",
      "Epoch  940/1000 Batch 2/8 Cost: 0.657491 Accuracy 64.84%\n",
      "Epoch  940/1000 Batch 3/8 Cost: 0.678719 Accuracy 58.59%\n",
      "Epoch  940/1000 Batch 4/8 Cost: 0.688167 Accuracy 55.47%\n",
      "Epoch  940/1000 Batch 5/8 Cost: 0.675104 Accuracy 59.38%\n",
      "Epoch  940/1000 Batch 6/8 Cost: 0.665425 Accuracy 62.50%\n",
      "Epoch  940/1000 Batch 7/8 Cost: 0.688466 Accuracy 55.47%\n",
      "Epoch  940/1000 Batch 8/8 Cost: 0.680901 Accuracy 57.69%\n",
      "Epoch  950/1000 Batch 1/8 Cost: 0.675620 Accuracy 59.38%\n",
      "Epoch  950/1000 Batch 2/8 Cost: 0.673051 Accuracy 60.16%\n",
      "Epoch  950/1000 Batch 3/8 Cost: 0.686273 Accuracy 56.25%\n",
      "Epoch  950/1000 Batch 4/8 Cost: 0.676084 Accuracy 59.38%\n",
      "Epoch  950/1000 Batch 5/8 Cost: 0.643986 Accuracy 68.75%\n",
      "Epoch  950/1000 Batch 6/8 Cost: 0.680416 Accuracy 57.81%\n",
      "Epoch  950/1000 Batch 7/8 Cost: 0.682783 Accuracy 57.03%\n",
      "Epoch  950/1000 Batch 8/8 Cost: 0.668467 Accuracy 61.54%\n",
      "Epoch  960/1000 Batch 1/8 Cost: 0.662486 Accuracy 63.28%\n",
      "Epoch  960/1000 Batch 2/8 Cost: 0.670480 Accuracy 60.94%\n",
      "Epoch  960/1000 Batch 3/8 Cost: 0.663083 Accuracy 63.28%\n",
      "Epoch  960/1000 Batch 4/8 Cost: 0.688898 Accuracy 55.47%\n",
      "Epoch  960/1000 Batch 5/8 Cost: 0.693371 Accuracy 53.91%\n",
      "Epoch  960/1000 Batch 6/8 Cost: 0.641246 Accuracy 69.53%\n",
      "Epoch  960/1000 Batch 7/8 Cost: 0.685806 Accuracy 56.25%\n",
      "Epoch  960/1000 Batch 8/8 Cost: 0.684028 Accuracy 56.73%\n",
      "Epoch  970/1000 Batch 1/8 Cost: 0.657943 Accuracy 64.84%\n",
      "Epoch  970/1000 Batch 2/8 Cost: 0.659780 Accuracy 64.06%\n",
      "Epoch  970/1000 Batch 3/8 Cost: 0.693825 Accuracy 53.91%\n",
      "Epoch  970/1000 Batch 4/8 Cost: 0.672368 Accuracy 60.16%\n",
      "Epoch  970/1000 Batch 5/8 Cost: 0.683082 Accuracy 57.03%\n",
      "Epoch  970/1000 Batch 6/8 Cost: 0.672698 Accuracy 60.16%\n",
      "Epoch  970/1000 Batch 7/8 Cost: 0.675281 Accuracy 59.38%\n",
      "Epoch  970/1000 Batch 8/8 Cost: 0.671963 Accuracy 60.58%\n",
      "Epoch  980/1000 Batch 1/8 Cost: 0.683298 Accuracy 57.03%\n",
      "Epoch  980/1000 Batch 2/8 Cost: 0.677475 Accuracy 58.59%\n",
      "Epoch  980/1000 Batch 3/8 Cost: 0.688910 Accuracy 55.47%\n",
      "Epoch  980/1000 Batch 4/8 Cost: 0.669984 Accuracy 60.94%\n",
      "Epoch  980/1000 Batch 5/8 Cost: 0.657278 Accuracy 64.84%\n",
      "Epoch  980/1000 Batch 6/8 Cost: 0.665266 Accuracy 62.50%\n",
      "Epoch  980/1000 Batch 7/8 Cost: 0.681345 Accuracy 57.81%\n",
      "Epoch  980/1000 Batch 8/8 Cost: 0.661174 Accuracy 63.46%\n",
      "Epoch  990/1000 Batch 1/8 Cost: 0.675059 Accuracy 59.38%\n",
      "Epoch  990/1000 Batch 2/8 Cost: 0.673002 Accuracy 60.16%\n",
      "Epoch  990/1000 Batch 3/8 Cost: 0.643660 Accuracy 68.75%\n",
      "Epoch  990/1000 Batch 4/8 Cost: 0.683365 Accuracy 57.03%\n",
      "Epoch  990/1000 Batch 5/8 Cost: 0.651399 Accuracy 66.41%\n",
      "Epoch  990/1000 Batch 6/8 Cost: 0.702208 Accuracy 51.56%\n",
      "Epoch  990/1000 Batch 7/8 Cost: 0.685946 Accuracy 56.25%\n",
      "Epoch  990/1000 Batch 8/8 Cost: 0.671941 Accuracy 60.58%\n",
      "Epoch 1000/1000 Batch 1/8 Cost: 0.654159 Accuracy 65.62%\n",
      "Epoch 1000/1000 Batch 2/8 Cost: 0.662256 Accuracy 63.28%\n",
      "Epoch 1000/1000 Batch 3/8 Cost: 0.709510 Accuracy 49.22%\n",
      "Epoch 1000/1000 Batch 4/8 Cost: 0.632531 Accuracy 71.88%\n",
      "Epoch 1000/1000 Batch 5/8 Cost: 0.660226 Accuracy 64.06%\n",
      "Epoch 1000/1000 Batch 6/8 Cost: 0.680629 Accuracy 57.81%\n",
      "Epoch 1000/1000 Batch 7/8 Cost: 0.696513 Accuracy 53.12%\n",
      "Epoch 1000/1000 Batch 8/8 Cost: 0.694897 Accuracy 53.85%\n"
     ]
    }
   ],
   "source": [
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    for batch_idx, samples in enumerate(dataloader):\n",
    "\n",
    "        x_train, y_train = samples\n",
    "        # H(x) 계산\n",
    "        hypothesis = model(x_train)\n",
    "\n",
    "        # cost 계산\n",
    "        # print(y_train.size())\n",
    "        hypothesis=hypothesis.squeeze()\n",
    "        # print(hypothesis.size())\n",
    "        cost = F.binary_cross_entropy(hypothesis, y_train)\n",
    "\n",
    "        # cost로 H(x) 개선\n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # 20번마다 로그 출력\n",
    "        if epoch % 10 == 0:\n",
    "            prediction = hypothesis >= torch.FloatTensor([0.5]) # 예측값이 0.5를 넘으면 True로 간주\n",
    "            correct_prediction = prediction.float() == y_train # 실제값과 일치하는 경우만 True로 간주\n",
    "            accuracy = correct_prediction.sum().item() / len(correct_prediction) # 정확도를 계산\n",
    "            print('Epoch {:4d}/{} Batch {}/{} Cost: {:.6f} Accuracy {:2.2f}%'.format( # 각 에포크마다 정확도를 출력\n",
    "                epoch, nb_epochs, batch_idx+1, len(dataloader), cost.item(), accuracy * 100,\n",
    "            ))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 Cost: 0.949955\n",
      "Epoch  100/1000 Cost: 0.937104\n",
      "Epoch  200/1000 Cost: 0.924774\n",
      "Epoch  300/1000 Cost: 0.912951\n",
      "Epoch  400/1000 Cost: 0.901622\n",
      "Epoch  500/1000 Cost: 0.890774\n",
      "Epoch  600/1000 Cost: 0.880392\n",
      "Epoch  700/1000 Cost: 0.870464\n",
      "Epoch  800/1000 Cost: 0.860974\n",
      "Epoch  900/1000 Cost: 0.851909\n",
      "Epoch 1000/1000 Cost: 0.843253\n"
     ]
    }
   ],
   "source": [
    "# optimizer 설정\n",
    "optimizer = optim.SGD([W, b], lr=0.001)\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "\n",
    "    # Cost 계산\n",
    "    hypothesis = torch.sigmoid(x_train.matmul(W) + b)\n",
    "    cost = -(y_train * torch.log(hypothesis) + \n",
    "             (1 - y_train) * torch.log(1 - hypothesis)).mean()\n",
    "\n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # 100번마다 로그 출력\n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6503],\n",
      "        [0.6323],\n",
      "        [0.6855],\n",
      "        [0.7107],\n",
      "        [0.6412],\n",
      "        [0.6318],\n",
      "        [0.7027],\n",
      "        [0.6445],\n",
      "        [0.7011],\n",
      "        [0.6907],\n",
      "        [0.6603],\n",
      "        [0.6967],\n",
      "        [0.7190],\n",
      "        [0.6834],\n",
      "        [0.6683],\n",
      "        [0.6789],\n",
      "        [0.7427],\n",
      "        [0.7068],\n",
      "        [0.7636],\n",
      "        [0.6321],\n",
      "        [0.6566],\n",
      "        [0.6452],\n",
      "        [0.6363],\n",
      "        [0.6114],\n",
      "        [0.6819],\n",
      "        [0.6627],\n",
      "        [0.6865],\n",
      "        [0.6180],\n",
      "        [0.7012],\n",
      "        [0.6402],\n",
      "        [0.6660],\n",
      "        [0.7404],\n",
      "        [0.6550],\n",
      "        [0.6267],\n",
      "        [0.6448],\n",
      "        [0.7168],\n",
      "        [0.6378],\n",
      "        [0.6728],\n",
      "        [0.7838],\n",
      "        [0.7227],\n",
      "        [0.7086],\n",
      "        [0.7187],\n",
      "        [0.7110],\n",
      "        [0.7781],\n",
      "        [0.6206],\n",
      "        [0.6343],\n",
      "        [0.6655],\n",
      "        [0.6482],\n",
      "        [0.6203],\n",
      "        [0.6260],\n",
      "        [0.6119],\n",
      "        [0.7211],\n",
      "        [0.6145],\n",
      "        [0.7013],\n",
      "        [0.6732],\n",
      "        [0.6597],\n",
      "        [0.6373],\n",
      "        [0.6620],\n",
      "        [0.6783],\n",
      "        [0.6370],\n",
      "        [0.7063],\n",
      "        [0.6451],\n",
      "        [0.6397],\n",
      "        [0.6369],\n",
      "        [0.6329],\n",
      "        [0.6214],\n",
      "        [0.6502],\n",
      "        [0.6892],\n",
      "        [0.6220],\n",
      "        [0.7360],\n",
      "        [0.6412],\n",
      "        [0.6220],\n",
      "        [0.6319],\n",
      "        [0.6446],\n",
      "        [0.6877],\n",
      "        [0.6387],\n",
      "        [0.6354],\n",
      "        [0.6296],\n",
      "        [0.6202],\n",
      "        [0.6295],\n",
      "        [0.6367],\n",
      "        [0.6299],\n",
      "        [0.6400],\n",
      "        [0.6276],\n",
      "        [0.6306],\n",
      "        [0.6222],\n",
      "        [0.6298],\n",
      "        [0.6520],\n",
      "        [0.6553],\n",
      "        [0.6302],\n",
      "        [0.7601],\n",
      "        [0.8034],\n",
      "        [0.7463],\n",
      "        [0.7940],\n",
      "        [0.6638],\n",
      "        [0.7964],\n",
      "        [0.8049],\n",
      "        [0.8043],\n",
      "        [0.7022],\n",
      "        [0.8036],\n",
      "        [0.7177],\n",
      "        [0.6954],\n",
      "        [0.8029],\n",
      "        [0.7110],\n",
      "        [0.7715],\n",
      "        [0.7816],\n",
      "        [0.6685],\n",
      "        [0.7021],\n",
      "        [0.6705],\n",
      "        [0.6829],\n",
      "        [0.6241],\n",
      "        [0.7167],\n",
      "        [0.7243],\n",
      "        [0.6386],\n",
      "        [0.7252],\n",
      "        [0.6288],\n",
      "        [0.6152],\n",
      "        [0.7487],\n",
      "        [0.8034],\n",
      "        [0.7602],\n",
      "        [0.7426],\n",
      "        [0.7892],\n",
      "        [0.7986],\n",
      "        [0.8010],\n",
      "        [0.7246],\n",
      "        [0.7743],\n",
      "        [0.7945],\n",
      "        [0.8118],\n",
      "        [0.7938],\n",
      "        [0.6453],\n",
      "        [0.6483],\n",
      "        [0.7075],\n",
      "        [0.6660],\n",
      "        [0.7044],\n",
      "        [0.6825],\n",
      "        [0.6824],\n",
      "        [0.6317],\n",
      "        [0.7257],\n",
      "        [0.6550],\n",
      "        [0.6748],\n",
      "        [0.6863],\n",
      "        [0.6365],\n",
      "        [0.6989],\n",
      "        [0.6631],\n",
      "        [0.7443],\n",
      "        [0.6580],\n",
      "        [0.6929],\n",
      "        [0.6457],\n",
      "        [0.6604],\n",
      "        [0.6184],\n",
      "        [0.7605],\n",
      "        [0.6446],\n",
      "        [0.7227],\n",
      "        [0.6844],\n",
      "        [0.6468],\n",
      "        [0.6415],\n",
      "        [0.6947],\n",
      "        [0.7516],\n",
      "        [0.6403],\n",
      "        [0.7378],\n",
      "        [0.6413],\n",
      "        [0.6204],\n",
      "        [0.6352],\n",
      "        [0.7539],\n",
      "        [0.6439],\n",
      "        [0.6711],\n",
      "        [0.6597],\n",
      "        [0.7617],\n",
      "        [0.6269],\n",
      "        [0.6675],\n",
      "        [0.6828],\n",
      "        [0.6468],\n",
      "        [0.6700],\n",
      "        [0.6165],\n",
      "        [0.6205],\n",
      "        [0.6878],\n",
      "        [0.6935],\n",
      "        [0.6934],\n",
      "        [0.6645],\n",
      "        [0.6978],\n",
      "        [0.6684],\n",
      "        [0.6672],\n",
      "        [0.6480],\n",
      "        [0.6640],\n",
      "        [0.7157],\n",
      "        [0.6481],\n",
      "        [0.6855],\n",
      "        [0.7558],\n",
      "        [0.7151],\n",
      "        [0.7052],\n",
      "        [0.6277],\n",
      "        [0.6682],\n",
      "        [0.6748],\n",
      "        [0.6536],\n",
      "        [0.6532],\n",
      "        [0.6288],\n",
      "        [0.6305],\n",
      "        [0.6602],\n",
      "        [0.6232],\n",
      "        [0.6624],\n",
      "        [0.6665],\n",
      "        [0.6206],\n",
      "        [0.7385],\n",
      "        [0.7184],\n",
      "        [0.7246],\n",
      "        [0.6363],\n",
      "        [0.6617],\n",
      "        [0.6219],\n",
      "        [0.6745],\n",
      "        [0.6473],\n",
      "        [0.6872],\n",
      "        [0.7279],\n",
      "        [0.7107],\n",
      "        [0.6778],\n",
      "        [0.7031],\n",
      "        [0.6824],\n",
      "        [0.6916],\n",
      "        [0.6887],\n",
      "        [0.7521],\n",
      "        [0.7481],\n",
      "        [0.6840],\n",
      "        [0.6384],\n",
      "        [0.7046],\n",
      "        [0.7145],\n",
      "        [0.6513],\n",
      "        [0.7618],\n",
      "        [0.6867],\n",
      "        [0.6276],\n",
      "        [0.7688],\n",
      "        [0.6518],\n",
      "        [0.7111],\n",
      "        [0.6648],\n",
      "        [0.6450],\n",
      "        [0.7738],\n",
      "        [0.7959],\n",
      "        [0.7969],\n",
      "        [0.6923],\n",
      "        [0.7033],\n",
      "        [0.6390],\n",
      "        [0.6709],\n",
      "        [0.6989],\n",
      "        [0.7331],\n",
      "        [0.7774],\n",
      "        [0.6169],\n",
      "        [0.6726],\n",
      "        [0.6312],\n",
      "        [0.6333],\n",
      "        [0.7066],\n",
      "        [0.6158],\n",
      "        [0.7436],\n",
      "        [0.6085],\n",
      "        [0.7750],\n",
      "        [0.6756],\n",
      "        [0.6260],\n",
      "        [0.7408],\n",
      "        [0.6563],\n",
      "        [0.6594],\n",
      "        [0.6911],\n",
      "        [0.6781],\n",
      "        [0.6551],\n",
      "        [0.5933],\n",
      "        [0.6401],\n",
      "        [0.6289],\n",
      "        [0.6287],\n",
      "        [0.6420],\n",
      "        [0.8013],\n",
      "        [0.6439],\n",
      "        [0.7507],\n",
      "        [0.7041],\n",
      "        [0.6386],\n",
      "        [0.6808],\n",
      "        [0.6367],\n",
      "        [0.6674],\n",
      "        [0.6922],\n",
      "        [0.6193],\n",
      "        [0.6838],\n",
      "        [0.5954],\n",
      "        [0.6770],\n",
      "        [0.7537],\n",
      "        [0.6553],\n",
      "        [0.6561],\n",
      "        [0.7231],\n",
      "        [0.6590],\n",
      "        [0.6734],\n",
      "        [0.6654],\n",
      "        [0.7334],\n",
      "        [0.6707],\n",
      "        [0.7528],\n",
      "        [0.6765],\n",
      "        [0.6538],\n",
      "        [0.6710],\n",
      "        [0.6543],\n",
      "        [0.6751],\n",
      "        [0.6184],\n",
      "        [0.6451],\n",
      "        [0.6716],\n",
      "        [0.7201],\n",
      "        [0.6656],\n",
      "        [0.6544],\n",
      "        [0.7030],\n",
      "        [0.7645],\n",
      "        [0.6726],\n",
      "        [0.6596],\n",
      "        [0.6648],\n",
      "        [0.7663],\n",
      "        [0.6708],\n",
      "        [0.6802],\n",
      "        [0.7936],\n",
      "        [0.7268],\n",
      "        [0.7106],\n",
      "        [0.6377],\n",
      "        [0.7972],\n",
      "        [0.7359],\n",
      "        [0.6903],\n",
      "        [0.7800],\n",
      "        [0.6543],\n",
      "        [0.6688],\n",
      "        [0.7255],\n",
      "        [0.6475],\n",
      "        [0.6708],\n",
      "        [0.6475],\n",
      "        [0.6710],\n",
      "        [0.7462],\n",
      "        [0.6479],\n",
      "        [0.6667],\n",
      "        [0.6910],\n",
      "        [0.6456],\n",
      "        [0.6776],\n",
      "        [0.6346],\n",
      "        [0.6598],\n",
      "        [0.7115],\n",
      "        [0.7349],\n",
      "        [0.6280],\n",
      "        [0.7768],\n",
      "        [0.6463],\n",
      "        [0.6394],\n",
      "        [0.6533],\n",
      "        [0.6650],\n",
      "        [0.6428],\n",
      "        [0.6576],\n",
      "        [0.6463],\n",
      "        [0.6573],\n",
      "        [0.7961],\n",
      "        [0.6162],\n",
      "        [0.7686],\n",
      "        [0.7514],\n",
      "        [0.6828],\n",
      "        [0.6044],\n",
      "        [0.6909],\n",
      "        [0.6973],\n",
      "        [0.7095],\n",
      "        [0.6574],\n",
      "        [0.6483],\n",
      "        [0.6572],\n",
      "        [0.6603],\n",
      "        [0.6900],\n",
      "        [0.6487],\n",
      "        [0.6160],\n",
      "        [0.6178],\n",
      "        [0.6319],\n",
      "        [0.6137],\n",
      "        [0.6331],\n",
      "        [0.6372],\n",
      "        [0.6929],\n",
      "        [0.7657],\n",
      "        [0.6844],\n",
      "        [0.6990],\n",
      "        [0.6363],\n",
      "        [0.6948],\n",
      "        [0.6605],\n",
      "        [0.6642],\n",
      "        [0.7051],\n",
      "        [0.6749],\n",
      "        [0.6601],\n",
      "        [0.6317],\n",
      "        [0.6674],\n",
      "        [0.6952],\n",
      "        [0.7311],\n",
      "        [0.6734],\n",
      "        [0.6614],\n",
      "        [0.6859],\n",
      "        [0.6446],\n",
      "        [0.6375],\n",
      "        [0.6847],\n",
      "        [0.6677],\n",
      "        [0.6633],\n",
      "        [0.6216],\n",
      "        [0.6796],\n",
      "        [0.6308],\n",
      "        [0.6396],\n",
      "        [0.6716],\n",
      "        [0.6357],\n",
      "        [0.6121],\n",
      "        [0.6035],\n",
      "        [0.6702],\n",
      "        [0.6607],\n",
      "        [0.6320],\n",
      "        [0.6716],\n",
      "        [0.6050],\n",
      "        [0.6876],\n",
      "        [0.6624],\n",
      "        [0.7080],\n",
      "        [0.6919],\n",
      "        [0.6933],\n",
      "        [0.7114],\n",
      "        [0.6330],\n",
      "        [0.6640],\n",
      "        [0.6372],\n",
      "        [0.7604],\n",
      "        [0.6601],\n",
      "        [0.6281],\n",
      "        [0.6835],\n",
      "        [0.6823],\n",
      "        [0.7335],\n",
      "        [0.6593],\n",
      "        [0.6747],\n",
      "        [0.6896],\n",
      "        [0.6682],\n",
      "        [0.7052],\n",
      "        [0.6030],\n",
      "        [0.6988],\n",
      "        [0.6286],\n",
      "        [0.7410],\n",
      "        [0.6915],\n",
      "        [0.6721],\n",
      "        [0.6874],\n",
      "        [0.6958],\n",
      "        [0.6383],\n",
      "        [0.7115],\n",
      "        [0.6859],\n",
      "        [0.7600],\n",
      "        [0.6787],\n",
      "        [0.6646],\n",
      "        [0.7263],\n",
      "        [0.6682],\n",
      "        [0.6063],\n",
      "        [0.7046],\n",
      "        [0.7280],\n",
      "        [0.6322],\n",
      "        [0.6557],\n",
      "        [0.7142],\n",
      "        [0.6517],\n",
      "        [0.7267],\n",
      "        [0.6747],\n",
      "        [0.7061],\n",
      "        [0.6766],\n",
      "        [0.6794],\n",
      "        [0.6682],\n",
      "        [0.6137],\n",
      "        [0.6295],\n",
      "        [0.6460],\n",
      "        [0.6765],\n",
      "        [0.7420],\n",
      "        [0.7553],\n",
      "        [0.7022],\n",
      "        [0.6727],\n",
      "        [0.6364],\n",
      "        [0.6238],\n",
      "        [0.6712],\n",
      "        [0.6809],\n",
      "        [0.6809],\n",
      "        [0.7501],\n",
      "        [0.6435],\n",
      "        [0.6198],\n",
      "        [0.6221],\n",
      "        [0.6873],\n",
      "        [0.6041],\n",
      "        [0.6677],\n",
      "        [0.7473],\n",
      "        [0.7226],\n",
      "        [0.6627],\n",
      "        [0.6942],\n",
      "        [0.6818],\n",
      "        [0.7314],\n",
      "        [0.7110],\n",
      "        [0.6654],\n",
      "        [0.6540],\n",
      "        [0.6796],\n",
      "        [0.6343],\n",
      "        [0.7337],\n",
      "        [0.6979],\n",
      "        [0.8005],\n",
      "        [0.6226],\n",
      "        [0.6630],\n",
      "        [0.6537],\n",
      "        [0.6499],\n",
      "        [0.6122],\n",
      "        [0.7673],\n",
      "        [0.6617],\n",
      "        [0.6683],\n",
      "        [0.6390],\n",
      "        [0.7859],\n",
      "        [0.6687],\n",
      "        [0.6680],\n",
      "        [0.6543],\n",
      "        [0.6648],\n",
      "        [0.6586],\n",
      "        [0.6981],\n",
      "        [0.6279],\n",
      "        [0.7245],\n",
      "        [0.6738],\n",
      "        [0.6763],\n",
      "        [0.7209],\n",
      "        [0.7573],\n",
      "        [0.6540],\n",
      "        [0.6562],\n",
      "        [0.7585],\n",
      "        [0.6611],\n",
      "        [0.7441],\n",
      "        [0.6478],\n",
      "        [0.6434],\n",
      "        [0.6493],\n",
      "        [0.7218],\n",
      "        [0.6543],\n",
      "        [0.7251],\n",
      "        [0.7092],\n",
      "        [0.6332],\n",
      "        [0.6013],\n",
      "        [0.7239],\n",
      "        [0.7429],\n",
      "        [0.6624],\n",
      "        [0.6765],\n",
      "        [0.7007],\n",
      "        [0.7378],\n",
      "        [0.7982],\n",
      "        [0.6601],\n",
      "        [0.5884],\n",
      "        [0.7032],\n",
      "        [0.6987],\n",
      "        [0.6521],\n",
      "        [0.7315],\n",
      "        [0.6922],\n",
      "        [0.6989],\n",
      "        [0.7065],\n",
      "        [0.6559],\n",
      "        [0.7410],\n",
      "        [0.6958],\n",
      "        [0.7348],\n",
      "        [0.6625],\n",
      "        [0.6479],\n",
      "        [0.6602],\n",
      "        [0.6847],\n",
      "        [0.6409],\n",
      "        [0.6334],\n",
      "        [0.6735],\n",
      "        [0.6731],\n",
      "        [0.6530],\n",
      "        [0.6517],\n",
      "        [0.6433],\n",
      "        [0.6147],\n",
      "        [0.6590],\n",
      "        [0.6226],\n",
      "        [0.6136],\n",
      "        [0.6867],\n",
      "        [0.6575],\n",
      "        [0.6691],\n",
      "        [0.6485],\n",
      "        [0.6635],\n",
      "        [0.7131],\n",
      "        [0.6675],\n",
      "        [0.6576],\n",
      "        [0.6098],\n",
      "        [0.7361],\n",
      "        [0.6950],\n",
      "        [0.7182],\n",
      "        [0.7462],\n",
      "        [0.6588],\n",
      "        [0.7085],\n",
      "        [0.6023],\n",
      "        [0.6062],\n",
      "        [0.7867],\n",
      "        [0.7339],\n",
      "        [0.6326],\n",
      "        [0.6421],\n",
      "        [0.6900],\n",
      "        [0.6144],\n",
      "        [0.6216],\n",
      "        [0.6580],\n",
      "        [0.6503],\n",
      "        [0.6196],\n",
      "        [0.6125],\n",
      "        [0.6819],\n",
      "        [0.7079],\n",
      "        [0.7045],\n",
      "        [0.6319],\n",
      "        [0.6535],\n",
      "        [0.6170],\n",
      "        [0.6280],\n",
      "        [0.6112],\n",
      "        [0.6037],\n",
      "        [0.7087],\n",
      "        [0.6321],\n",
      "        [0.7129],\n",
      "        [0.6607],\n",
      "        [0.7007],\n",
      "        [0.7507],\n",
      "        [0.6592],\n",
      "        [0.6183],\n",
      "        [0.6031],\n",
      "        [0.6563],\n",
      "        [0.6798],\n",
      "        [0.7691],\n",
      "        [0.6427],\n",
      "        [0.6171],\n",
      "        [0.6568],\n",
      "        [0.6200],\n",
      "        [0.6255],\n",
      "        [0.6182],\n",
      "        [0.7311],\n",
      "        [0.6148],\n",
      "        [0.7158],\n",
      "        [0.6765],\n",
      "        [0.7007],\n",
      "        [0.6505],\n",
      "        [0.7203],\n",
      "        [0.7482],\n",
      "        [0.6427],\n",
      "        [0.6802],\n",
      "        [0.6819],\n",
      "        [0.6131],\n",
      "        [0.6802],\n",
      "        [0.6742],\n",
      "        [0.6586],\n",
      "        [0.7984],\n",
      "        [0.7915],\n",
      "        [0.6349],\n",
      "        [0.6856],\n",
      "        [0.6489],\n",
      "        [0.6604],\n",
      "        [0.6858],\n",
      "        [0.6968],\n",
      "        [0.7405],\n",
      "        [0.6923],\n",
      "        [0.7109],\n",
      "        [0.6856],\n",
      "        [0.7019],\n",
      "        [0.6481],\n",
      "        [0.6870],\n",
      "        [0.7682],\n",
      "        [0.6608],\n",
      "        [0.6628],\n",
      "        [0.6444],\n",
      "        [0.6769],\n",
      "        [0.6476],\n",
      "        [0.6525],\n",
      "        [0.6547],\n",
      "        [0.6724],\n",
      "        [0.6695],\n",
      "        [0.7590],\n",
      "        [0.6373],\n",
      "        [0.6494],\n",
      "        [0.7070],\n",
      "        [0.7229],\n",
      "        [0.6453],\n",
      "        [0.6583],\n",
      "        [0.6495],\n",
      "        [0.6871],\n",
      "        [0.7215],\n",
      "        [0.6439],\n",
      "        [0.6429],\n",
      "        [0.6601],\n",
      "        [0.6503],\n",
      "        [0.6192],\n",
      "        [0.6406],\n",
      "        [0.7111],\n",
      "        [0.6652],\n",
      "        [0.7082],\n",
      "        [0.7139],\n",
      "        [0.6653],\n",
      "        [0.6726],\n",
      "        [0.7563],\n",
      "        [0.6121],\n",
      "        [0.6075],\n",
      "        [0.6329],\n",
      "        [0.6628],\n",
      "        [0.6275],\n",
      "        [0.6453],\n",
      "        [0.7607],\n",
      "        [0.7386],\n",
      "        [0.6589],\n",
      "        [0.6548],\n",
      "        [0.6827],\n",
      "        [0.6314],\n",
      "        [0.6520],\n",
      "        [0.6530],\n",
      "        [0.6347],\n",
      "        [0.7007],\n",
      "        [0.6998],\n",
      "        [0.6236],\n",
      "        [0.7440],\n",
      "        [0.6153],\n",
      "        [0.6428],\n",
      "        [0.6008],\n",
      "        [0.7327],\n",
      "        [0.6262],\n",
      "        [0.6137],\n",
      "        [0.6509],\n",
      "        [0.6449],\n",
      "        [0.6630],\n",
      "        [0.6955],\n",
      "        [0.6984],\n",
      "        [0.6218],\n",
      "        [0.6924],\n",
      "        [0.6261],\n",
      "        [0.7194],\n",
      "        [0.6538],\n",
      "        [0.7061],\n",
      "        [0.6803],\n",
      "        [0.6459],\n",
      "        [0.7321],\n",
      "        [0.7117],\n",
      "        [0.7292],\n",
      "        [0.7091],\n",
      "        [0.6179],\n",
      "        [0.7347],\n",
      "        [0.6677],\n",
      "        [0.6587],\n",
      "        [0.6273],\n",
      "        [0.6428],\n",
      "        [0.6265],\n",
      "        [0.6318],\n",
      "        [0.6775],\n",
      "        [0.6703],\n",
      "        [0.6791],\n",
      "        [0.6554],\n",
      "        [0.6728],\n",
      "        [0.6955],\n",
      "        [0.6493],\n",
      "        [0.6765],\n",
      "        [0.6816],\n",
      "        [0.7033],\n",
      "        [0.6792],\n",
      "        [0.6638],\n",
      "        [0.6966],\n",
      "        [0.7078],\n",
      "        [0.6463],\n",
      "        [0.6592],\n",
      "        [0.6676],\n",
      "        [0.7019],\n",
      "        [0.7022],\n",
      "        [0.6516],\n",
      "        [0.6507],\n",
      "        [0.6431],\n",
      "        [0.6899],\n",
      "        [0.6724],\n",
      "        [0.7027],\n",
      "        [0.6782],\n",
      "        [0.6334],\n",
      "        [0.6683],\n",
      "        [0.6681],\n",
      "        [0.6958],\n",
      "        [0.6604],\n",
      "        [0.6513],\n",
      "        [0.6119],\n",
      "        [0.6160],\n",
      "        [0.6681],\n",
      "        [0.7330],\n",
      "        [0.6603],\n",
      "        [0.6056],\n",
      "        [0.6080],\n",
      "        [0.6988],\n",
      "        [0.7003],\n",
      "        [0.6162],\n",
      "        [0.6431],\n",
      "        [0.7426],\n",
      "        [0.7126],\n",
      "        [0.7092],\n",
      "        [0.6451],\n",
      "        [0.7337],\n",
      "        [0.6316],\n",
      "        [0.6225],\n",
      "        [0.6250],\n",
      "        [0.6612],\n",
      "        [0.6588],\n",
      "        [0.6653],\n",
      "        [0.6609],\n",
      "        [0.7122],\n",
      "        [0.6904],\n",
      "        [0.6856],\n",
      "        [0.6778],\n",
      "        [0.7362],\n",
      "        [0.7310],\n",
      "        [0.6458],\n",
      "        [0.6652],\n",
      "        [0.6503],\n",
      "        [0.6524],\n",
      "        [0.6284],\n",
      "        [0.6665],\n",
      "        [0.6178],\n",
      "        [0.6240],\n",
      "        [0.6588],\n",
      "        [0.6587],\n",
      "        [0.6282],\n",
      "        [0.5972],\n",
      "        [0.5966],\n",
      "        [0.6631],\n",
      "        [0.6638],\n",
      "        [0.6917],\n",
      "        [0.6413],\n",
      "        [0.6509],\n",
      "        [0.6491],\n",
      "        [0.6458],\n",
      "        [0.6149],\n",
      "        [0.6477],\n",
      "        [0.6654],\n",
      "        [0.6185],\n",
      "        [0.6562],\n",
      "        [0.6632],\n",
      "        [0.6611],\n",
      "        [0.6491],\n",
      "        [0.6435],\n",
      "        [0.6403],\n",
      "        [0.6430],\n",
      "        [0.7457],\n",
      "        [0.6238],\n",
      "        [0.6351],\n",
      "        [0.6234],\n",
      "        [0.6365],\n",
      "        [0.6508],\n",
      "        [0.6286],\n",
      "        [0.6100],\n",
      "        [0.6846],\n",
      "        [0.6254],\n",
      "        [0.6391],\n",
      "        [0.7263],\n",
      "        [0.6667],\n",
      "        [0.6304],\n",
      "        [0.6388],\n",
      "        [0.6314],\n",
      "        [0.6366],\n",
      "        [0.7102],\n",
      "        [0.6320],\n",
      "        [0.6206],\n",
      "        [0.7650],\n",
      "        [0.6451],\n",
      "        [0.7158],\n",
      "        [0.6928],\n",
      "        [0.7342],\n",
      "        [0.6374],\n",
      "        [0.6716],\n",
      "        [0.6649],\n",
      "        [0.7001],\n",
      "        [0.6407],\n",
      "        [0.6336],\n",
      "        [0.7540],\n",
      "        [0.6468],\n",
      "        [0.6333],\n",
      "        [0.6667],\n",
      "        [0.6331],\n",
      "        [0.6311],\n",
      "        [0.6423],\n",
      "        [0.6501],\n",
      "        [0.6365],\n",
      "        [0.6877],\n",
      "        [0.6489],\n",
      "        [0.6725],\n",
      "        [0.7274],\n",
      "        [0.6294],\n",
      "        [0.6264],\n",
      "        [0.6570],\n",
      "        [0.6275],\n",
      "        [0.6309],\n",
      "        [0.7278],\n",
      "        [0.6917],\n",
      "        [0.6748],\n",
      "        [0.6086],\n",
      "        [0.6391],\n",
      "        [0.7365],\n",
      "        [0.6987],\n",
      "        [0.7100],\n",
      "        [0.6666],\n",
      "        [0.6252],\n",
      "        [0.7268],\n",
      "        [0.6935],\n",
      "        [0.7199],\n",
      "        [0.7474],\n",
      "        [0.6581],\n",
      "        [0.6531],\n",
      "        [0.6034],\n",
      "        [0.6405],\n",
      "        [0.7452],\n",
      "        [0.6025],\n",
      "        [0.6626],\n",
      "        [0.7494],\n",
      "        [0.7320],\n",
      "        [0.6702],\n",
      "        [0.6833],\n",
      "        [0.7631],\n",
      "        [0.7335],\n",
      "        [0.6392],\n",
      "        [0.7646],\n",
      "        [0.7121],\n",
      "        [0.6382],\n",
      "        [0.6761],\n",
      "        [0.6825],\n",
      "        [0.7394],\n",
      "        [0.7586],\n",
      "        [0.7257],\n",
      "        [0.6672],\n",
      "        [0.6698],\n",
      "        [0.6792],\n",
      "        [0.6565],\n",
      "        [0.7157],\n",
      "        [0.6936],\n",
      "        [0.6879],\n",
      "        [0.7061],\n",
      "        [0.6715],\n",
      "        [0.6829],\n",
      "        [0.6635],\n",
      "        [0.6860],\n",
      "        [0.7069],\n",
      "        [0.7069],\n",
      "        [0.6771],\n",
      "        [0.7133],\n",
      "        [0.6610],\n",
      "        [0.7252],\n",
      "        [0.6812],\n",
      "        [0.6654],\n",
      "        [0.6732],\n",
      "        [0.7081],\n",
      "        [0.6920],\n",
      "        [0.6854],\n",
      "        [0.6694],\n",
      "        [0.7140],\n",
      "        [0.7330],\n",
      "        [0.7329],\n",
      "        [0.7187],\n",
      "        [0.6609],\n",
      "        [0.6280],\n",
      "        [0.7545],\n",
      "        [0.7623],\n",
      "        [0.6026],\n",
      "        [0.7588],\n",
      "        [0.6342],\n",
      "        [0.6494],\n",
      "        [0.6346],\n",
      "        [0.6337],\n",
      "        [0.7264],\n",
      "        [0.6997],\n",
      "        [0.6495],\n",
      "        [0.7094],\n",
      "        [0.6772],\n",
      "        [0.7677],\n",
      "        [0.7613],\n",
      "        [0.7353],\n",
      "        [0.7629],\n",
      "        [0.6827],\n",
      "        [0.7643],\n",
      "        [0.7345],\n",
      "        [0.7388],\n",
      "        [0.7770],\n",
      "        [0.6367],\n",
      "        [0.7377],\n",
      "        [0.7352],\n",
      "        [0.7806],\n",
      "        [0.6471],\n",
      "        [0.7333],\n",
      "        [0.7511],\n",
      "        [0.7042],\n",
      "        [0.7207],\n",
      "        [0.6407],\n",
      "        [0.6880],\n",
      "        [0.6353],\n",
      "        [0.7274],\n",
      "        [0.7326],\n",
      "        [0.6690],\n",
      "        [0.7352],\n",
      "        [0.7654],\n",
      "        [0.7174],\n",
      "        [0.7301],\n",
      "        [0.7762],\n",
      "        [0.7213],\n",
      "        [0.7623],\n",
      "        [0.7290],\n",
      "        [0.7312],\n",
      "        [0.7142],\n",
      "        [0.6662],\n",
      "        [0.6764],\n",
      "        [0.7280],\n",
      "        [0.6342],\n",
      "        [0.6745],\n",
      "        [0.6889],\n",
      "        [0.5941],\n",
      "        [0.7052],\n",
      "        [0.6550],\n",
      "        [0.6318],\n",
      "        [0.7226],\n",
      "        [0.6630],\n",
      "        [0.7447],\n",
      "        [0.7182],\n",
      "        [0.6535],\n",
      "        [0.6901],\n",
      "        [0.6787],\n",
      "        [0.6889],\n",
      "        [0.7205],\n",
      "        [0.6637],\n",
      "        [0.7122],\n",
      "        [0.6762],\n",
      "        [0.6845],\n",
      "        [0.6811]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "hypothesis = torch.sigmoid(x_train.matmul(W) + b)\n",
    "print(hypothesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True]])\n"
     ]
    }
   ],
   "source": [
    "prediction = hypothesis >= torch.FloatTensor([0.5])\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0\n",
       "0    1\n",
       "1    1\n",
       "2    1\n",
       "3    1\n",
       "4    1\n",
       "..  ..\n",
       "995  1\n",
       "996  1\n",
       "997  1\n",
       "998  1\n",
       "999  1\n",
       "\n",
       "[1000 rows x 1 columns]"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction=pd.DataFrame(prediction)\n",
    "prediction=prediction.replace([True,False],[1,0])\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7117],\n",
      "        [0.6638],\n",
      "        [0.6497],\n",
      "        [0.6189],\n",
      "        [0.6085],\n",
      "        [0.6206],\n",
      "        [0.7385],\n",
      "        [0.6414],\n",
      "        [0.6605],\n",
      "        [0.7508],\n",
      "        [0.6485],\n",
      "        [0.7712],\n",
      "        [0.6543],\n",
      "        [0.6632],\n",
      "        [0.6357],\n",
      "        [0.6130],\n",
      "        [0.6533],\n",
      "        [0.6229],\n",
      "        [0.6282],\n",
      "        [0.6496],\n",
      "        [0.6451],\n",
      "        [0.7149],\n",
      "        [0.6769],\n",
      "        [0.7030],\n",
      "        [0.6205],\n",
      "        [0.6871],\n",
      "        [0.6521],\n",
      "        [0.6412],\n",
      "        [0.6882],\n",
      "        [0.6632],\n",
      "        [0.6466],\n",
      "        [0.6290],\n",
      "        [0.6302],\n",
      "        [0.6985],\n",
      "        [0.7007],\n",
      "        [0.6262],\n",
      "        [0.6527],\n",
      "        [0.6621],\n",
      "        [0.7332],\n",
      "        [0.6838],\n",
      "        [0.6337],\n",
      "        [0.6773],\n",
      "        [0.6660],\n",
      "        [0.6788],\n",
      "        [0.7306],\n",
      "        [0.6803],\n",
      "        [0.6486],\n",
      "        [0.6133],\n",
      "        [0.6479],\n",
      "        [0.7316],\n",
      "        [0.7215],\n",
      "        [0.6476],\n",
      "        [0.6264],\n",
      "        [0.6648],\n",
      "        [0.6645],\n",
      "        [0.7152],\n",
      "        [0.6982],\n",
      "        [0.7629],\n",
      "        [0.7508],\n",
      "        [0.6392],\n",
      "        [0.6064],\n",
      "        [0.7231],\n",
      "        [0.6393],\n",
      "        [0.7187],\n",
      "        [0.6235],\n",
      "        [0.6543],\n",
      "        [0.6665],\n",
      "        [0.7075],\n",
      "        [0.7214],\n",
      "        [0.7415],\n",
      "        [0.6498],\n",
      "        [0.6348],\n",
      "        [0.7415],\n",
      "        [0.7189],\n",
      "        [0.6362],\n",
      "        [0.7499],\n",
      "        [0.6665],\n",
      "        [0.6595],\n",
      "        [0.7145],\n",
      "        [0.6867],\n",
      "        [0.7005],\n",
      "        [0.7023],\n",
      "        [0.7265],\n",
      "        [0.6648],\n",
      "        [0.6817],\n",
      "        [0.6661],\n",
      "        [0.6294],\n",
      "        [0.6323],\n",
      "        [0.6559],\n",
      "        [0.6353],\n",
      "        [0.6172],\n",
      "        [0.6387],\n",
      "        [0.7499],\n",
      "        [0.7115],\n",
      "        [0.6431],\n",
      "        [0.6193],\n",
      "        [0.7909],\n",
      "        [0.6621],\n",
      "        [0.6413],\n",
      "        [0.6588],\n",
      "        [0.7776],\n",
      "        [0.7184],\n",
      "        [0.7179],\n",
      "        [0.7292],\n",
      "        [0.6961],\n",
      "        [0.6689],\n",
      "        [0.6686],\n",
      "        [0.6076],\n",
      "        [0.6310],\n",
      "        [0.6504],\n",
      "        [0.6680],\n",
      "        [0.6414],\n",
      "        [0.6373],\n",
      "        [0.6901],\n",
      "        [0.6054],\n",
      "        [0.6799],\n",
      "        [0.6644],\n",
      "        [0.7302],\n",
      "        [0.6962],\n",
      "        [0.7787],\n",
      "        [0.6957],\n",
      "        [0.6766],\n",
      "        [0.6903],\n",
      "        [0.6233],\n",
      "        [0.6628],\n",
      "        [0.6708],\n",
      "        [0.6589],\n",
      "        [0.7444],\n",
      "        [0.7053],\n",
      "        [0.7513],\n",
      "        [0.6546],\n",
      "        [0.7452],\n",
      "        [0.6177],\n",
      "        [0.7641],\n",
      "        [0.7628],\n",
      "        [0.6843],\n",
      "        [0.6852],\n",
      "        [0.5976],\n",
      "        [0.7078],\n",
      "        [0.6710],\n",
      "        [0.6694],\n",
      "        [0.6276],\n",
      "        [0.7788],\n",
      "        [0.6623],\n",
      "        [0.7086],\n",
      "        [0.7737],\n",
      "        [0.7150],\n",
      "        [0.6746],\n",
      "        [0.6584],\n",
      "        [0.7664],\n",
      "        [0.7630],\n",
      "        [0.6392],\n",
      "        [0.7472],\n",
      "        [0.6637],\n",
      "        [0.7449],\n",
      "        [0.6559],\n",
      "        [0.6525],\n",
      "        [0.6371],\n",
      "        [0.6409],\n",
      "        [0.6771],\n",
      "        [0.6208],\n",
      "        [0.6234],\n",
      "        [0.6769],\n",
      "        [0.7833],\n",
      "        [0.6929],\n",
      "        [0.6857],\n",
      "        [0.6598],\n",
      "        [0.6832],\n",
      "        [0.7699],\n",
      "        [0.7093],\n",
      "        [0.6660],\n",
      "        [0.6030],\n",
      "        [0.6095],\n",
      "        [0.7653],\n",
      "        [0.5906],\n",
      "        [0.7037],\n",
      "        [0.7587],\n",
      "        [0.7607],\n",
      "        [0.7762],\n",
      "        [0.6507],\n",
      "        [0.6076],\n",
      "        [0.6113],\n",
      "        [0.7230],\n",
      "        [0.6378],\n",
      "        [0.6925],\n",
      "        [0.7305],\n",
      "        [0.7278],\n",
      "        [0.7691],\n",
      "        [0.6716],\n",
      "        [0.6122],\n",
      "        [0.7327],\n",
      "        [0.6241],\n",
      "        [0.7049],\n",
      "        [0.6381],\n",
      "        [0.6713],\n",
      "        [0.6675],\n",
      "        [0.6568],\n",
      "        [0.6553],\n",
      "        [0.6542],\n",
      "        [0.7383],\n",
      "        [0.7608],\n",
      "        [0.6301],\n",
      "        [0.6599],\n",
      "        [0.6440],\n",
      "        [0.7426],\n",
      "        [0.7158],\n",
      "        [0.6842],\n",
      "        [0.7439],\n",
      "        [0.6739],\n",
      "        [0.6073],\n",
      "        [0.7174],\n",
      "        [0.6386],\n",
      "        [0.6958],\n",
      "        [0.7184],\n",
      "        [0.7114],\n",
      "        [0.7039],\n",
      "        [0.6814],\n",
      "        [0.7276],\n",
      "        [0.6771],\n",
      "        [0.7786],\n",
      "        [0.6730],\n",
      "        [0.6620],\n",
      "        [0.6327],\n",
      "        [0.7644],\n",
      "        [0.7435],\n",
      "        [0.6391],\n",
      "        [0.7614],\n",
      "        [0.6350],\n",
      "        [0.6652],\n",
      "        [0.7691],\n",
      "        [0.6558],\n",
      "        [0.6978],\n",
      "        [0.7222],\n",
      "        [0.6801],\n",
      "        [0.6477],\n",
      "        [0.6671],\n",
      "        [0.7394],\n",
      "        [0.6113],\n",
      "        [0.6489],\n",
      "        [0.6562],\n",
      "        [0.6086],\n",
      "        [0.6872],\n",
      "        [0.6409],\n",
      "        [0.6974],\n",
      "        [0.7195],\n",
      "        [0.7491],\n",
      "        [0.6394],\n",
      "        [0.6276],\n",
      "        [0.7753],\n",
      "        [0.6443],\n",
      "        [0.7558],\n",
      "        [0.6677],\n",
      "        [0.6943],\n",
      "        [0.6963],\n",
      "        [0.6917],\n",
      "        [0.7392],\n",
      "        [0.7349],\n",
      "        [0.6151],\n",
      "        [0.7102],\n",
      "        [0.7436],\n",
      "        [0.6600],\n",
      "        [0.6595],\n",
      "        [0.6601],\n",
      "        [0.6389],\n",
      "        [0.6489],\n",
      "        [0.6737],\n",
      "        [0.6731],\n",
      "        [0.7586],\n",
      "        [0.7520],\n",
      "        [0.7054],\n",
      "        [0.7094],\n",
      "        [0.7393],\n",
      "        [0.6177],\n",
      "        [0.7301],\n",
      "        [0.6347],\n",
      "        [0.6335],\n",
      "        [0.6838],\n",
      "        [0.7162],\n",
      "        [0.6431],\n",
      "        [0.6745],\n",
      "        [0.7393],\n",
      "        [0.7365],\n",
      "        [0.7353],\n",
      "        [0.6675],\n",
      "        [0.6711],\n",
      "        [0.6628],\n",
      "        [0.7393],\n",
      "        [0.6552],\n",
      "        [0.6391],\n",
      "        [0.7060],\n",
      "        [0.6996],\n",
      "        [0.7785],\n",
      "        [0.6281],\n",
      "        [0.6980],\n",
      "        [0.6696],\n",
      "        [0.7400],\n",
      "        [0.6817],\n",
      "        [0.6569],\n",
      "        [0.6304],\n",
      "        [0.7150],\n",
      "        [0.6852],\n",
      "        [0.7133],\n",
      "        [0.7042],\n",
      "        [0.8002],\n",
      "        [0.6938],\n",
      "        [0.7462],\n",
      "        [0.7089],\n",
      "        [0.7004],\n",
      "        [0.7214],\n",
      "        [0.6268],\n",
      "        [0.6322],\n",
      "        [0.6856],\n",
      "        [0.6603],\n",
      "        [0.6594],\n",
      "        [0.6012],\n",
      "        [0.6290],\n",
      "        [0.6330],\n",
      "        [0.6027],\n",
      "        [0.6749],\n",
      "        [0.6498],\n",
      "        [0.6710],\n",
      "        [0.7053],\n",
      "        [0.6247],\n",
      "        [0.6271],\n",
      "        [0.6834],\n",
      "        [0.6484],\n",
      "        [0.7076],\n",
      "        [0.6775],\n",
      "        [0.6135],\n",
      "        [0.6318],\n",
      "        [0.7007],\n",
      "        [0.7439],\n",
      "        [0.6119],\n",
      "        [0.6726],\n",
      "        [0.7920],\n",
      "        [0.6184],\n",
      "        [0.7190],\n",
      "        [0.7395],\n",
      "        [0.6815],\n",
      "        [0.6861],\n",
      "        [0.7130],\n",
      "        [0.6430],\n",
      "        [0.6345],\n",
      "        [0.7139],\n",
      "        [0.7265],\n",
      "        [0.6354],\n",
      "        [0.7146],\n",
      "        [0.6545],\n",
      "        [0.6793],\n",
      "        [0.7658],\n",
      "        [0.5992],\n",
      "        [0.6072],\n",
      "        [0.5920],\n",
      "        [0.7098],\n",
      "        [0.6608],\n",
      "        [0.6531],\n",
      "        [0.6605],\n",
      "        [0.6640],\n",
      "        [0.7007],\n",
      "        [0.6476],\n",
      "        [0.6637],\n",
      "        [0.6581],\n",
      "        [0.6409],\n",
      "        [0.7815],\n",
      "        [0.7615],\n",
      "        [0.7687],\n",
      "        [0.6638],\n",
      "        [0.6843],\n",
      "        [0.6515],\n",
      "        [0.6847],\n",
      "        [0.7135],\n",
      "        [0.6046],\n",
      "        [0.6091],\n",
      "        [0.7147],\n",
      "        [0.6210],\n",
      "        [0.7303],\n",
      "        [0.7568],\n",
      "        [0.6405],\n",
      "        [0.7532],\n",
      "        [0.6571],\n",
      "        [0.6306],\n",
      "        [0.6613],\n",
      "        [0.6866],\n",
      "        [0.6175],\n",
      "        [0.6454],\n",
      "        [0.6035],\n",
      "        [0.6422],\n",
      "        [0.6425],\n",
      "        [0.6422],\n",
      "        [0.6806],\n",
      "        [0.6251],\n",
      "        [0.6402],\n",
      "        [0.6442],\n",
      "        [0.6939],\n",
      "        [0.6369],\n",
      "        [0.6476],\n",
      "        [0.6253],\n",
      "        [0.7450],\n",
      "        [0.6083],\n",
      "        [0.6572],\n",
      "        [0.6421],\n",
      "        [0.6778],\n",
      "        [0.6604],\n",
      "        [0.6633],\n",
      "        [0.6975],\n",
      "        [0.6682],\n",
      "        [0.7402],\n",
      "        [0.7083],\n",
      "        [0.6974],\n",
      "        [0.6160],\n",
      "        [0.6296],\n",
      "        [0.7363],\n",
      "        [0.6531],\n",
      "        [0.6260],\n",
      "        [0.7141],\n",
      "        [0.6195],\n",
      "        [0.7213],\n",
      "        [0.6934],\n",
      "        [0.6437],\n",
      "        [0.6412],\n",
      "        [0.7232],\n",
      "        [0.7596],\n",
      "        [0.7561],\n",
      "        [0.6541],\n",
      "        [0.6373],\n",
      "        [0.7012],\n",
      "        [0.6814],\n",
      "        [0.7296],\n",
      "        [0.7021],\n",
      "        [0.6395],\n",
      "        [0.7236],\n",
      "        [0.6827],\n",
      "        [0.7781],\n",
      "        [0.7342],\n",
      "        [0.6624],\n",
      "        [0.6909],\n",
      "        [0.6391],\n",
      "        [0.6797],\n",
      "        [0.6245],\n",
      "        [0.6419],\n",
      "        [0.7208],\n",
      "        [0.7322],\n",
      "        [0.7184],\n",
      "        [0.6564],\n",
      "        [0.7075],\n",
      "        [0.6370],\n",
      "        [0.7009],\n",
      "        [0.6525],\n",
      "        [0.7372],\n",
      "        [0.7157],\n",
      "        [0.7417],\n",
      "        [0.6939],\n",
      "        [0.7441],\n",
      "        [0.7132],\n",
      "        [0.7317],\n",
      "        [0.7096],\n",
      "        [0.7011],\n",
      "        [0.7232],\n",
      "        [0.7236],\n",
      "        [0.7585],\n",
      "        [0.7245],\n",
      "        [0.7056],\n",
      "        [0.7234],\n",
      "        [0.7125],\n",
      "        [0.6374],\n",
      "        [0.6578],\n",
      "        [0.5782],\n",
      "        [0.6243],\n",
      "        [0.7152],\n",
      "        [0.7069],\n",
      "        [0.6483],\n",
      "        [0.6521],\n",
      "        [0.6456],\n",
      "        [0.6750],\n",
      "        [0.6453],\n",
      "        [0.6596],\n",
      "        [0.6404],\n",
      "        [0.6290],\n",
      "        [0.7419],\n",
      "        [0.7228],\n",
      "        [0.7242],\n",
      "        [0.6579],\n",
      "        [0.7128],\n",
      "        [0.7503],\n",
      "        [0.6264],\n",
      "        [0.6210],\n",
      "        [0.5941],\n",
      "        [0.7511],\n",
      "        [0.6540],\n",
      "        [0.6183],\n",
      "        [0.6879],\n",
      "        [0.6535],\n",
      "        [0.6449],\n",
      "        [0.6180],\n",
      "        [0.6939],\n",
      "        [0.6682],\n",
      "        [0.6979],\n",
      "        [0.6910],\n",
      "        [0.7011],\n",
      "        [0.6934],\n",
      "        [0.6584],\n",
      "        [0.7121],\n",
      "        [0.6320],\n",
      "        [0.6279],\n",
      "        [0.6110],\n",
      "        [0.7806],\n",
      "        [0.7633],\n",
      "        [0.7151],\n",
      "        [0.6386],\n",
      "        [0.6620],\n",
      "        [0.6942],\n",
      "        [0.6696],\n",
      "        [0.6732],\n",
      "        [0.6977],\n",
      "        [0.6432],\n",
      "        [0.6540],\n",
      "        [0.6475],\n",
      "        [0.6764],\n",
      "        [0.6639],\n",
      "        [0.6721],\n",
      "        [0.7043],\n",
      "        [0.7296],\n",
      "        [0.6928],\n",
      "        [0.6640],\n",
      "        [0.6356],\n",
      "        [0.6403],\n",
      "        [0.7338],\n",
      "        [0.6907],\n",
      "        [0.7202],\n",
      "        [0.6181],\n",
      "        [0.6220],\n",
      "        [0.6092],\n",
      "        [0.6707],\n",
      "        [0.6299],\n",
      "        [0.6306],\n",
      "        [0.6711],\n",
      "        [0.7381],\n",
      "        [0.7349],\n",
      "        [0.6711],\n",
      "        [0.6181],\n",
      "        [0.6302],\n",
      "        [0.6751],\n",
      "        [0.6780],\n",
      "        [0.6256],\n",
      "        [0.7485],\n",
      "        [0.7090],\n",
      "        [0.7051],\n",
      "        [0.6869],\n",
      "        [0.6897],\n",
      "        [0.6631],\n",
      "        [0.7184],\n",
      "        [0.6604],\n",
      "        [0.6188],\n",
      "        [0.6936],\n",
      "        [0.6277],\n",
      "        [0.6792],\n",
      "        [0.6852],\n",
      "        [0.6381],\n",
      "        [0.7831],\n",
      "        [0.6777],\n",
      "        [0.6961],\n",
      "        [0.6752],\n",
      "        [0.6944],\n",
      "        [0.6632],\n",
      "        [0.6806],\n",
      "        [0.6531],\n",
      "        [0.7594],\n",
      "        [0.7463],\n",
      "        [0.7307],\n",
      "        [0.6059],\n",
      "        [0.6073],\n",
      "        [0.6840],\n",
      "        [0.7156],\n",
      "        [0.6319],\n",
      "        [0.6389],\n",
      "        [0.7074],\n",
      "        [0.7168],\n",
      "        [0.6610],\n",
      "        [0.6814],\n",
      "        [0.6174],\n",
      "        [0.6674],\n",
      "        [0.6622],\n",
      "        [0.7302],\n",
      "        [0.7117],\n",
      "        [0.7464],\n",
      "        [0.7376],\n",
      "        [0.6676],\n",
      "        [0.7645],\n",
      "        [0.6486],\n",
      "        [0.6711],\n",
      "        [0.6830],\n",
      "        [0.6302],\n",
      "        [0.6354],\n",
      "        [0.6052],\n",
      "        [0.6809],\n",
      "        [0.7745],\n",
      "        [0.7185],\n",
      "        [0.6732],\n",
      "        [0.6200],\n",
      "        [0.6753],\n",
      "        [0.6379],\n",
      "        [0.6518],\n",
      "        [0.6045],\n",
      "        [0.6215],\n",
      "        [0.6607],\n",
      "        [0.7792],\n",
      "        [0.6019],\n",
      "        [0.6278],\n",
      "        [0.6655],\n",
      "        [0.6779],\n",
      "        [0.7176],\n",
      "        [0.6765],\n",
      "        [0.6376],\n",
      "        [0.6834],\n",
      "        [0.7629],\n",
      "        [0.6540],\n",
      "        [0.7032],\n",
      "        [0.6953],\n",
      "        [0.6164],\n",
      "        [0.6312],\n",
      "        [0.6956],\n",
      "        [0.6599],\n",
      "        [0.6060],\n",
      "        [0.7111],\n",
      "        [0.6970],\n",
      "        [0.7008],\n",
      "        [0.6973],\n",
      "        [0.7623],\n",
      "        [0.7318],\n",
      "        [0.7168],\n",
      "        [0.6824],\n",
      "        [0.6955],\n",
      "        [0.6048],\n",
      "        [0.6355],\n",
      "        [0.6034],\n",
      "        [0.5984],\n",
      "        [0.6585],\n",
      "        [0.6489],\n",
      "        [0.6022],\n",
      "        [0.6790],\n",
      "        [0.7414],\n",
      "        [0.7073],\n",
      "        [0.7589],\n",
      "        [0.6465],\n",
      "        [0.7313],\n",
      "        [0.6498],\n",
      "        [0.6349],\n",
      "        [0.7643],\n",
      "        [0.6473],\n",
      "        [0.6938],\n",
      "        [0.6945],\n",
      "        [0.5919],\n",
      "        [0.6439],\n",
      "        [0.6410],\n",
      "        [0.6716],\n",
      "        [0.6574],\n",
      "        [0.7359],\n",
      "        [0.7229],\n",
      "        [0.7017],\n",
      "        [0.6285],\n",
      "        [0.7010],\n",
      "        [0.7146],\n",
      "        [0.7220],\n",
      "        [0.6462],\n",
      "        [0.6595],\n",
      "        [0.7365],\n",
      "        [0.7356],\n",
      "        [0.7178],\n",
      "        [0.6464],\n",
      "        [0.7294],\n",
      "        [0.6517],\n",
      "        [0.6483],\n",
      "        [0.6336],\n",
      "        [0.6099],\n",
      "        [0.7275],\n",
      "        [0.6585],\n",
      "        [0.6403],\n",
      "        [0.6956],\n",
      "        [0.7256],\n",
      "        [0.6842],\n",
      "        [0.6726],\n",
      "        [0.6920],\n",
      "        [0.6527],\n",
      "        [0.7273],\n",
      "        [0.6053],\n",
      "        [0.6137],\n",
      "        [0.6634],\n",
      "        [0.6634],\n",
      "        [0.6350],\n",
      "        [0.6424],\n",
      "        [0.6216],\n",
      "        [0.6587],\n",
      "        [0.7224],\n",
      "        [0.6873],\n",
      "        [0.7341],\n",
      "        [0.7380],\n",
      "        [0.6928],\n",
      "        [0.6364],\n",
      "        [0.6280],\n",
      "        [0.7240],\n",
      "        [0.6324],\n",
      "        [0.6950],\n",
      "        [0.6379],\n",
      "        [0.6763],\n",
      "        [0.6686],\n",
      "        [0.7152],\n",
      "        [0.6842],\n",
      "        [0.7041],\n",
      "        [0.7030],\n",
      "        [0.6316],\n",
      "        [0.6792],\n",
      "        [0.6641],\n",
      "        [0.7126],\n",
      "        [0.6301],\n",
      "        [0.6261],\n",
      "        [0.6372],\n",
      "        [0.6236],\n",
      "        [0.6092],\n",
      "        [0.6713],\n",
      "        [0.6602],\n",
      "        [0.6370],\n",
      "        [0.6624],\n",
      "        [0.6259],\n",
      "        [0.6428],\n",
      "        [0.6637],\n",
      "        [0.6742],\n",
      "        [0.7201],\n",
      "        [0.6383],\n",
      "        [0.7966],\n",
      "        [0.7054],\n",
      "        [0.6815],\n",
      "        [0.6667],\n",
      "        [0.7449],\n",
      "        [0.6752],\n",
      "        [0.7299],\n",
      "        [0.6322],\n",
      "        [0.6488],\n",
      "        [0.6645],\n",
      "        [0.7105],\n",
      "        [0.7892],\n",
      "        [0.6551],\n",
      "        [0.7075],\n",
      "        [0.6244],\n",
      "        [0.7386],\n",
      "        [0.7380],\n",
      "        [0.6925],\n",
      "        [0.7004],\n",
      "        [0.6060],\n",
      "        [0.6372],\n",
      "        [0.6876],\n",
      "        [0.6571],\n",
      "        [0.7174],\n",
      "        [0.6558],\n",
      "        [0.6455],\n",
      "        [0.7534],\n",
      "        [0.6597],\n",
      "        [0.6579],\n",
      "        [0.7182],\n",
      "        [0.6273],\n",
      "        [0.6955],\n",
      "        [0.7280],\n",
      "        [0.7135],\n",
      "        [0.7551],\n",
      "        [0.7221],\n",
      "        [0.6722],\n",
      "        [0.6609],\n",
      "        [0.7162],\n",
      "        [0.6168],\n",
      "        [0.6159],\n",
      "        [0.6744],\n",
      "        [0.7882],\n",
      "        [0.7622],\n",
      "        [0.6210],\n",
      "        [0.6539],\n",
      "        [0.6814],\n",
      "        [0.6981],\n",
      "        [0.6491],\n",
      "        [0.6722],\n",
      "        [0.7350],\n",
      "        [0.7510],\n",
      "        [0.6534],\n",
      "        [0.6406],\n",
      "        [0.6420],\n",
      "        [0.6228],\n",
      "        [0.6664],\n",
      "        [0.6450],\n",
      "        [0.6112],\n",
      "        [0.6155],\n",
      "        [0.6513],\n",
      "        [0.6962],\n",
      "        [0.7093],\n",
      "        [0.5934],\n",
      "        [0.6836],\n",
      "        [0.6389],\n",
      "        [0.6180],\n",
      "        [0.6170],\n",
      "        [0.7080],\n",
      "        [0.7172],\n",
      "        [0.6169],\n",
      "        [0.6732],\n",
      "        [0.6248],\n",
      "        [0.6740],\n",
      "        [0.6204],\n",
      "        [0.7087],\n",
      "        [0.6680],\n",
      "        [0.7164],\n",
      "        [0.6993],\n",
      "        [0.6019],\n",
      "        [0.7160],\n",
      "        [0.6364],\n",
      "        [0.6488],\n",
      "        [0.6177],\n",
      "        [0.6658],\n",
      "        [0.7155],\n",
      "        [0.7304],\n",
      "        [0.6464],\n",
      "        [0.6285],\n",
      "        [0.6477],\n",
      "        [0.6824],\n",
      "        [0.6473],\n",
      "        [0.7377],\n",
      "        [0.6553],\n",
      "        [0.7467],\n",
      "        [0.7259],\n",
      "        [0.6752],\n",
      "        [0.6190],\n",
      "        [0.6880],\n",
      "        [0.6730],\n",
      "        [0.7460],\n",
      "        [0.6440],\n",
      "        [0.6614],\n",
      "        [0.6226],\n",
      "        [0.7346],\n",
      "        [0.7625],\n",
      "        [0.6840],\n",
      "        [0.7743],\n",
      "        [0.7182],\n",
      "        [0.7346],\n",
      "        [0.7383],\n",
      "        [0.7222],\n",
      "        [0.6128],\n",
      "        [0.6744],\n",
      "        [0.7044],\n",
      "        [0.7285],\n",
      "        [0.6858],\n",
      "        [0.6956],\n",
      "        [0.7675],\n",
      "        [0.7712],\n",
      "        [0.6349],\n",
      "        [0.5874],\n",
      "        [0.7512],\n",
      "        [0.7392],\n",
      "        [0.5963],\n",
      "        [0.7292],\n",
      "        [0.5851],\n",
      "        [0.5997],\n",
      "        [0.6616],\n",
      "        [0.5861],\n",
      "        [0.6143],\n",
      "        [0.6719],\n",
      "        [0.7157],\n",
      "        [0.7289],\n",
      "        [0.6941],\n",
      "        [0.6944],\n",
      "        [0.7297],\n",
      "        [0.6517],\n",
      "        [0.7070],\n",
      "        [0.6279],\n",
      "        [0.6506],\n",
      "        [0.6281],\n",
      "        [0.7656],\n",
      "        [0.7249],\n",
      "        [0.6403],\n",
      "        [0.6593],\n",
      "        [0.6340],\n",
      "        [0.6711],\n",
      "        [0.6796],\n",
      "        [0.6994],\n",
      "        [0.7090],\n",
      "        [0.7259],\n",
      "        [0.6230],\n",
      "        [0.6362],\n",
      "        [0.7450],\n",
      "        [0.6772],\n",
      "        [0.6334],\n",
      "        [0.6705],\n",
      "        [0.7050],\n",
      "        [0.7212],\n",
      "        [0.7019],\n",
      "        [0.7490],\n",
      "        [0.6221],\n",
      "        [0.6211],\n",
      "        [0.6632],\n",
      "        [0.6521],\n",
      "        [0.6938],\n",
      "        [0.7115],\n",
      "        [0.7292],\n",
      "        [0.6634],\n",
      "        [0.6110],\n",
      "        [0.6991],\n",
      "        [0.7322],\n",
      "        [0.7037],\n",
      "        [0.6413],\n",
      "        [0.7526],\n",
      "        [0.6786],\n",
      "        [0.6390],\n",
      "        [0.6497],\n",
      "        [0.6649],\n",
      "        [0.7321],\n",
      "        [0.7499],\n",
      "        [0.6419],\n",
      "        [0.6303],\n",
      "        [0.6442],\n",
      "        [0.6612],\n",
      "        [0.6899],\n",
      "        [0.6355],\n",
      "        [0.7125],\n",
      "        [0.7566],\n",
      "        [0.6154],\n",
      "        [0.6716],\n",
      "        [0.6800],\n",
      "        [0.6386],\n",
      "        [0.7745],\n",
      "        [0.6291],\n",
      "        [0.6417],\n",
      "        [0.7508],\n",
      "        [0.7509],\n",
      "        [0.7057],\n",
      "        [0.6435],\n",
      "        [0.6270],\n",
      "        [0.6420],\n",
      "        [0.7404],\n",
      "        [0.7418],\n",
      "        [0.6931],\n",
      "        [0.7022],\n",
      "        [0.6585],\n",
      "        [0.6762],\n",
      "        [0.6722],\n",
      "        [0.6653],\n",
      "        [0.7377],\n",
      "        [0.6558],\n",
      "        [0.6578],\n",
      "        [0.6218],\n",
      "        [0.6199],\n",
      "        [0.6133],\n",
      "        [0.6706],\n",
      "        [0.6445],\n",
      "        [0.6236],\n",
      "        [0.7462],\n",
      "        [0.7386],\n",
      "        [0.7231],\n",
      "        [0.6672],\n",
      "        [0.7400],\n",
      "        [0.6641],\n",
      "        [0.6249],\n",
      "        [0.6241],\n",
      "        [0.7199],\n",
      "        [0.6185],\n",
      "        [0.7535],\n",
      "        [0.6236],\n",
      "        [0.6808],\n",
      "        [0.7361],\n",
      "        [0.5899],\n",
      "        [0.6188],\n",
      "        [0.6522],\n",
      "        [0.6280],\n",
      "        [0.7373],\n",
      "        [0.7375],\n",
      "        [0.6745],\n",
      "        [0.7568],\n",
      "        [0.6800],\n",
      "        [0.6551],\n",
      "        [0.6399],\n",
      "        [0.6605],\n",
      "        [0.6723],\n",
      "        [0.6575],\n",
      "        [0.6781],\n",
      "        [0.5924],\n",
      "        [0.6666],\n",
      "        [0.6549],\n",
      "        [0.6223],\n",
      "        [0.7778],\n",
      "        [0.6633],\n",
      "        [0.6580],\n",
      "        [0.6552],\n",
      "        [0.6320],\n",
      "        [0.6893],\n",
      "        [0.6899],\n",
      "        [0.7236],\n",
      "        [0.6981],\n",
      "        [0.6948],\n",
      "        [0.6193],\n",
      "        [0.6190],\n",
      "        [0.6685],\n",
      "        [0.6331],\n",
      "        [0.6244],\n",
      "        [0.6382],\n",
      "        [0.6548],\n",
      "        [0.6783],\n",
      "        [0.6466]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "hypothesis2 = torch.sigmoid(x_test.matmul(W) + b)\n",
    "print(hypothesis2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction2 = hypothesis2 >= torch.FloatTensor([0.5])\n",
    "print(prediction2)\n",
    "\n",
    "prediction2=pd.DataFrame(prediction2)\n",
    "prediction2=prediction2.replace([True,False],[1,0])\n",
    "prediction2"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f68a52d6746b8edec33e9d0848c665d5f17e646007ceb6470c84aeea645ccfe6"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit ('dacon': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
