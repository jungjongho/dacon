{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "import easydict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#데이터 불러오기\n",
    "train = pd.read_csv('C:\\\\Users\\\\user\\\\Desktop\\\\computing\\\\outside\\\\dacon\\\\jobcare_recommend\\\\JobCare_data\\\\train.csv')\n",
    "test = pd.read_csv('C:\\\\Users\\\\user\\\\Desktop\\\\computing\\\\outside\\\\dacon\\\\jobcare_recommend\\\\JobCare_data\\\\test.csv')\n",
    "d_code=pd.read_csv('C:\\\\Users\\\\user\\\\Desktop\\\\computing\\\\outside\\\\dacon\\\\jobcare_recommend\\\\Jobcare_data\\\\속성_D_코드.csv',index_col=0).T.to_dict()\n",
    "h_code=pd.read_csv('C:\\\\Users\\\\user\\\\Desktop\\\\computing\\\\outside\\\\dacon\\\\jobcare_recommend\\\\Jobcare_data\\\\속성_H_코드.csv',index_col=0).T.to_dict()\n",
    "l_code=pd.read_csv('C:\\\\Users\\\\user\\\\Desktop\\\\computing\\\\outside\\\\dacon\\\\jobcare_recommend\\\\Jobcare_data\\\\속성_L_코드.csv',index_col=0).T.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#데이터 재가공\n",
    "def add_code(df, d_code, h_code, l_code):\n",
    "    df = df.copy()   \n",
    "\n",
    "    # D Code\n",
    "    df['person_prefer_d_1_n'] = df['person_prefer_d_1'].apply(lambda x: d_code[x]['속성 D 세분류코드'])\n",
    "    df['person_prefer_d_1_s'] = df['person_prefer_d_1'].apply(lambda x: d_code[x]['속성 D 소분류코드'])\n",
    "    df['person_prefer_d_1_m'] = df['person_prefer_d_1'].apply(lambda x: d_code[x]['속성 D 중분류코드'])\n",
    "    df['person_prefer_d_1_l'] = df['person_prefer_d_1'].apply(lambda x: d_code[x]['속성 D 대분류코드'])\n",
    "\n",
    "    df['person_prefer_d_2_n'] = df['person_prefer_d_2'].apply(lambda x: d_code[x]['속성 D 세분류코드'])\n",
    "    df['person_prefer_d_2_s'] = df['person_prefer_d_2'].apply(lambda x: d_code[x]['속성 D 소분류코드'])\n",
    "    df['person_prefer_d_2_m'] = df['person_prefer_d_2'].apply(lambda x: d_code[x]['속성 D 중분류코드'])\n",
    "    df['person_prefer_d_2_l'] = df['person_prefer_d_2'].apply(lambda x: d_code[x]['속성 D 대분류코드'])\n",
    "\n",
    "    df['person_prefer_d_3_n'] = df['person_prefer_d_3'].apply(lambda x: d_code[x]['속성 D 세분류코드'])\n",
    "    df['person_prefer_d_3_s'] = df['person_prefer_d_3'].apply(lambda x: d_code[x]['속성 D 소분류코드'])\n",
    "    df['person_prefer_d_3_m'] = df['person_prefer_d_3'].apply(lambda x: d_code[x]['속성 D 중분류코드'])\n",
    "    df['person_prefer_d_3_l'] = df['person_prefer_d_3'].apply(lambda x: d_code[x]['속성 D 대분류코드'])\n",
    "\n",
    "    df['contents_attribute_d_n'] = df['contents_attribute_d'].apply(lambda x: d_code[x]['속성 D 세분류코드'])\n",
    "    df['contents_attribute_d_s'] = df['contents_attribute_d'].apply(lambda x: d_code[x]['속성 D 소분류코드'])\n",
    "    df['contents_attribute_d_m'] = df['contents_attribute_d'].apply(lambda x: d_code[x]['속성 D 중분류코드'])\n",
    "    df['contents_attribute_d_l'] = df['contents_attribute_d'].apply(lambda x: d_code[x]['속성 D 대분류코드'])\n",
    "\n",
    "    # H Code\n",
    "    df['person_prefer_h_1_m'] = df['person_prefer_h_1'].apply(lambda x: h_code[x]['속성 H 중분류코드'])\n",
    "    df['person_prefer_h_2_m'] = df['person_prefer_h_2'].apply(lambda x: h_code[x]['속성 H 중분류코드'])\n",
    "    df['person_prefer_h_3_m'] = df['person_prefer_h_3'].apply(lambda x: h_code[x]['속성 H 중분류코드'])\n",
    "    df['contents_attribute_h_m'] = df['contents_attribute_h'].apply(lambda x: h_code[x]['속성 H 중분류코드'])\n",
    "\n",
    "    df['person_prefer_h_1_l'] = df['person_prefer_h_1'].apply(lambda x: h_code[x]['속성 H 대분류코드'])\n",
    "    df['person_prefer_h_2_l'] = df['person_prefer_h_2'].apply(lambda x: h_code[x]['속성 H 대분류코드'])\n",
    "    df['person_prefer_h_3_l'] = df['person_prefer_h_3'].apply(lambda x: h_code[x]['속성 H 대분류코드'])\n",
    "    df['contents_attribute_h_l'] = df['contents_attribute_h'].apply(lambda x: h_code[x]['속성 H 대분류코드'])\n",
    "\n",
    "    # L Code\n",
    "    df['contents_attribute_l_n'] = df['contents_attribute_l'].apply(lambda x: l_code[x]['속성 L 세분류코드'])\n",
    "    df['contents_attribute_l_s'] = df['contents_attribute_l'].apply(lambda x: l_code[x]['속성 L 소분류코드'])\n",
    "    df['contents_attribute_l_m'] = df['contents_attribute_l'].apply(lambda x: l_code[x]['속성 L 중분류코드'])\n",
    "    df['contents_attribute_l_l'] = df['contents_attribute_l'].apply(lambda x: l_code[x]['속성 L 대분류코드'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "train = add_code(train, d_code, h_code, l_code)\n",
    "test = add_code(test, d_code, h_code, l_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.drop(['id', 'contents_open_dt','person_rn', 'contents_rn'], axis=1) \n",
    "test = test.drop(['id', 'contents_open_dt','person_rn', 'contents_rn'], axis=1)\n",
    "\n",
    "# print(train.head())\n",
    "train['d_l_match_yn']=train['d_l_match_yn'].replace([True,False],[1,0])\n",
    "train['d_m_match_yn']=train['d_m_match_yn'].replace([True,False],[1,0])\n",
    "train['d_s_match_yn']=train['d_s_match_yn'].replace([True,False],[1,0])\n",
    "train['h_l_match_yn']=train['h_l_match_yn'].replace([True,False],[1,0])\n",
    "train['h_m_match_yn']=train['h_m_match_yn'].replace([True,False],[1,0])\n",
    "train['h_s_match_yn']=train['h_s_match_yn'].replace([True,False],[1,0])\n",
    "\n",
    "test['d_l_match_yn']=test['d_l_match_yn'].replace([True,False],[1,0])\n",
    "test['d_m_match_yn']=test['d_m_match_yn'].replace([True,False],[1,0])\n",
    "test['d_s_match_yn']=test['d_s_match_yn'].replace([True,False],[1,0])\n",
    "test['h_l_match_yn']=test['h_l_match_yn'].replace([True,False],[1,0])\n",
    "test['h_m_match_yn']=test['h_m_match_yn'].replace([True,False],[1,0])\n",
    "test['h_s_match_yn']=test['h_s_match_yn'].replace([True,False],[1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#컬럼 순서 변경\n",
    "train.columns\n",
    "train=train[['d_l_match_yn', 'd_m_match_yn', 'd_s_match_yn', 'h_l_match_yn',\n",
    "       'h_m_match_yn', 'h_s_match_yn', 'person_attribute_a',\n",
    "       'person_attribute_a_1', 'person_attribute_b', 'person_prefer_c',\n",
    "       'person_prefer_d_1', 'person_prefer_d_2', 'person_prefer_d_3',\n",
    "       'person_prefer_e', 'person_prefer_f', 'person_prefer_g',\n",
    "       'person_prefer_h_1', 'person_prefer_h_2', 'person_prefer_h_3',\n",
    "       'contents_attribute_i', 'contents_attribute_a',\n",
    "       'contents_attribute_j_1', 'contents_attribute_j',\n",
    "       'contents_attribute_c', 'contents_attribute_k', 'contents_attribute_l',\n",
    "       'contents_attribute_d', 'contents_attribute_m', 'contents_attribute_e',\n",
    "       'contents_attribute_h', 'person_prefer_d_1_n',\n",
    "       'person_prefer_d_1_s', 'person_prefer_d_1_m', 'person_prefer_d_1_l',\n",
    "       'person_prefer_d_2_n', 'person_prefer_d_2_s', 'person_prefer_d_2_m',\n",
    "       'person_prefer_d_2_l', 'person_prefer_d_3_n', 'person_prefer_d_3_s',\n",
    "       'person_prefer_d_3_m', 'person_prefer_d_3_l', 'contents_attribute_d_n',\n",
    "       'contents_attribute_d_s', 'contents_attribute_d_m',\n",
    "       'contents_attribute_d_l', 'person_prefer_h_1_m', 'person_prefer_h_2_m',\n",
    "       'person_prefer_h_3_m', 'contents_attribute_h_m', 'person_prefer_h_1_l',\n",
    "       'person_prefer_h_2_l', 'person_prefer_h_3_l', 'contents_attribute_h_l',\n",
    "       'contents_attribute_l_n', 'contents_attribute_l_s',\n",
    "       'contents_attribute_l_m', 'contents_attribute_l_l','target']]\n",
    "\n",
    "test=test[['d_l_match_yn', 'd_m_match_yn', 'd_s_match_yn', 'h_l_match_yn',\n",
    "       'h_m_match_yn', 'h_s_match_yn', 'person_attribute_a',\n",
    "       'person_attribute_a_1', 'person_attribute_b', 'person_prefer_c',\n",
    "       'person_prefer_d_1', 'person_prefer_d_2', 'person_prefer_d_3',\n",
    "       'person_prefer_e', 'person_prefer_f', 'person_prefer_g',\n",
    "       'person_prefer_h_1', 'person_prefer_h_2', 'person_prefer_h_3',\n",
    "       'contents_attribute_i', 'contents_attribute_a',\n",
    "       'contents_attribute_j_1', 'contents_attribute_j',\n",
    "       'contents_attribute_c', 'contents_attribute_k', 'contents_attribute_l',\n",
    "       'contents_attribute_d', 'contents_attribute_m', 'contents_attribute_e',\n",
    "       'contents_attribute_h', 'person_prefer_d_1_n',\n",
    "       'person_prefer_d_1_s', 'person_prefer_d_1_m', 'person_prefer_d_1_l',\n",
    "       'person_prefer_d_2_n', 'person_prefer_d_2_s', 'person_prefer_d_2_m',\n",
    "       'person_prefer_d_2_l', 'person_prefer_d_3_n', 'person_prefer_d_3_s',\n",
    "       'person_prefer_d_3_m', 'person_prefer_d_3_l', 'contents_attribute_d_n',\n",
    "       'contents_attribute_d_s', 'contents_attribute_d_m',\n",
    "       'contents_attribute_d_l', 'person_prefer_h_1_m', 'person_prefer_h_2_m',\n",
    "       'person_prefer_h_3_m', 'contents_attribute_h_m', 'person_prefer_h_1_l',\n",
    "       'person_prefer_h_2_l', 'person_prefer_h_3_l', 'contents_attribute_h_l',\n",
    "       'contents_attribute_l_n', 'contents_attribute_l_s',\n",
    "       'contents_attribute_l_m', 'contents_attribute_l_l']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(46404, 58)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "\n",
    "train.shape\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   1    1    1    0    0    0    1    4    3    5  275  370  369    8\n",
      "    1    1    4   95   59    3    3   10    2    1    2 1608  275    1\n",
      "    4  139  275  274  274  216  369  368  297  216  369  368  297  216\n",
      "  275  274  274  216  316  398  368  422    3   94   58   94 1607 1606\n",
      " 1605 2016]\n",
      "1\n",
      "torch.Size([10000, 58])\n",
      "tensor(1.)\n",
      "(1000, 58)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "x = train.iloc[0:10000, :-1]\n",
    "y = train.iloc[0:10000, -1]\n",
    "\n",
    "x= x.to_numpy()\n",
    "print(x[0])\n",
    "print(y[0])\n",
    "x=np.where(x > 1000, x/1000, x)\n",
    "x=np.where(x > 100, x/100, x)\n",
    "x=np.where(x > 10, x/10, x)\n",
    "# x=x/100\n",
    "x=torch.Tensor(x)\n",
    "print(x.size())\n",
    "print(x[0][1])\n",
    "y=torch.Tensor(y)\n",
    "# print(y)\n",
    "\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(x, y, test_size=0.2, shuffle=True, stratify=y, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "# print(x)\n",
    "# print(y)\n",
    "\n",
    "\n",
    "batch_size=32\n",
    "\n",
    "dataset_train = TensorDataset(x_train, y_train)\n",
    "dataloader_train = DataLoader(dataset_train, batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "dataset_valid = TensorDataset(x_valid, y_valid)\n",
    "dataloader_valid = DataLoader(dataset_valid, batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "x_test=test.iloc[0:1000]\n",
    "x_test=x_test.to_numpy()\n",
    "x_test=np.where(x_test > 1000, x_test/1000, x_test)\n",
    "x_test=np.where(x_test > 100, x_test/100, x_test)\n",
    "x_test=np.where(x_test > 10, x_test/10, x_test)\n",
    "x_test=x_test/100\n",
    "\n",
    "print(x_test.shape)\n",
    "x_test=torch.FloatTensor(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class LogisticRegression(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "\n",
    "        # self.linear = nn.Sequential(\n",
    "        self.linear1=nn.Linear(58,32)\n",
    "        self.batch_normal1=nn.BatchNorm1d(32)\n",
    "        self.LeakRelu=nn.LeakyReLU()\n",
    "\n",
    "        self.linear2=nn.Linear(32, 32)\n",
    "        self.batch_normal2=nn.BatchNorm1d(32)\n",
    "        self.LeakRelu=nn.LeakyReLU()\n",
    "\n",
    "        self.linear3=nn.Linear(32, 16)\n",
    "        self.batch_normal3=nn.BatchNorm1d(16)\n",
    "        self.LeakRelu=nn.LeakyReLU()\n",
    "\n",
    "        self.linear4=nn.Linear(16, 1)\n",
    "        self.batch_normal4=nn.BatchNorm1d(1)\n",
    "        self.LeakRelu=nn.LeakyReLU()\n",
    "\n",
    "        self.sigmoid=nn.Sigmoid()    \n",
    "        torch.nn.init.xavier_uniform_(self.linear1.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.linear2.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.linear3.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.linear4.weight)\n",
    "        # )        \n",
    "\n",
    "        self.model=nn.Sequential(self.linear1, self.batch_normal1, self.LeakRelu, self.linear2,self.batch_normal2, self.LeakRelu, self.linear3,self.batch_normal3,self.LeakRelu,\n",
    "        self.linear4, self.batch_normal4,self.LeakRelu,self.sigmoid)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "model1=LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Module.parameters of LogisticRegression(\n",
      "  (linear1): Linear(in_features=58, out_features=32, bias=True)\n",
      "  (batch_normal1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (LeakRelu): LeakyReLU(negative_slope=0.01)\n",
      "  (linear2): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (batch_normal2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (linear3): Linear(in_features=32, out_features=16, bias=True)\n",
      "  (batch_normal3): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (linear4): Linear(in_features=16, out_features=1, bias=True)\n",
      "  (batch_normal4): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (sigmoid): Sigmoid()\n",
      "  (model): Sequential(\n",
      "    (0): Linear(in_features=58, out_features=32, bias=True)\n",
      "    (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.01)\n",
      "    (3): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (4): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): LeakyReLU(negative_slope=0.01)\n",
      "    (6): Linear(in_features=32, out_features=16, bias=True)\n",
      "    (7): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): LeakyReLU(negative_slope=0.01)\n",
      "    (9): Linear(in_features=16, out_features=1, bias=True)\n",
      "    (10): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (11): LeakyReLU(negative_slope=0.01)\n",
      "    (12): Sigmoid()\n",
      "  )\n",
      ")>\n"
     ]
    }
   ],
   "source": [
    "optimizer1 = optim.Adam(model1.parameters(), lr=0.001)\n",
    "print(model1.parameters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/100 Batch 1/250 Cost: 0.785893 Accuracy 40.62%  f1 score : 0.457143\n",
      "Epoch    0/100 Batch 101/250 Cost: 0.687099 Accuracy 46.88%  f1 score : 0.585366\n",
      "Epoch    0/100 Batch 201/250 Cost: 0.683651 Accuracy 68.75%  f1 score : 0.736842\n",
      "Epoch   10/100 Batch 1/250 Cost: 0.702255 Accuracy 56.25%  f1 score : 0.222222\n",
      "Epoch   10/100 Batch 101/250 Cost: 0.675796 Accuracy 53.12%  f1 score : 0.285714\n",
      "Epoch   10/100 Batch 201/250 Cost: 0.690203 Accuracy 68.75%  f1 score : 0.166667\n",
      "Epoch   20/100 Batch 1/250 Cost: 0.688154 Accuracy 68.75%  f1 score : 0.375000\n",
      "Epoch   20/100 Batch 101/250 Cost: 0.665947 Accuracy 75.00%  f1 score : 0.500000\n",
      "Epoch   20/100 Batch 201/250 Cost: 0.670840 Accuracy 71.88%  f1 score : 0.307692\n",
      "Epoch   30/100 Batch 1/250 Cost: 0.654789 Accuracy 71.88%  f1 score : 0.470588\n",
      "Epoch   30/100 Batch 101/250 Cost: 0.661993 Accuracy 75.00%  f1 score : 0.333333\n",
      "Epoch   30/100 Batch 201/250 Cost: 0.738954 Accuracy 59.38%  f1 score : 0.315789\n",
      "Epoch   40/100 Batch 1/250 Cost: 0.648097 Accuracy 65.62%  f1 score : 0.476190\n",
      "Epoch   40/100 Batch 101/250 Cost: 0.662577 Accuracy 65.62%  f1 score : 0.352941\n",
      "Epoch   40/100 Batch 201/250 Cost: 0.711479 Accuracy 65.62%  f1 score : 0.266667\n",
      "Epoch   50/100 Batch 1/250 Cost: 0.640121 Accuracy 68.75%  f1 score : 0.375000\n",
      "Epoch   50/100 Batch 101/250 Cost: 0.671786 Accuracy 65.62%  f1 score : 0.352941\n",
      "Epoch   50/100 Batch 201/250 Cost: 0.668426 Accuracy 62.50%  f1 score : 0.333333\n",
      "Epoch   60/100 Batch 1/250 Cost: 0.654168 Accuracy 59.38%  f1 score : 0.315789\n",
      "Epoch   60/100 Batch 101/250 Cost: 0.662589 Accuracy 62.50%  f1 score : 0.400000\n",
      "Epoch   60/100 Batch 201/250 Cost: 0.657487 Accuracy 65.62%  f1 score : 0.421053\n",
      "Epoch   70/100 Batch 1/250 Cost: 0.645344 Accuracy 87.50%  f1 score : 0.600000\n",
      "Epoch   70/100 Batch 101/250 Cost: 0.711153 Accuracy 68.75%  f1 score : 0.375000\n",
      "Epoch   70/100 Batch 201/250 Cost: 0.661568 Accuracy 71.88%  f1 score : 0.470588\n",
      "Epoch   80/100 Batch 1/250 Cost: 0.614796 Accuracy 75.00%  f1 score : 0.555556\n",
      "Epoch   80/100 Batch 101/250 Cost: 0.765639 Accuracy 46.88%  f1 score : 0.105263\n",
      "Epoch   80/100 Batch 201/250 Cost: 0.620166 Accuracy 71.88%  f1 score : 0.470588\n",
      "Epoch   90/100 Batch 1/250 Cost: 0.681968 Accuracy 65.62%  f1 score : 0.352941\n",
      "Epoch   90/100 Batch 101/250 Cost: 0.623957 Accuracy 68.75%  f1 score : 0.444444\n",
      "Epoch   90/100 Batch 201/250 Cost: 0.641538 Accuracy 78.12%  f1 score : 0.588235\n",
      "Epoch  100/100 Batch 1/250 Cost: 0.718841 Accuracy 53.12%  f1 score : 0.285714\n",
      "Epoch  100/100 Batch 101/250 Cost: 0.659041 Accuracy 84.38%  f1 score : 0.666667\n",
      "Epoch  100/100 Batch 201/250 Cost: 0.748552 Accuracy 68.75%  f1 score : 0.285714\n"
     ]
    }
   ],
   "source": [
    "nb_epochs = 100\n",
    "accuracy_list=[]\n",
    "\n",
    "model1.train()\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    for batch_idx, samples in enumerate(dataloader_train):\n",
    "\n",
    "        x_train, y_train = samples\n",
    "\n",
    "        x_train=x_train\n",
    "        y_train=y_train\n",
    "\n",
    "\n",
    "        # H(x) 계산\n",
    "        hypothesis = model1(x_train)\n",
    "        \n",
    "        \n",
    "    \n",
    "        # cost 계산\n",
    "        # print(y_train.size())\n",
    "        hypothesis=hypothesis.squeeze()\n",
    "        # print(hypothesis.size())\n",
    "        # print(hypothesis)\n",
    "        # print(y_train)\n",
    "        cost = F.binary_cross_entropy(hypothesis, y_train)\n",
    "\n",
    "        # cost로 H(x) 개선\n",
    "        optimizer1.zero_grad()\n",
    "        cost.backward()\n",
    "        optimizer1.step()\n",
    "\n",
    "        bigo=torch.FloatTensor([0.5])\n",
    "        \n",
    "\n",
    "        # 20번마다 로그 출력\n",
    "        if epoch % 10 == 0:\n",
    "          if batch_idx %100 == 0:\n",
    "              prediction = hypothesis >= bigo # 예측값이 0.5를 넘으면 True로 간주\n",
    "              correct_prediction = prediction.float() == y_train # 실제값과 일치하는 경우만 True로 간주\n",
    "              accuracy = correct_prediction.sum().item() / len(correct_prediction) # 정확도를 계산\n",
    "              f1 = f1_score(y_train.cpu(), prediction.float().cpu())\n",
    "              print('Epoch {:4d}/{} Batch {}/{} Cost: {:.6f} Accuracy {:2.2f}%  f1 score : {:2.6f}'.format( # 각 에포크마다 정확도를 출력\n",
    "                  epoch, nb_epochs, batch_idx+1, len(dataloader_train), cost.item(), accuracy * 100, f1\n",
    "              ))\n",
    "\n",
    "    torch.save(model1.state_dict(), f'{epoch:04d}.pt')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4948],\n",
      "        [0.4939],\n",
      "        [0.4963],\n",
      "        ...,\n",
      "        [0.4999],\n",
      "        [0.6048],\n",
      "        [0.4960]])\n",
      "torch.Size([2000, 1])\n",
      "Accuracy: 0.62\n"
     ]
    }
   ],
   "source": [
    "# 테스트 데이터를 사용하여 모델을 테스트한다.\n",
    "with torch.no_grad(): # torch.no_grad()를 하면 gradient 계산을 수행하지 않는다.\n",
    "    X_valid = x_valid\n",
    "    Y_valid = y_valid\n",
    "    model1.eval()\n",
    "    prediction = model1(X_valid)\n",
    "\n",
    "    print(prediction)\n",
    "\n",
    "    predictions = prediction >= bigo # 예측값이 0.5를 넘으면 True로 간주\n",
    "    \n",
    "    # print(predictions)\n",
    "    # print('='*20)\n",
    "    # print(predictions.size())\n",
    "    # print('='*20)\n",
    "    # print(predictions)\n",
    "    # print(predictions.shape)\n",
    "    Y_valid=torch.unsqueeze(Y_valid,1)\n",
    "    print(Y_valid.shape)\n",
    "\n",
    "    correct_prediction = predictions == Y_valid # 실제값과 일치하는 경우만 True로 간주\n",
    "\n",
    "    # print(y_valid)\n",
    "\n",
    "    # correct_prediction=predictions.replace([True,False],[1,0])\n",
    "\n",
    "    # print(correct_prediction)\n",
    "    # print(correct_prediction.size())\n",
    "    # print(len(correct_prediction))\n",
    "\n",
    "    accuracy = correct_prediction.sum().item() / len(correct_prediction) # 정확도를 계산\n",
    "\n",
    "    print('Accuracy:', accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      True\n",
      "1      True\n",
      "2      True\n",
      "3      True\n",
      "4      True\n",
      "       ... \n",
      "995    True\n",
      "996    True\n",
      "997    True\n",
      "998    True\n",
      "999    True\n",
      "Length: 1000, dtype: bool\n"
     ]
    }
   ],
   "source": [
    "result=pd.read_csv('C:\\\\Users\\\\user\\\\Desktop\\\\computing\\\\outside\\\\dacon\\\\jobcare_recommend\\\\Jobcare_data\\\\sample_submission.csv')\n",
    "\n",
    "\n",
    "hypothesis=model1(x_test)\n",
    "\n",
    "prediction = hypothesis >= torch.FloatTensor([0.5]) # 예측값이 0.5를 넘으면 True로 간주\n",
    "prediction = torch.squeeze(prediction)\n",
    "prediction=pd.Series(prediction)\n",
    "print(prediction)\n",
    "prediction=prediction.replace([True,False],[1,0])\n",
    "\n",
    "result['target']=prediction\n",
    "result.to_csv('result.csv',index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6255\n"
     ]
    }
   ],
   "source": [
    "model=LogisticRegression()\n",
    "\n",
    "model.load_state_dict(torch.load('C:\\\\Users\\\\user\\\\Desktop\\\\computing\\\\outside\\\\dacon\\\\jobcare_recommend\\\\0040.pt'))\n",
    "model.eval()\n",
    "\n",
    "result=model(X_valid)\n",
    "result1 = result >= bigo # 예측값이 0.5를 넘으면 True로 간주\n",
    "correct_prediction1 = result1 == Y_valid\n",
    "\n",
    "accuracy1 = correct_prediction1.sum().item() / len(correct_prediction) # 정확도를 계산\n",
    "\n",
    "print('Accuracy:', accuracy1)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f68a52d6746b8edec33e9d0848c665d5f17e646007ceb6470c84aeea645ccfe6"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit ('dacon': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
