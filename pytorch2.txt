1월 4일

1. 데이터들의 수치가 1000단위가 넘어 가는것이 있어서

sigmoid함수를 지나칠때 너무 큰수때문에 측정이 안된다

그래서 모든숫자들을 한자리수로 자릿수를 옮긴뒤에 모두 100을 나눠줘서 일단 모델이 돌아가도록한다.

2. 내 컴퓨터에서 돌려서 일단 1000개의 데이터로만 모듈을 돌려봤는데 실제 학습데이터는 약 50만개

테스트 데이터는 약 5만개 정도된다.

이 모든 데이터를 사용할수 있을까? 일단 내컴퓨터에서는 부담되어 코랩에서 돌려봤을때 10000개조차 버거워 보였다.

아직 batch의 개념을 사용하지 않았지만 이제는 사용해볼 때가 된것 같다.

3. 데이터를 다루는 과정에서 target이 마지막이 아니라 중간쯤에 끼게 되었다. 이경우 칼럼의 순서를 강제로 바꿔주었다. (편의를 위해)


1월 5일

미니배치 적용 기존에 10000개의 데이터에서 미니배치를 적용하여 좀더 원할히 모델을 돌리도록함


1월 6일 

직접 만든 모델을 돌려보는 도중 sigmoid대신에 ReLU함수를 사용하니 

  cost = F.binary_cross_entropy(hypothesis, y_train)에서

RuntimeError: all elements of input should be between 0 and 1
가 발생하는 것을 확인할수 있었다. 

어째서 다른 원소가 들어가는가? 확인해볼 필요가 있을것이다.

적은 데이터에서 사용하던것을 GPU(colab)을 사용해보기 시작했다.


생각보다 정확도가 60%에서 올라가지 않는다 많은데이터 (약 50만개)에비해서 너무 얇은 은닉층(2층)이 그이유 일까? 어떠한 방법으로 모델의 정확도를 높일수 있는지 생각해봐야 할것같다.

추후에는 validation을 섞어서 실제 테스트 데이터에서 점수를 뽑아볼것이다.

