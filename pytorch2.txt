1월 4일

1. 데이터들의 수치가 1000단위가 넘어 가는것이 있어서

sigmoid함수를 지나칠때 너무 큰수때문에 측정이 안된다

그래서 모든숫자들을 한자리수로 자릿수를 옮긴뒤에 모두 100을 나눠줘서 일단 모델이 돌아가도록한다.

2. 내 컴퓨터에서 돌려서 일단 1000개의 데이터로만 모듈을 돌려봤는데 실제 학습데이터는 약 50만개

테스트 데이터는 약 5만개 정도된다.

이 모든 데이터를 사용할수 있을까? 일단 내컴퓨터에서는 부담되어 코랩에서 돌려봤을때 10000개조차 버거워 보였다.

아직 batch의 개념을 사용하지 않았지만 이제는 사용해볼 때가 된것 같다.

3. 데이터를 다루는 과정에서 target이 마지막이 아니라 중간쯤에 끼게 되었다. 이경우 칼럼의 순서를 강제로 바꿔주었다. (편의를 위해)


1월 5일

미니배치 적용 기존에 10000개의 데이터에서 미니배치를 적용하여 좀더 원할히 모델을 돌리도록함


1월 6일 

직접 만든 모델을 돌려보는 도중 sigmoid대신에 ReLU함수를 사용하니 

  cost = F.binary_cross_entropy(hypothesis, y_train)에서

RuntimeError: all elements of input should be between 0 and 1
가 발생하는 것을 확인할수 있었다. 

어째서 다른 원소가 들어가는가? 확인해볼 필요가 있을것이다.

적은 데이터에서 사용하던것을 GPU(colab)을 사용해보기 시작했다.


생각보다 정확도가 60%에서 올라가지 않는다 많은데이터 (약 50만개)에비해서 너무 얇은 은닉층(2층)이 그이유 일까? 어떠한 방법으로 모델의 정확도를 높일수 있는지 생각해봐야 할것같다.

추후에는 validation을 섞어서 실제 테스트 데이터에서 점수를 뽑아볼것이다.


1월 10일

1. 초기값에 Xavier을 적용시켰다. sigmoid를 사용하는 경우는 Xavier, ReLU를 사용하는경우에는 He초기값을 사용하는것이 좋다고한다.

2. validation 정확도와 정답table을 뽑아냈다.

3. 역시 자료형을 맞춰주는건 항상 짜증난다.

4. 각 에폭마다 test와 validation의 정확도를 비교해보려한다.

5. 모델 저장과 불러오기 에대한 간단한 실습

6.시각화를 해보려 남은시간을 소모했으나 과연 가능할지...

1월 11일

1. 간단히 f1_score와 accuracy를 train과 validation을 나눠서 시각화했음

2 . colab에 save_and_load코드와 visualize두 코드를 합쳐서 모델을 돌리기 시도

3. 학습이 0.6에서 제대로 올라가지 않음 데이터의 변수가 너무 많아서 그런것인가 조금 건드려볼 필요가 있을것 같음


1월 12일

변수가 너무 많은것 같아 데이터의 상황을 보고 데이터 변수를 제거해보자 한다.

변수중 모든데이터가 동일한 값을 띄는 'person_prefer_f','person_prefer_g'의 경우 제거해주었고

대회측에서 제공한 속성 D,H,L코드의 세부코드 또한 제거하였다. 굳이 DHL이 함수적으로 대응하는 것이 아닌데 정말 큰의미가 있을것인지 의문이 들었다.(DHL에서 생성되는 변수만해도 10개가 넘어가기에 너무 과도하다고 생각되었다.)


